#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{"/config/config.tex"}
\allowdisplaybreaks
\end_preamble
\use_default_options true
\master ../lyxmain.lyx
\begin_modules
theorems-ams-chap-bytype
enumitem
tcolorbox
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.12
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 1
\use_package stackrel 2
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 2cm
\footskip 2cm
\secnumdepth -1
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Solutions for exercises to chapter 2
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\biggp}[1]{\bigg( #1 \bigg)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.1 - Bernoulli distribution's expectation, variance, normalization,
 entropy
\end_layout

\end_inset

 Verify that the Bernoulli distribution (2.2) satisfies the following properties
 
\begin_inset Formula 
\begin{align*}
\sum_{x=0}^{1}f(x\vert\mu) & =1,\\
\E[X] & =\mu,\\
\mathrm{Var}[X] & =\mu(1-\mu).
\end{align*}

\end_inset

Show that the entropy 
\begin_inset Formula $H(X)$
\end_inset

 of a Bernoulli distributed random binary variable 
\begin_inset Formula $X$
\end_inset

 is given by 
\begin_inset Formula 
\[
H(X)=-\mu\ln\mu-(1-\mu)\ln(1-\mu).
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the discussion below, 
\begin_inset Formula $X$
\end_inset

 is a random variable following Bernoulli distribution.
 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To ch`eck normalization, we note 
\begin_inset Formula 
\[
\sum_{x=0}^{1}f_{X}(x\vert\mu)=\mu+(1-\mu)=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the expectation, note that 
\begin_inset Formula 
\[
\E[X]=\sum_{x=0}^{1}xf_{X}(x\vert\mu)=1\cdot\mu+0\cdot(1-\mu)=\mu.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, we note that 
\begin_inset Formula 
\[
\text{Var}[X]=\E[X^{2}]-(\E X)^{2}=\mu-\mu^{2}=\mu(1-\mu).
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the entropy, we note that 
\begin_inset Formula 
\[
H(X)=-\sum_{x=0}^{1}f_{X}(x\vert\mu)f\log f_{X}(x|\mu)=-\mu\log\mu-(1-\mu)\log1-\mu.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.2 - Symmetric Bernoulli distribution's expectation, variance, normaliza
tion, entropy
\end_layout

\end_inset

 The form of the Bernoulli distribution given by (2.2) is not symmetric between
 the two values of 
\begin_inset Formula $X$
\end_inset

.
 In some situations, it will be more convenient to use an equivalent formulation
 for which 
\begin_inset Formula $X\in\{-1,1\}$
\end_inset

, in which case the distribution can be written
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To show it's normalized, we note
\begin_inset Formula 
\[
\sum_{x\in\{-1,1\}}f_{X}(x|\mu)=\left(\frac{1-\mu}{2}\right)^{2/2}\left(\frac{1+\mu}{2}\right)^{0}+\left(\frac{1-\mu}{2}\right)^{0}\left(\frac{1+\mu}{2}\right)^{1}=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its expectation, we note 
\begin_inset Formula 
\[
\E[X]=\sum_{x\in\{-1,1\}}xf_{X}(x|\mu)=\left(\frac{1+\mu}{2}\right)-\left(\frac{1-\mu}{2}\right)=\mu.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its variance, we note 
\begin_inset Formula 
\[
\text{Var}[X]=\E[X^{2}]-(\E[X])^{2}=\left(\frac{1+\mu}{2}\right)+\left(\frac{1-\mu}{2}\right)-\mu^{2}=1-\mu^{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find its entropy, we note 
\begin_inset Formula 
\[
H(X)=-\sum_{x\in\{-1,1\}}f_{X}(x|\mu)\log f_{X}(x|\mu)=-\left(\frac{1-\mu}{2}\right)\log\frac{1-\mu}{2}-\left(\frac{1+\mu}{2}\right)\log\frac{1+\mu}{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.3 - Binomial distribution is normalized
\end_layout

\end_inset

 In this exercise, we prove that the binomial distribution (2.9) is normalized.
 First use the definition (2.10) of the number of combinations of 
\begin_inset Formula $m$
\end_inset

 identical objects chosen from a total of 
\begin_inset Formula $N$
\end_inset

 to show that
\begin_inset Formula 
\[
\binom{N}{m}+\binom{N}{m-1}=\binom{N+1}{m}.
\]

\end_inset

Use this result to prove by induction the following result
\begin_inset Formula 
\[
(1+x)^{N}=\sum_{m=0}^{N}\binom{N}{m}x^{m},
\]

\end_inset

which is known as the binomial theorem, and which is valid for all real
 values of 
\begin_inset Formula $x.$
\end_inset

 Finally, show that the binomial distribution is normalized, so that 
\begin_inset Formula 
\[
\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{N-m}=1,
\]

\end_inset

which can be done by first pulling out a factor 
\begin_inset Formula $(1-\mu)^{N}$
\end_inset

 out of the summation and then making use of the binomial theorem.
 
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 First, we show Eq.(2.262) holds: note that 
\begin_inset Formula 
\begin{align*}
\binom{N}{m}+\binom{N}{m-1} & =\frac{N!}{m!(N-m)!}+\frac{N!}{(m-1)!(N-m+1)!}\\
 & =\frac{N!(N-m+1)}{m!(N-m+1)!}+\frac{mN!}{m!(N-m+1)!}\\
 & =\frac{(N+1)!}{m!((N+1)-m)!}\\
 & =\binom{N+1}{m}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To prove the binomial theorem, we induce on 
\begin_inset Formula $N.$
\end_inset

 For the base case 
\begin_inset Formula $N=1$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

, it is trivially true:
\begin_inset Formula 
\begin{align*}
(1+x)^{1} & =\binom{1}{0}x^{0}+\binom{1}{1}x^{1}=1+x,\\
(1+x)^{0} & =\binom{0}{0}x^{0}=1.
\end{align*}

\end_inset

Now suppose the claim holds for 
\begin_inset Formula $N=k.$
\end_inset

 Then for 
\begin_inset Formula $N=k+1$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
(1+x)^{k+1} & =(1+x)(1+x)^{k}=(1+x)\sum_{m=0}^{N}\binom{N}{m}x^{m}\\
 & =\sum_{m=0}^{M}\binom{N}{m}x^{m}+\sum_{m=0}^{N}\binom{N}{m}x^{m+1}\\
 & =\binom{N}{0}x^{0}+\sum_{m=1}^{M}\binom{N}{m}x^{m}+\sum_{m=1}^{N}\binom{N}{m-1}x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\binom{N+1}{0}x^{0}+\sum_{m=1}^{M}\left(\binom{N}{m}+\binom{N}{m-1}\right)x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\binom{N+1}{0}x^{0}+\sum_{m=1}^{M}\binom{N+1}{m}x^{m}+\binom{N+1}{N+1}x^{N+1}\\
 & =\sum_{m=0}^{N+1}\binom{N}{m}x^{m}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Now to show that the binomial distribution is normalized, we note that 
\begin_inset Formula 
\begin{align*}
\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{N-m} & =(1-\mu)^{N}\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{-m}\\
 & =(1-\mu)^{N}\sum_{m=0}^{N}\binom{N}{m}\left(\frac{\mu}{1-\mu}\right)^{m}\\
 & =(1-\mu)^{N}\left(1+\frac{\mu}{1-\mu}\right)^{N}.\tag{by binomial theorem}\\
 & =\left[(1-\mu)\left(1+\frac{\mu}{1-\mu}\right)\right]^{N}
\end{align*}

\end_inset

Since 
\begin_inset Formula 
\begin{align*}
(1-\mu)\left(1+\frac{\mu}{1-\mu}\right) & =1+\frac{\mu}{1-\mu}-\mu-\frac{\mu^{2}}{1-\mu}\\
 & =1+\frac{\mu-\mu+\mu^{2}-\mu^{2}}{1-\mu}\\
 & =1,
\end{align*}

\end_inset

it follows that 
\begin_inset Formula $\sum_{m=0}^{N}\binom{N}{m}\mu^{m}(1-\mu)^{N-m}=1,$
\end_inset

 and thus the result follows.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.4 - Binomial distribution's expectation and variance 
\end_layout

\end_inset

 Show that the mean of the binomial distribution is given by (2.11).
 To do this, differentiate both sides of the normalization condition (2.264)
 with respect to 
\begin_inset Formula $\mu$
\end_inset

 and then rearrange to obtain an expression for the mean of 
\begin_inset Formula $n$
\end_inset

.
 Similarly, by differentiating (2.264) twice with respect to 
\begin_inset Formula $\mu$
\end_inset

 and making use of the result (2.11) for the mean of the binomial distribution
 prove the result (2.12) for the variance of the binomial.
 
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 Following the hint, we differentiate Eq.(2.264) w.r.t 
\begin_inset Formula $\mu$
\end_inset

 once:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\mu}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} = & n\cdot\sum_{n=1}^{N-1}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=1}^{N-1}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
 & -N(1-\mu)^{N-1}+N\mu^{N-1}\\
= & n\cdot\sum_{n=1}^{N}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=0}^{N-1}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
= & n\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n-1}(1-\mu)^{N-n}-(N-n)\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n-1}\\
= & \frac{n}{\mu}\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}-\frac{N-n}{1-\mu}\cdot\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\\
= & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right).
\end{align*}

\end_inset

Since 
\begin_inset Formula $\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}=1,$
\end_inset

 it follows that 
\begin_inset Formula 
\begin{align*}
\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right]=0 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right][\mu(1-\mu)]=0\\
 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}[n(1-\mu)-(N-n)\mu]=0\\
 & \iff\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)=0.\tag{1}
\end{align*}

\end_inset

Now we rearrange Eq.(1):
\begin_inset Formula 
\[
\sum_{n=0}^{N}n\cdot\binom{N}{n}\mu^{n}(1-\mu)^{N-n}=N\mu\left(\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right)=N\mu.
\]

\end_inset

The result follows by observing that 
\begin_inset Formula 
\[
\E[X]=\sum_{n=0}^{N}n\cdot\binom{N}{n}\mu^{n}(1-\mu)^{N-n}
\]

\end_inset


\end_layout

\begin_layout Enumerate
To facilitate notation, we let 
\begin_inset Formula $\varphi(\mu)=\sum_{n=1}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\frac{n}{\mu}-\frac{N-n}{1-\mu}\right].$
\end_inset

 Then following the hint, we differentiate twice Eq.(2.264) w.r.t.
 
\begin_inset Formula $\mu$
\end_inset

 and get 
\begin_inset Formula 
\begin{align*}
\frac{\partial^{2}}{\partial\mu^{2}}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\}  & =\frac{\partial\varphi(\mu)}{\partial\mu}\\
 & =\sum_{n=0}^{N}\underbrace{\frac{\partial}{\partial\mu}\left\{ \binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)\right\} }_{:=H(\mu).}.
\end{align*}

\end_inset

Hence, it suffices to evaluate 
\begin_inset Formula $H(\mu)$
\end_inset


\begin_inset Formula 
\begin{align*}
H(\mu) & =\frac{\partial}{\partial\mu}\left\{ \binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} \left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)+\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\frac{\partial}{\partial\mu}\left\{ \frac{n}{\mu}-\frac{N-n}{1-\mu}\right\} \\
 & =\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}+\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]\\
 & =\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right].
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\mu^{2}}\left\{ \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\right\} =\underbrace{\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]=0}_{(2)}.
\]

\end_inset

Now, we arrange Eq.(2) and get 
\begin_inset Formula 
\begin{align*}
 & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[\left(\frac{n}{\mu}-\frac{N-n}{1-\mu}\right)^{2}-\frac{N-n}{(1-\mu)^{2}}-\frac{n}{\mu^{2}}\right](\mu^{2}(1-\mu)^{2})=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(n(1-\mu)-(N-n)\mu)^{2}-(N-n)\mu^{2}-n(1-\mu)^{2}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(n-N\mu)^{2}-(N-n)\mu^{2}-n(1-\mu)^{2}\right]=0\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}\left[(N-n)\mu^{2}+n(1-\mu)^{2}\right]\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(N\mu^{2}+n-2n\mu)\\
\iff & \sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}=N\mu-N\mu^{2}=N\mu(1-\mu).
\end{align*}

\end_inset

The conclusion can be drawn by observing that
\begin_inset Formula 
\[
\text{Var}[X]=\E[(X-\E[X])^{2}]=\sum_{n=0}^{N}\sum_{n=0}^{N}\binom{N}{n}\mu^{n}(1-\mu)^{N-n}(n-N\mu)^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.5 - Beta distribution is normalized 
\end_layout

\end_inset

 In this exercise, we prove that the beta distribution, given by (2.13),
 is correctly normalized, so that (2.14) holds.
 This is equivalent to showing that 
\begin_inset Formula 
\[
\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}.
\]

\end_inset

From the definition (1.141) of the gamma function, we have 
\begin_inset Formula 
\[
\Gamma(a)\Gamma(b)=\int_{0}^{\infty}\exp(-x)x^{a-1}dx\int_{0}^{\infty}\exp(-y)y^{b-1}dy.
\]

\end_inset

Use this expression to prove (2.265) as follows.
 First bring the integral over 
\begin_inset Formula $y$
\end_inset

 inside the integrand of the integral of 
\begin_inset Formula $x$
\end_inset

, next make the change of variable 
\begin_inset Formula $t=y+x$
\end_inset

, where 
\begin_inset Formula $x$
\end_inset

 is fixed, then interchange the order of the 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 integrations, and finally make the change of variable 
\begin_inset Formula $x=t\mu$
\end_inset

 where 
\begin_inset Formula $t$
\end_inset

 is fixed.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we note that 
\begin_inset Formula 
\begin{align*}
\Gamma(a)\Gamma(b) & =\int_{0}^{\infty}e^{-x}x^{a-1}dx\int_{0}^{\infty}e^{-y}y^{b-1}dy\\
 & =\int_{0}^{\infty}\int_{0}^{\infty}e^{-(x+y)}x^{a-1}y^{b-1}dydx.\tag{1}
\end{align*}

\end_inset

Now, we make a change of variable 
\begin_inset Formula 
\[
x+y=t\implies\begin{cases}
y=t-x\\
y\geq0\Leftrightarrow t-x\geq0\Leftrightarrow t\geq x\\
x\geq0\\
dt=dy
\end{cases}.
\]

\end_inset

Therefore, it follows that 
\begin_inset Formula 
\begin{align*}
\text{Eq.(1)} & =\int_{0}^{\infty}\int_{x}^{\infty}e^{-t}x^{a-1}(t-x)^{b-1}dtdx\\
 & =\int_{0}^{\infty}\int_{0}^{t}e^{-t}x^{a-1}(t-x)^{b-1}dxdt\tag{by Fubini's theorem}\\
 & =\int_{0}^{\infty}\int_{0}^{1}e^{-t}(t\mu)^{a-1}(t-t\mu)^{b-1}td\mu dt\tag{2}\\
 & =\int_{0}^{\infty}e^{-t}t^{a-1}t^{b-1}tdt\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu\\
 & =\Gamma(a+b)\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu,
\end{align*}

\end_inset

where 
\begin_inset Formula $\text{Eq.(2)}$
\end_inset

 follows from a change of variables 
\begin_inset Formula 
\[
x=t\mu\implies\begin{cases}
0\leq x\leq t\Leftrightarrow0\leq t\mu\leq t\Leftrightarrow0\leq\mu\leq t\\
dx=td\mu
\end{cases}.
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\int_{0}^{1}\mu^{a-1}(1-\mu)^{b-1}d\mu=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)},
\]

\end_inset

and as a result the Beta density integrates to 1.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.6 - Beta distribution's expectation, variance, mode 
\end_layout

\end_inset

 Make use of the result (2.265) to show that the mean, variance, and mode
 of the beta distribution (2.13) are given respectively by 
\begin_inset Formula 
\begin{align*}
\E[\mu] & =\frac{a}{a+b},\\
\mathrm{Var}[\mu] & =\frac{ab}{(a+b)^{2}(a+b+1)},\\
\mathrm{mode}[\mu] & =\frac{a-1}{a+b-1}.
\end{align*}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a random variable that follows Beta distribution with parameter 
\begin_inset Formula $a,b\in\mathbb{R}^{+}.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To find the expectation, note that 
\begin_inset Formula 
\begin{align*}
\E[X] & =\int_{0}^{1}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}xdx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}x^{(a+1)-1}(1-x)^{b-1}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}\tag{by Problem 2.5}\\
 & =\frac{\Gamma(a+b)a\Gamma(a)\Gamma(b)}{\Gamma(a)\Gamma(b)\Gamma(a+b)\Gamma(a+b)}\tag{since \ensuremath{\Gamma(x+1)=x\Gamma(x).}}\\
 & =\frac{a}{a+b}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, we first note 
\begin_inset Formula 
\begin{align*}
\E[X^{2}] & =\int\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}x^{2}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}x^{a+2-1}(1-x)^{b-1}dx\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)}\\
 & =\frac{a(a+1)}{(a+b+1)(a+b)}.
\end{align*}

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\text{Var}[X] & =\E[X^{2}]-(\E[X])^{2}=\frac{a(a+1)}{(a+b+1)(a+b)}-\left(\frac{a}{a+b}\right)^{2}\\
 & =\frac{a(a+1)(a+b)-a^{2}(a+b+1)}{(a+b+1)(a+b)^{2}}\\
 & =\frac{ab}{(a+b+1)(a+b)^{2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Since the mode of a continuous probability distribution is defined as its
 density function's critical point, it suffices for us to differentiate
 
\begin_inset Formula $f_{X}(x)$
\end_inset

 and find the critical points.
 Note that 
\begin_inset Formula 
\begin{align*}
\frac{\partial f_{X}(x)}{\partial x} & =\frac{\partial}{\partial x}\left[\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}\right]\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right].
\end{align*}

\end_inset

Setting it to zero yields 
\begin_inset Formula 
\begin{align*}
 & \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right]=0\\
\iff & \left[(a-1)x^{a-2}(1-x)^{b-1}+(b-1)x^{a-1}(1-x)^{b-2}\right]=0\\
\iff & (a-1)(1-x)=(b-1)x\\
\iff & x=\frac{a-1}{a+b-2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.7 - Comparison between posterior mean and MLE for Bernoulli model
 
\end_layout

\end_inset

 Consider a binomial random variable 
\begin_inset Formula $X$
\end_inset

 given by (2.9), with prior distribution for 
\begin_inset Formula $\mu$
\end_inset

 given by the beta distribution (2.13), and suppose we have observed m occurrence
s of 
\begin_inset Formula $X=1$
\end_inset

 and 
\begin_inset Formula $l$
\end_inset

 occurrences of 
\begin_inset Formula $X=0$
\end_inset

.
 Show that the posterior mean value of 
\begin_inset Formula $X$
\end_inset

 lies between the prior mean and the maximum likelihood estimate for 
\begin_inset Formula $\mu$
\end_inset

.
 To do this, show that the posterior mean can be written as 
\begin_inset Formula $\lambda$
\end_inset

 times the prior mean plus 
\begin_inset Formula $(1-\lambda)$
\end_inset

 times the maximum likelihood estimate, where 
\begin_inset Formula $0\leq\lambda\leq1$
\end_inset

.
 This illustrates the concept of the posterior distribution being a compromise
 between the prior distribution and the maximum likelihood solution.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
The book didn't go through the details of deriving some of the calculations.
 Although these calculations are simple, they are worth doing by hand at
 least once.
 Hence, we show them here.
 For notation, we let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 denote the sample data, 
\begin_inset Formula $(x_{1},\ldots x_{N})$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
First, we find the posterior mean for the Bernoulli model.
 By assumption, the parameter of interest, 
\begin_inset Formula $\mu,$
\end_inset

 follows beta distribution, i.e.
 
\begin_inset Formula 
\[
f(\mu|a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}.
\]

\end_inset

And the likelihood function after sampling the data is given by 
\begin_inset Formula 
\[
f(\mathcal{X}\vert\mu)=\mu^{\sum_{i=1}^{N}x_{i}}(1-\mu)^{\sum_{i=1}^{N}(1-x_{i})}=\mu^{n}(1-\mu)^{m}.
\]

\end_inset

Therefore, we have the posterior as 
\begin_inset Formula 
\begin{align*}
f(\mu|\vert\mathcal{X}) & \propto f(\mu\vert a,b)\cdot f(x_{1},\ldots,x_{N}\vert\mu)\\
 & =\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mu^{n}(1-\mu)^{m}\\
 & \propto\mu^{a+n-1}(1-\mu)^{b+m-1}.
\end{align*}

\end_inset

Since 
\begin_inset Formula $f(\mu|\mathcal{X})$
\end_inset

 should integrates to 1 in order to be a valid probability density function,
 in view of Problem 2.5 we see that 
\begin_inset Formula 
\[
f(\mu\vert\mathcal{X})=\frac{\Gamma(a+b+n+m)}{\Gamma(a)\Gamma(b)}\mu^{a+n-1}(1-\mu)^{b+m-1}\sim\text{Beta}(a+n,b+m).
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\E_{\mu|\mathcal{X}}[\mu]=\frac{a+n}{a+b+n+m}
\]

\end_inset

as desired.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Next, we find 
\begin_inset Formula $\mu_{MLE}.$
\end_inset

 First, we write out the likelihood equation, 
\begin_inset Formula 
\[
f(\mathcal{X}|\mu)=\prod_{i=1}^{N}\mu^{x_{i}}(1-\mu)^{1-x_{i}}=\mu^{\sum_{i=1}^{N}x_{i}}(1-\mu)^{\sum_{i=1}^{N}(1-x_{i})},
\]

\end_inset

from which we can get the log-likelihood equation as 
\begin_inset Formula 
\[
\ell(\mu)=\log f(\mathcal{X}\vert\mu)=\left(\sum_{i=1}^{N}x_{i}\right)\log\mu+\left(\sum_{i=1}^{N}(1-x_{i})\right)\log(1-\mu).
\]

\end_inset

Now we differentiate and set to zero 
\begin_inset Formula 
\begin{align*}
 & \frac{\partial\ell(\mu)}{\partial\mu}=\left(\sum_{i=1}^{N}x_{i}\right)\frac{1}{\mu}-\left(\sum_{i=1}^{N}(1-x_{i})\right)\frac{1}{1-\mu}=0\\
\iff & \left(\sum_{i=1}^{N}x_{i}\right)(1-\mu)-\left(\sum_{i=1}^{N}(1-x_{i})\right)\mu=0\\
\iff & \frac{1}{\mu}=\frac{\sum_{i=1}^{N}(1-x_{i})}{\sum_{i=1}^{N}x_{i}}+1=\frac{\sum_{i=1}^{N}1-\sum_{i=1}^{N}x_{i}+\sum_{i=1}^{N}x_{i}}{\sum_{i=1}^{N}x_{i}}\\
\iff & \mu_{MLE}=\frac{n}{n+m}.
\end{align*}

\end_inset

Now it suffices to show that 
\begin_inset Formula 
\[
\frac{a+n}{a+b+n+m}\in\text{Seg}\left(\frac{a}{a+b},\frac{n}{n+m}\right),
\]

\end_inset

where Seg means the line segment whose endpoints are 
\begin_inset Formula $a/(a+b)$
\end_inset

 and 
\begin_inset Formula $n/(n+m)$
\end_inset

.
 To show this, it suffices to show that the solution, denoted as 
\begin_inset Formula $\lambda_{*}$
\end_inset

, to the equation 
\begin_inset Formula 
\[
\lambda\left(\frac{a}{a+b}\right)+(1-\lambda)\frac{n}{n+m}=\frac{a+n}{a+b+n+m}
\]

\end_inset

lies in 
\begin_inset Formula $(0,1).$
\end_inset

 Solving the equation yields 
\begin_inset Formula 
\[
\lambda_{*}=\frac{a+b}{a+b+m+n}.
\]

\end_inset

Then the claim is true since 
\begin_inset Formula $a,b,n,m>0$
\end_inset

 by assumption.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.9 - Dirichlet distribution is normalized 
\end_layout

\end_inset

 In this exercise, we prove the normalization of the Dirichlet distribution
 (2.38) using induction.
 We have already shown in Exercise 2.5 that the beta distribution, which
 is a special case of the Dirichlet for 
\begin_inset Formula $M=2$
\end_inset

, is normalized.
 We now assume that the Dirichlet distribution is normalized for 
\begin_inset Formula $M-1$
\end_inset

 variables and prove that it is normalized for 
\begin_inset Formula $M$
\end_inset

 variables.
 To do this, consider the Dirichlet distribution over 
\begin_inset Formula $M$
\end_inset

 variables, and take account of the constraint 
\begin_inset Formula $\sum_{k=1}^{M}\mu_{k}$
\end_inset

 by eliminating 
\begin_inset Formula $\mu_{m}$
\end_inset

 , so that the Dirichlet is written 
\begin_inset Formula 
\[
p_{M}(\mu_{1},\ldots,\mu_{M-1})=C_{M}\prod_{k=1}^{M-1}\mu_{k}^{\alpha_{k}-1}\bigg(1-\sum_{j=1}^{M-1}\mu_{j}\bigg)^{\alpha_{M}-1}
\]

\end_inset

and out goal is to find an expression for 
\begin_inset Formula $C_{M}.$
\end_inset

 To do this, integrate over 
\begin_inset Formula $\mu_{M-1}$
\end_inset

, taking care over the limits of integration, and then make a change of
 variable so that this integral has limits 0 and 1.
 By assuming the correct result for 
\begin_inset Formula $C_{M-1}$
\end_inset

 and making use of (2.265), derive the expression for 
\begin_inset Formula $C_{M}$
\end_inset

 .
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
In the discussion below, we let 
\begin_inset Formula $f_{D}(\mu)$
\end_inset

 denote the density function for a Dirichlet distribution whose parameter
 
\begin_inset Formula $\mu$
\end_inset

 is in 
\begin_inset Formula $K$
\end_inset

 dimensional Euclidean space.
 We will use a slightly different approach from the one derived from the
 hint from the book.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
We need to show that 
\begin_inset Formula 
\[
\int_{\mathbb{S}_{K}}f_{D}(\mu)d\mu=\int_{\mathbb{S}_{K}}\frac{\Gamma(\sum_{i=1}^{K}\alpha_{i})}{\prod_{i=1}^{K}\Gamma(\alpha_{i})}\prod_{i=1}^{K-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{K-1}\mu_{i}\right)^{\alpha_{K}-1}d\mu=1,
\]

\end_inset

where 
\begin_inset Formula $\mathbb{S}_{k}:=\{x\in\mathbb{R}^{k}:\sum_{i=1}^{k}x_{k}=1,x_{i}\geq0,i=0,...,k\}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-simplex in Euclidean space.
 Following the idea in Problem 2.5, it suffices for us to show that 
\begin_inset Formula 
\[
I_{\mu}(k):=\int_{\mathbb{S}_{k}}\prod_{i=1}^{k-1}\mu_{i}^{\alpha_{i}}\left(1-\sum_{i=1}^{k-1}\mu_{i}\right)^{\alpha_{k}-1}d\mu=\frac{\prod_{i=1}^{k}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{k}\alpha_{i})},
\]

\end_inset

for any 
\begin_inset Formula $\mathbb{N}\ni k\geq2.$
\end_inset

 We prove this using inducting on 
\begin_inset Formula $k$
\end_inset

.
 For the base case 
\begin_inset Formula $k=2$
\end_inset

, note 
\begin_inset Formula 
\begin{align*}
I_{\mu}(2) & =\int_{\{\mu\in\mathbb{R}^{2}:\mu_{1}+\mu_{2}=1,\mu_{1}\geq0,\mu\geq0\}}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu\\
 & =\int_{\{\mu\in\mathbb{R}^{2}:\mu_{1}\times\mu_{2}\in[0.1]\times[0,1]\}}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu\tag{1}\\
 & =\int_{0}^{1}d\mu_{2}\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}(1-\mu_{1})^{\alpha_{2}-1}d\mu_{1}\tag{by Fubini's theorem }\\
 & =\frac{\Gamma(a_{1})\Gamma(\alpha_{2})}{\Gamma(\alpha_{1}+\alpha_{2})}.
\end{align*}

\end_inset

where Eq.(1) follows from the observation that for any 
\begin_inset Formula $\mathbb{N}\ni k\geq2$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\mathbb{S}_{k} & =\left\{ x\in\mathbb{R}^{k}:\sum_{i=1}^{k-1}x_{i}=1-x_{k},x_{k}\in[0,1],x_{i}\geq0,i=1,...,k\right\} \\
 & =\left\{ x\in\mathbb{R}^{k}:\sum_{i=1}^{k-1}x_{i}\leq1,x_{k}\in[0,1],x_{i}\geq0,i=1,...,k-1\right\} ,
\end{align*}

\end_inset

where the equality can be verified by an element trace.
 Now assume the claim is true for 
\begin_inset Formula $k=n.$
\end_inset

 Before going into the inductive step, we carefully formulate the inductive
 hypothesis: note that 
\begin_inset Formula 
\begin{align*}
I_{\mu}(n) & =\int_{\mathbb{S}_{n}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\int_{\left\{ \mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{n}\in[0,1],\mu_{1\leq i\leq n-1}\geq0\right\} }\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\int_{0}^{1}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d\mu_{n}d(\times_{i=1}^{n-1}\mu_{i})\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\int_{0}^{1}d\mu_{n}\\
 & =\int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=1}^{n-1}\mu_{i}\leq1,\mu_{1\leq i\leq n-1}\in[0,1]\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
 & =\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{i}}\mu_{2}^{\alpha_{2}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu_{n-1}d\mu_{n-2}\cdots d\mu_{1}\tag{2}\\
 & =\frac{\prod_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma\left(\sum_{i=1}^{n}\alpha_{i}\right)}
\end{align*}

\end_inset

for any 
\begin_inset Formula $\{\alpha_{1},\ldots,\alpha_{n}\}$
\end_inset

 s.t.
 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}=1.$
\end_inset

 Also note that Eq.(2) follows from repeated application of Fubini's theorem
 in the following way: 
\begin_inset Formula 
\begin{align*}
 & \text{Eq.(2)}\\
= & \int_{\{\mu\in\mathbb{R}^{n}:\sum_{i=2}^{n-1}\mu_{i}\leq1-\mu_{1},\mu_{1}\in[0,1],\mu_{2\leq i\leq n-1}\geq0\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{\{(\mu_{2},\dots,\mu_{n})\in\mathbb{R}^{n-1}:\sum_{i=2}^{n-1}\mu_{i}\leq1-\mu_{1},\mu_{2\leq n-1}\geq0\}}\prod_{i=2}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=1}^{n-1}\mu_{i})\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{\big\{\stackrel{(\mu_{2},\dots,\mu_{n})\in\mathbb{R}^{n-1}:\sum_{i=3}^{n-1}\mu_{i}\leq1-\mu_{1}-\mu_{2}}{\mu_{3\leq i\leq n-1}\geq0,\mu_{2}\in[0,1-\mu_{1}]}\big\}}\prod_{i=2}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=2}^{n-1}\mu_{i})d\mu_{1}\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{1}}\mu_{2}^{\alpha_{2}-1}\int_{\big\{\stackrel{(\mu_{3},\dots,\mu_{n})\in\mathbb{R}^{n-2}:\sum_{i=3}^{n-1}\mu_{i}\leq1-\mu_{1}-\mu_{2},}{\mu_{3\leq i\leq n-1}\geq0}\big\}}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}}d(\times_{i=3}^{n-1}\mu_{i})d\mu_{2}d\mu_{1}\\
\cdots\\
= & \int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{i}}\mu_{2}^{\alpha_{2}-2}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu_{n-1}d\mu_{n-2}\cdots d\mu_{1}\tag{2}
\end{align*}

\end_inset

We also prove a lemma to facilitate the inductive step.
 
\end_layout

\begin_layout Lemma
For any 
\begin_inset Formula $a\in\mathbb{R}-\{0\}$
\end_inset

 and 
\begin_inset Formula $m,n>0,$
\end_inset

 the following integral identity holds: 
\begin_inset CommandInset label
LatexCommand label
name "lem: problem2.9 lem1"

\end_inset


\begin_inset Formula 
\[
\int_{0}^{1}x^{m-1}(1-x)^{n-1}dx=\frac{1}{a^{m+n-1}}\int_{0}^{a}y^{m-1}(a-y)^{n-1}dy,
\]

\end_inset

and as a result 
\begin_inset Formula 
\[
\int_{0}^{a}y^{m-1}(a-y)^{n-1}=a^{m+n-1}\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}
\]

\end_inset


\end_layout

\begin_layout Proof
By change of variable 
\begin_inset Formula $x=y/a,$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\int_{0}^{1}x^{m-1}(1-x)^{n-1}dx & =\frac{1}{a}\int_{0}^{a}\left(\frac{y}{a}\right)^{m-1}\left(1-\frac{y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}a^{m+n-2}\left(\frac{y}{a}\right)^{m-1}\left(1-\frac{y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}a^{m-1}\left(\frac{y}{a}\right)^{m-1}a^{n-1}\left(\frac{a-y}{a}\right)^{n-1}dy\\
 & =\frac{1}{a^{m+n-1}}\int_{0}^{a}y^{m-1}(a-y)^{n-1}dy.
\end{align*}

\end_inset

That 
\begin_inset Formula $\int_{0}^{a}y^{m-1}(a-y)^{n-1}=\frac{1}{a^{m+n-1}}\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$
\end_inset

 then directly follows from Problem 2.5.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Then for 
\begin_inset Formula $k=n+1,$
\end_inset

 again by repeated application of Fubini's theorem we have 
\begin_inset Formula 
\begin{align*}
I_{\mu}(n+1) & =\int_{\mathbb{S}_{n+1}}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu\\
 & =\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\int_{0}^{1-\mu_{1}}\mu_{2}^{\alpha_{2}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu_{n}d\mu_{n-1}\cdots d\mu_{1}.\tag{3}
\end{align*}

\end_inset

Note that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: problem2.9 lem1"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset Formula 
\begin{align*}
 & \int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n}\mu_{i}\right)^{\alpha_{n+1}-1}d\mu_{n}\\
=\ \  & \int_{0}^{1-\sum_{i=1}^{n-1}\mu_{i}}\mu_{n}^{\alpha_{n}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}-\mu_{n}\right)^{\alpha_{n+1}-1}d\mu_{n}\\
=\ \  & \frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+\alpha_{n+1}-1}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula 
\begin{align*}
\text{Eq.(3)} & =\frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\int_{0}^{1}\mu_{1}^{\alpha_{1}-1}\cdots\int_{0}^{1-\sum_{i=1}^{n-2}\mu_{i}}\mu_{n-1}^{\alpha_{n-1}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+\alpha_{n+1}-1}d\mu_{n-1}\cdots d\mu_{1}\\
 & =\frac{\Gamma(\alpha_{n})\Gamma(\alpha_{n+1})}{\Gamma(\alpha_{n}+\alpha_{n+1})}\frac{\Gamma(\alpha_{1})\Gamma(\alpha_{2})\cdots\Gamma(\alpha_{n}+\alpha_{n+1})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n+1})}\tag{by inductive hypothesis}\\
 & =\frac{\prod_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma\left(\sum_{i=1}^{n}\alpha_{i}\right)}
\end{align*}

\end_inset

as desired.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.10 - Dirichlet distribution's expectation, variance and covariance
\end_layout

\end_inset

 Using the property 
\begin_inset Formula $\Gamma(x+1)=x\Gamma(x)$
\end_inset

 of the gamma function, derive the following results for the mean, and covarianc
e of the Dirichlet distribution given by (2.38) 
\begin_inset Formula 
\begin{align*}
 & \E[\mu_{j}]=\frac{\alpha_{j}}{\alpha_{0}},\\
 & \mathrm{Var}[\mu_{j}]=\frac{\alpha_{j}(\alpha_{0}-\alpha_{j})}{\alpha_{0}^{2}(\alpha_{0}+1)},\\
 & \mathrm{Cov}[\mu_{j}\mu_{l}]=-\frac{\alpha_{j}\alpha_{l}}{\alpha_{0}^{2}(\alpha_{0}+1)},\ \ j\neq l,
\end{align*}

\end_inset

where 
\begin_inset Formula $\alpha_{0}$
\end_inset

 is defined by 
\begin_inset Formula $(2.39)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the discussion below, we let 
\begin_inset Formula $\mu$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-dimensional random vector s.t 
\begin_inset Formula $\mu\sim\mathrm{Dir}(\alpha)$
\end_inset

 and 
\begin_inset Formula $\mathbb{S}_{n}$
\end_inset

 denote the standard simplex in 
\begin_inset Formula $\mathbb{R}^{n}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To find the expectation, note that 
\begin_inset Formula 
\begin{align*}
\E[\mu_{j}] & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\mu_{j}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu\\
 & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+1-1}d\mu & \text{if }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i\geq1,i\neq j}^{n-1}\mu_{j}^{\alpha_{i}-1}\mu_{j}^{\alpha_{j}+1-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }j\in\{1,...,n-1\}
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}\frac{\prod_{i\geq1,i\neq j}^{n-1}\Gamma(\alpha_{i})\Gamma(\alpha_{j}+1)}{\Gamma\left((\sum_{i=1}^{n}\alpha_{i})+1\right)}\\
 & =\frac{\alpha_{j}}{\sum_{i=1}^{n}\alpha_{i}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, note that using the same argument as in part(1), 
\begin_inset Formula 
\begin{align*}
\E[\mu_{j}^{2}] & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i=1}^{n-1}\mu_{i}^{\alpha_{i}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+2-1}d\mu & \text{if }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{i\geq1,i\neq j}^{n-1}\mu_{j}^{\alpha_{i}-1}\mu_{j}^{\alpha_{j}+2-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }j\in\{1,...,n-1\}
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}\frac{\prod_{i\geq1,i\neq j}^{n-1}\Gamma(\alpha_{i})\Gamma(\alpha_{j}+2)}{\Gamma\left((\sum_{i=1}^{n}\alpha_{i})+2\right)}\\
 & =\frac{\alpha_{j}(\alpha_{j}+1)}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}.
\end{align*}

\end_inset

Hence, 
\begin_inset Formula 
\begin{align*}
\text{Var}[\mu_{j}] & =\E[\mu_{j}^{2}]-(\E[\mu_{j}])^{2}\\
 & =\frac{\alpha_{j}(\alpha_{j}+1)}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}-\frac{\alpha_{j}^{2}}{(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{(\sum_{i=1}^{n}\alpha_{i})\alpha_{j}(\alpha_{j}+1)-(\sum_{i=1}^{n}\alpha_{i}+1)\alpha_{j}^{2}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{\alpha_{j}(\sum_{i=1}^{n}\alpha_{i}-\alpha_{j})}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
To find the covariance, note that 
\begin_inset Formula 
\begin{align*}
\E[\mu_{i}\mu_{j}] & =\begin{cases}
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{k\geq1,k\notin\{i,j\}}^{n-1}\mu_{k}^{\alpha_{i}-1}\mu_{i\in\{i,j\}}^{\alpha_{i\in\{i,j\}}-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}+1-1}d\mu & \text{if }i=n\text{ or }j=n\\
\int_{\mathbb{S}_{n}}\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\prod_{k\geq1,k\neq i,j}^{n-1}\mu_{k}^{\alpha_{i}-1}\mu_{i}^{\alpha_{i}+1-1}\mu_{j}^{\alpha_{j}+1-1}\left(1-\sum_{i=1}^{n-1}\mu_{i}\right)^{\alpha_{n}-1}d\mu & \text{if }i\neq n\text{ and }j\neq n
\end{cases}\\
 & =\frac{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{n})}{\Gamma(\alpha_{1}+\cdots+\alpha_{n})}\frac{\prod_{k\geq1,k\neq i,j}^{n}\Gamma(\alpha_{k})\Gamma(\alpha_{i}+1)\Gamma(\alpha_{j}+1)}{\Gamma(\sum_{i=k}^{n}\alpha_{k}+2)}\\
 & =\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}.
\end{align*}

\end_inset

Therefore, it follows that 
\begin_inset Formula 
\begin{align*}
\text{Cov}[\mu_{i},\mu_{j}] & =\E[\mu_{i}\mu_{j}]-\E[\mu_{i}]\E[\mu_{j}]\\
 & =\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})}-\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =\frac{\alpha_{i}\alpha_{j}(\sum_{i=1}^{n}\alpha_{i})-(\sum_{i=1}^{n}\alpha_{i}+1)\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}\\
 & =-\frac{\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}\alpha_{i}+1)(\sum_{i=1}^{n}\alpha_{i})^{2}}.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.11 - Expression for 
\begin_inset Formula $\mathbb{E}[\log\mathrm{Dir}(\alpha)]$
\end_inset


\end_layout

\end_inset

 By expressing the expectation of 
\begin_inset Formula $\ln\mu_{j}$
\end_inset

 under the Dirichlet distribution (2.38) as a derivative with respect to
 
\begin_inset Formula $\alpha_{j}$
\end_inset

, show that 
\begin_inset Formula 
\[
\E[\ln\mu_{j}]=\psi(\alpha_{j})-\psi(\alpha_{0})
\]

\end_inset

where 
\begin_inset Formula $\alpha_{0}$
\end_inset

 is given by (2.39) and 
\begin_inset Formula 
\[
\psi(a)\equiv\frac{d}{da}\ln\Gamma(a)
\]

\end_inset

is the digamma function.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-dimensional random vector such that 
\begin_inset Formula $X\sim\mathrm{Dir}(\alpha).$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\shape default
Note that for 
\begin_inset Formula $(\mu_{1},\ldots,\mu_{n})$
\end_inset

 in the 
\begin_inset Formula $n$
\end_inset

-dimensional standard simplex, we have 
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\alpha_{j}}\left[\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\right] & =\ln\mu_{j}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}.
\end{align*}

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\E[\ln\mu_{j}] & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\ln\mu_{j}\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}d\mu\\
 & =\int_{\mathbb{S}_{n}}\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\left[\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}\right]d\mu\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\int\prod_{i=1}^{n}\mu_{i}^{\alpha_{i}-1}d\mu\tag{by Leibniz rule}\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}\frac{\partial}{\partial\alpha_{j}}\left[\frac{\Pi_{i=1}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\right]\tag{(1)}.
\end{align*}

\end_inset

Now we simplify Eq.(1), 
\begin_inset Formula 
\begin{align*}
\text{Eq.(1)} & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\prod_{i=1}^{n}\Gamma(\alpha_{i})}\left[\frac{\prod_{i\geq1,i\neq j}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})^{2}}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial\alpha_{j}}\right]\\
 & =\frac{\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\prod_{i=1}^{n}\Gamma(\alpha_{i})}\Biggr[\frac{\prod_{i\geq1,i\neq j}^{n}\Gamma(\alpha_{i})}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})^{2}}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial(\sum_{i=1}^{n}\alpha_{i})}\underbrace{\frac{\partial(\sum_{i=1}^{n}\alpha_{i})}{\partial\alpha_{j}}}_{=1}\Biggr]\\
 & =\frac{1}{\Gamma(\alpha_{j})}\frac{\partial\Gamma(\alpha_{j})}{\partial\alpha_{j}}-\frac{1}{\Gamma(\sum_{i=1}^{n}\alpha_{i})}\frac{\partial\Gamma(\sum_{i=1}^{n}\alpha_{i})}{\partial(\sum_{i=1}^{n}\alpha_{i})}\\
 & =\frac{\partial}{\partial\alpha_{j}}\ln\Gamma(\alpha_{j})-\frac{\partial}{\partial(\sum_{i=1}^{n}\alpha_{i})}\ln\Gamma\bigg(\sum_{i=1}^{n}\alpha_{i}\bigg).
\end{align*}

\end_inset

Hence, 
\begin_inset Formula $\E[\ln\mu_{j}]=\psi(\alpha_{j})-\psi(\sum_{i=1}^{n}\alpha_{i})$
\end_inset

 as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.12 - Uniform distribution's normalization, expectation, variance
 
\end_layout

\end_inset

 The uniform distribution for a continuous variable 
\begin_inset Formula $X$
\end_inset

 is defined by 
\begin_inset Formula 
\[
\mathrm{U}(x\vert a,b)=\frac{1}{b-a},\ a\leq x\leq b.
\]

\end_inset

Verify that this distribution is normalized, and find expressions for its
 mean and variance.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
In the discussion below, the 
\begin_inset Formula $X$
\end_inset

 be a random variable such that 
\begin_inset Formula $X\sim\text{Uniform}(a,b)$
\end_inset

 with 
\begin_inset Formula $f_{X}(x)=\frac{1}{b-a}\mathbbm{1}_{[a,b]}.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 To see the normalization, note that 
\begin_inset Formula 
\[
\int\frac{1}{b-a}\mathbbm{1}_{[a,b]}dx=\frac{1}{b-a}(b-a)=1.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the expectation, note 
\begin_inset Formula 
\[
\E[X]=\int_{a}^{b}x\frac{1}{b-a}dx=\left[\frac{x^{2}}{2}\right]_{a}^{b}(b-a)=\frac{b^{2}-a^{2}}{2}\frac{1}{(b-a)}=\frac{a+b}{2}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
To find the variance, first we note that 
\begin_inset Formula 
\begin{align*}
\E[X^{2}] & =\int_{a}^{b}x^{2}\frac{1}{b-a}dx=\left[\frac{x^{3}}{3}\right]_{a}^{b}(b-a)=\frac{b^{3}-a^{3}}{3}\frac{1}{(b-a)}\\
 & =\frac{(b-a)(a^{2}+b^{2}+ab)}{3(b-a)}=\frac{a^{2}+b^{2}+ab}{3}.
\end{align*}

\end_inset

 And thus 
\begin_inset Formula 
\begin{align*}
\text{Var}[X] & =\E[X^{2}]-(\E[X])^{2}=\frac{a^{2}+b^{2}+ab}{3}-\left(\frac{a+b}{2}\right)^{2}\\
 & =\frac{4a^{2}+4b^{2}+4ab-3a^{2}-3b^{2}-6ab}{12}\\
 & =\frac{a^{2}+b^{2}-2ab}{12}\\
 & =\frac{(a-b)^{2}}{12}
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.14 - Multidimensional gaussian maximizes entropy 
\end_layout

\end_inset

 This exercise demonstrates that the multivariate distribution with maximum
 entropy, for a given covariance, is a Gaussian.
 The entropy of a distribution 
\begin_inset Formula $p(x)$
\end_inset

 is given by 
\begin_inset Formula 
\[
H(X)=-\int p(x)\ln p(x)dx.
\]

\end_inset

We with to maximize 
\begin_inset Formula $H(X)$
\end_inset

 over all distribution 
\begin_inset Formula $p(x)$
\end_inset

 subject to the constraints that 
\begin_inset Formula $p(x)$
\end_inset

 be normalized and that it have specific mean and covariance, so that 
\begin_inset Formula 
\begin{align*}
 & \int p(x)dx=1,\\
 & \int p(x)xdx=\mu,\\
 & \int p(x)(x-\mu)(x-\mu)^{T}dx=\Sigma.
\end{align*}

\end_inset

By performing a variational maximization of (2.279) and using Lagrange multiplier
s to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum
 likelihood distribution is given by the Gaussian (2.43).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we write out the Lagrangian 
\begin_inset Formula 
\begin{align*}
\mathcal{L}(p(x))= & -\int p(x)\ln p(x)dx+\left\langle \begin{bmatrix}\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}
\end{bmatrix},\begin{bmatrix}\int p(x)dx-1\\
\int p(x)xdx-\mu\\
\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma
\end{bmatrix}\right\rangle _{\text{prod}}\\
= & \int-p(x)\ln p(x)dx\\
 & +\left\langle \lambda_{1},\int p(x)dx-1\right\rangle +\left\langle \lambda_{2},\int p(x)xdx-\mu\right\rangle +\left\langle \lambda_{3},\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right\rangle \\
= & \int-p(x)\ln p(x)dx\\
 & +\lambda_{1}\left(\int p(x)dx-1\right)+\lambda_{2}^{T}\left(\int p(x)xdx-\mu\right)+\text{tr}\left(\lambda_{3}^{T}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)\\
= & \int-p(x)\ln p(x)dx\\
 & +\lambda_{1}\left(\int p(x)dx-1\right)+\lambda_{2}^{T}\left(\int p(x)xdx-\mu\right)+\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)\tag{1}\\
= & \int-p(x)\ln p(x)+\lambda_{1}p(x)+\lambda_{2}^{T}p(x)x+\text{tr}((x-\mu)^{T}\lambda_{3}p(x)(x-\mu))dx-(\lambda_{1}+\lambda_{2}^{T}\mu+\lambda_{3}\Sigma)\\
:= & \int F(p(x)dx+C,\tag{by relabeling}
\end{align*}

\end_inset

where Eq.(1) follow can be justified as follows.
 Note that 
\begin_inset Formula 
\begin{align*}
 & \int p(x)(x-\mu)(x-\mu)^{T}dx\\
=\ \  & \int\begin{bmatrix}p(x)(x_{1}-\mu)(x_{1}-\mu) & p(x)(x_{1}-\mu)(x_{2}-\mu) & \cdots & p(x)(x_{1}-\mu)(x_{n}-\mu)\\
p(x)(x_{2}-\mu)(x_{1}-\mu) & \ddots & \cdots & p(x)(x_{2}-\mu)(x_{n}-\mu)\\
\vdots & \vdots & \cdots & \vdots\\
p(x)(x_{n}-\mu)(x_{1}-\mu) & p(x)(x_{n}-\mu)(x_{2}-\mu) & \cdots & p(x)(x_{n}-\mu)(x_{n}-\mu)
\end{bmatrix}dx\\
=\ \  & \begin{bmatrix}\int p(x)(x_{1}-\mu)(x_{1}-\mu)dx & \int p(x)(x_{1}-\mu)(x_{2}-\mu)dx & \cdots & \int p(x)(x_{1}-\mu)(x_{n}-\mu)dx\\
\int p(x)(x_{2}-\mu)(x_{1}-\mu)dx & \ddots & \cdots & \int p(x)(x_{2}-\mu)(x_{n}-\mu)dx\\
\vdots & \vdots & \cdots & \vdots\\
\int p(x)(x_{n}-\mu)(x_{1}-\mu)dx & \int p(x)(x_{n}-\mu)(x_{2}-\mu)dx & \cdots & \int p(x)(x_{n}-\mu)(x_{n}-\mu)dx
\end{bmatrix},
\end{align*}

\end_inset

which is symmetric, whence by cyclic property of trace it follows that 
\begin_inset Formula 
\begin{align*}
\text{tr}\left(\lambda_{3}^{T}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right) & =\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)^{T}\right)\\
 & =\text{tr}\left(\lambda_{3}\left(\int p(x)(x-\mu)(x-\mu)^{T}dx-\Sigma\right)\right)
\end{align*}

\end_inset

To maximize, we take the functional derivative and set it to zero: 
\begin_inset Formula 
\begin{align*}
 & \frac{\delta L(p(x)}{\delta p(x)}=\frac{\partial F(p(x))}{\partial p(x)}=-\ln p(x)-1+\lambda_{1}+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)=0\\
\implies & p(x)=\exp\{\lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\}.
\end{align*}

\end_inset

Now we substitute 
\begin_inset Formula $p(x)$
\end_inset

 into the constraints:
\begin_inset Formula 
\begin{align*}
\int p(x)xdx= & \int\exp\{\lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\}xdx\\
= & \int\exp\left\{ \left(x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)^{T}\lambda_{3}\left(x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} dx\\
= & \int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} \left(y+\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy\tag{2}\\
= & \int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy\tag{3}\\
 & +\int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} \left(\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy\tag{4}\\
= & \mu,
\end{align*}

\end_inset

where Eq.(2) follows from change of variable 
\begin_inset Formula $y=x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}.$
\end_inset

 We take a closer look at Eq.(2).
 A couple of claims are in order.
 
\end_layout

\begin_layout Lemma
The following identity holds:
\begin_inset Formula 
\[
\int_{\mathbb{R}^{n}}\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy=0,
\]

\end_inset

where 
\begin_inset Formula $y\in\mathbb{R}^{n}.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "lem: odd function in multivariate dimension"

\end_inset


\end_layout

\begin_layout Proof
The key to proving it is that the integrand, denoted as 
\begin_inset Formula $\varphi(y),$
\end_inset

 is a "odd" function in multidimensional space: 
\begin_inset Formula 
\begin{align*}
\varphi(-y) & =\exp\left\{ (-y^{T})\lambda_{3}(-y)-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} (-y)\\
 & =-\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} y\\
 & =-\varphi(y).
\end{align*}

\end_inset

Then note that 
\begin_inset Formula 
\[
\mathbb{R}^{n}=\underbrace{\left(\bigcup_{(\#_{1},\#_{2},\cdots,\#_{n})\in\prod_{i=1}^{n}\{+,-\}}\prod_{i=1}^{n}\mathbb{R}^{\#_{i}}\right)}_{:=P}\bigcup\underbrace{\left(\bigcup_{(\#_{1},\#_{2},\cdots,\#_{n})\in\prod_{i=1}^{n}\{0,1\},\exists\#_{i}=0\ \text{for some}\ i}\prod_{i=1}^{n}\mathbb{R}^{\#_{i}}\right)}_{:=N},
\]

\end_inset

where 
\begin_inset Formula $\mathbb{R}^{+}:=\{x\in\mathbb{R}|x>0\}$
\end_inset

, 
\begin_inset Formula $\mathbb{R}^{-}:=\{x\in\mathbb{R}|x<0\}$
\end_inset

, 
\begin_inset Formula $\mathbb{R}^{0}:=\{0\},$
\end_inset

and 
\begin_inset Formula $\mathbb{R}^{1}:=\mathbb{R}$
\end_inset

.
 Now we can rewrite the integral of interest as 
\begin_inset Formula 
\[
\int_{P\cup N}\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{1}-1\right\} ydy=\int_{N}\varphi(y)dy+\int_{P}\varphi(y)dy.
\]

\end_inset

Since 
\begin_inset Formula $m(N)=0,$
\end_inset

 (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
after " Lemma 3.5"
key "stein2005real"
literal "false"

\end_inset

), 
\begin_inset Formula $\int_{N}\varphi(y)dy=0.$
\end_inset

 On the other hand, since 
\begin_inset Formula $P$
\end_inset

 can be written as 
\begin_inset Formula $2^{n}$
\end_inset

 disjoint unions by definition (note that for 
\begin_inset Formula $(\#_{1},\ldots,\#_{n})\neq(\tilde{\#}_{1},\ldots,\tilde{\#}_{2}),$
\end_inset

 we have (
\begin_inset Formula $\prod_{i=1}^{n}\mathbb{R}^{\#_{i}})\cap(\prod_{i=1}^{n}\mathbb{R}^{\tilde{\#}_{i}})=\emptyset$
\end_inset

), we have that 
\begin_inset Formula 
\[
\int_{P}\varphi(y)dy=\int_{\sqcup_{i=1}^{2^{n}}P_{i}}\varphi(y)dy=\sum_{i=1}^{2^{n}}\int_{P_{i}}\varphi(y)dy.
\]

\end_inset

Since for any 
\begin_inset Formula $i\in\{1,...,2^{n}\},$
\end_inset

 there exists some 
\begin_inset Formula $j\neq i\in\{1,...,2^{n}\}$
\end_inset

 such that 
\begin_inset Formula $P_{i}=(-1)\cdot P_{j}$
\end_inset

, we can rewrite the last term in the previous term as the sum over pairs
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{2^{n}}\int_{P_{i}}\varphi(y)dy & =\sum_{(i,j)}\left(\int_{P_{i}}\varphi(y)dy+\int_{P_{j}}\varphi(y)dy\right)=\sum_{(i,j)}\left(\int_{P_{i}}-\varphi(-y)dy+\int_{P_{j}}\varphi(y)dy\right)\\
 & =\sum_{(i,j)}\left(\int_{-P_{i}}\varphi(-(-x))dx+\int_{P_{j}}\varphi(y)dy\right)\tag{by \ref{thm: transformation thm}}\\
 & =\sum_{(i,j)}\left(-\int_{P_{j}}\varphi(x)dx+\int_{P_{j}}\varphi(y)dy\right)\\
 & =0.
\end{align*}

\end_inset

Therefore, it follows that the desired integral is zero.
 
\begin_inset CommandInset label
LatexCommand label
name "lem: problem-2.14-lem1"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Therefore, we have 
\begin_inset Formula 
\begin{align*}
\text{Eq.(2)} & =\int\exp\left\{ y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}+\lambda_{2}^{T}\mu+\lambda_{2}-1\right\} \left(\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}\right)dy.\tag{5}
\end{align*}

\end_inset

Now we break it down and evaluate Eq.(3) term by term, note that 
\begin_inset Formula 
\[
1=\int\exp\left\{ \lambda_{1}-1+\lambda_{2}^{T}x+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} dx=\int\exp\left\{ \lambda_{1}-1+\lambda_{2}^{T}\mu+y^{T}\lambda_{3}y-\frac{1}{4}\lambda_{2}\lambda_{3}^{-1}\lambda_{2}\right\} dy,
\]

\end_inset

by change of variable 
\begin_inset Formula $y=x-\mu+\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}.$
\end_inset

 Hence, substituting these results back, we get 
\begin_inset Formula 
\[
\text{Eq.(2)}=\mu-\frac{1}{2}\lambda_{3}^{-1}\lambda_{2}=\mu\iff\lambda_{3}^{-1}\lambda_{2}=0\implies\lambda_{2}=0,
\]

\end_inset

where the last implication can be justified as follows: suppose 
\begin_inset Formula $\lambda_{3}=0,$
\end_inset

 then 
\begin_inset Formula $p(x)=\exp\{\lambda_{1}-1+\lambda_{2}^{T}x\}$
\end_inset

 is a constant.
 And thus 
\begin_inset Formula $\int_{\mathbb{R}^{n}}p(x)dx=\infty$
\end_inset

 unless 
\begin_inset Formula $p(x)=0,$
\end_inset

 in which case the integral evaluates to 0, which does not satisfy Eq.(2.280).
 Hence, it follows that 
\begin_inset Formula 
\[
p(x)=\exp\left\{ \lambda_{1}-1+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} .
\]

\end_inset

Now, we substitute back into the last constraint: 
\begin_inset Formula 
\[
\int\exp\left\{ \lambda_{1}-1+(x-\mu)^{T}\lambda_{3}(x-\mu)\right\} (x-\mu)(x-\mu)^{T}dx=\Sigma.
\]

\end_inset

In order to find a solution, we recall that 
\begin_inset Formula 
\[
\int\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right\} (x-\mu)(x-\mu)^{T}dx=\Sigma.
\]

\end_inset

Hence, by comparison of the coefficients, we see that 
\begin_inset Formula $\lambda_{3}=-\frac{1}{2}\Sigma^{-1}$
\end_inset

 and 
\begin_inset Formula 
\[
\exp\left\{ \lambda_{1}-1\right\} =\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\implies\lambda_{1}=\log\left(\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}\right)+1
\]

\end_inset

forms a set of admissible solution.
 With this set of 
\begin_inset Formula $\lambda$
\end_inset

's, we see that the 
\begin_inset Formula $p(x)$
\end_inset

 is the Gaussian density and thus it follows that multivariate Gaussian
 distribution is a minimizer of the calculus of variation program proposed
 in this problem.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.15 - Entropy of multivariate gaussian
\end_layout

\end_inset

 Show that the entropy of the multivariate Gaussian 
\begin_inset Formula $\mathrm{N}(x\vert\mu,\Sigma)$
\end_inset

 is given by 
\begin_inset Formula 
\[
H(X)=\frac{1}{2}\ln\left|\Sigma\right|+\frac{D}{2}(1+\ln(2\pi))
\]

\end_inset

where 
\begin_inset Formula $D$
\end_inset

 is the dimensionality of 
\begin_inset Formula $X$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the discussion below, let 
\begin_inset Formula $X$
\end_inset

 be a random vector such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma)$
\end_inset

 with density 
\begin_inset Formula $\varphi(x\vert\mu,\Sigma)$
\end_inset

.
 First, we give a lemma to be used later.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{R}}(n,m)$
\end_inset

 and 
\begin_inset Formula $B(x)\in\mathrm{Mat}_{\mathbb{R}}(m,n)$
\end_inset

.
 Then the following identity holds:
\begin_inset CommandInset label
LatexCommand label
name "lem: problem 2.15 lem1"

\end_inset


\begin_inset Formula 
\[
\mathrm{tr}\left(\int AB(x)dx\right)=\int\mathrm{tr}(AB(x))dx.
\]

\end_inset


\end_layout

\begin_layout Proof
Just write out the equation and follow definitions: 
\begin_inset Formula 
\begin{align*}
\text{tr}\left(\int AB(x)dx\right) & =\sum_{i=1}^{n}\left(\int AB(x)dx\right)_{ii}=\sum_{i=1}^{n}\left(\int\sum_{j=1}^{n}A_{ij}B_{ij}(x)dx\right)\\
 & =\int\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}B_{ij}(x)dx=\int\text{tr}(AB(x))dx
\end{align*}

\end_inset

as desired.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 Now, we go back to the proof.
 Note that 
\begin_inset Formula 
\begin{align*}
H(X) & =-\int\varphi(x|\mu,\Sigma)\ln\varphi(x|\mu,\Sigma)\\
 & =\int\varphi(x|\mu,\Sigma)\left(\log\frac{1}{(2\pi)^{D/2}\left|\Sigma\right|^{1/2}}+\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)(x-\mu)^{T}\Sigma^{-1}(x-\mu)d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)\text{tr}((x-\mu)^{T}\Sigma^{-1}(x-\mu))d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\int\varphi(x|\mu,\Sigma)\text{tr}(\Sigma^{-1}(x-\mu)(x-\mu)^{T})d\mu\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}\left(\Sigma^{-1}\int\varphi(x|\mu,\Sigma)(x-\mu)(x-\mu)^{T}d\mu\right)\tag{by \ref{lem: problem 2.15 lem1}}\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}(\Sigma^{-1}\Sigma)\\
 & =\log(2\pi)^{D/2}+\log\left|\Sigma\right|^{1/2}+\frac{1}{2}\text{tr}(I_{D})\\
 & =\frac{D}{2}(\log2\pi+1)+\frac{1}{2}\log\left|\Sigma\right|.
\end{align*}

\end_inset

as desired.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status collapsed

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.16 - Entropy of sum of two gaussians
\end_layout

\end_inset

 Consider two random variables 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 having Gaussian distribution with means 
\begin_inset Formula $\mu_{1},\mu_{2}$
\end_inset

 and precisions 
\begin_inset Formula $\tau_{1},\tau_{2}$
\end_inset

 respectively.
 Derive an expression for the differential entropy of the variable 
\begin_inset Formula $X=X_{1}+X_{2}.$
\end_inset

 To do this, first find the distribution of 
\begin_inset Formula $X$
\end_inset

 by using the relation 
\begin_inset Formula 
\[
f(x)=\int_{-\infty}^{\infty}f(x\vert x_{2})f(x_{2})dx_{2}
\]

\end_inset

and completing the square in the exponent.
 Then observe that this represents the convolution of two Gaussian distributions
, which itself will be Gaussian, and finally make use of the result (1.110)
 for the entropy of the univariate Gaussian.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
This problem can be solved in various ways.
 The method proposed by hint given in the problem is limited in the sense
 that it is hard to generalized to arbitrary transformations and requires
 a lot of computation, which is error prone.
 We shall take a different approach here.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

 First, we give a lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random vector such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Then 
\begin_inset Formula $Y=AX+b\sim\mathrm{MVN}(A\mu+b,A\Sigma A^{*})$
\end_inset

 for 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{R}}(m,n)$
\end_inset

 and 
\begin_inset Formula $b\in\mathbb{R}^{m}.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "lem: distribution of linear transformation of gaussian"

\end_inset


\end_layout

\begin_layout Standard
To prove this lemma, we need to develop more theory and give more background
 knowledge about multivariate Gaussian distribution, which we present below.
\end_layout

\begin_layout Subsubsection*
Supplement knowledge 
\end_layout

\begin_layout Standard
In the book, the notion of a Gaussian distribution was mainly introduced
 as a maximizer of a calculus of variation problem under some constraint.
 This is completely valid and useful.
 But the addition of some more auxiliary definitions and results will help
 us gain a more through understanding of this distribution.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

In the discussion below, assume we have derived the univariate normal distributi
on using the book's view point.
 But now we use another route to push the result to the general setting.
 First, a few lemmas.
 
\end_layout

\begin_layout Lemma
The characteristic function for the 
\begin_inset Formula $\mathrm{N}(\mu,\sigma^{2})$
\end_inset

 is given by 
\begin_inset Formula 
\[
\varphi(t)=e^{it\mu}e^{-\frac{1}{2}\sigma^{2}t^{2}}.
\]

\end_inset


\end_layout

\begin_layout Proof
Following the definition, we have 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\E[\exp(ity)]=\int_{\mathbb{R}}\exp(itx)\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)dx\\
 & =\frac{1}{\sqrt{2\pi}\sigma}\int_{\mathbb{R}}\exp(it(y+\mu))\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy\tag{by letting \ensuremath{y=x-\mu}}\\
 & =\frac{1}{\sqrt{2\pi}\sigma}e^{it\mu}\underbrace{\int_{\mathbb{R}}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy}_{:=\phi(t)}.
\end{align*}

\end_inset

In order to find a more explicit form of 
\begin_inset Formula $\widehat{\mu}(t)$
\end_inset

, we evaluate 
\begin_inset Formula $\phi(t).$
\end_inset

 Now note that 
\begin_inset Formula 
\begin{align*}
\left|\frac{\partial}{\partial t}\left(\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right)\right| & =\left|iy\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right|\leq y\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\in L^{1}(\mathbb{R}).
\end{align*}

\end_inset

Then a corollary of DCT, we have 
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial t}\phi(t) & =\int\frac{\partial}{\partial t}\left\{ \exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right\} dy=\int iy\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy\\
 & =\left[-(i\sigma^{2})^{2}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)\right]_{-\infty}^{\infty}-\sigma^{2}t\int_{-\infty}^{\infty}\exp(ity)\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy=-\sigma^{2}t\phi(t).
\end{align*}

\end_inset

Note that this is a first order differential equation.
 Moreover, observe that we also have following initial condition: 
\begin_inset Formula 
\[
\phi(0)=\int_{\mathbb{R}}\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy=\sqrt{2\pi}\sigma\underbrace{\frac{1}{\sqrt{2\pi}\sigma}\int_{\mathbb{R}}\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right)dy}_{=1\text{ since it's gaussian density}}=\sqrt{2\pi}\sigma.
\]

\end_inset

Using integrating factor, we find its general solution if 
\begin_inset Formula $\phi(t)=ce^{-\frac{1}{2}\sigma^{2}t^{2}}.$
\end_inset

 Substituting back into the initial condition, we get that 
\begin_inset Formula $c=\sqrt{2\pi}\sigma$
\end_inset

 and as a result 
\begin_inset Formula $\phi(t)=ce^{-\frac{1}{2}\sigma^{2}t^{2}}.$
\end_inset

 Hence, it follows that 
\begin_inset Formula $\varphi(t)=e^{it\mu}e^{-\frac{1}{2}\sigma^{2}t^{2}}$
\end_inset

 as desired.
 
\end_layout

\begin_layout Lemma
The characteristic function for 
\begin_inset Formula $\mathrm{MVN}(\mu,\Sigma)$
\end_inset

 in 
\begin_inset Formula $n$
\end_inset

-dimensional Euclidean space is given by 
\begin_inset CommandInset label
LatexCommand label
name "lem: cf of gaussian"

\end_inset


\begin_inset Formula 
\[
\varphi(t)=\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)
\]

\end_inset


\end_layout

\begin_layout Proof
First, we follow the definition of multivariate characteristic function
 to write 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\int_{\mathbb{R}^{n}}\exp(i\left\langle t,x\right\rangle )\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu\right)\\
 & =\int_{\mathbb{R}^{n}}\exp(i\left\langle t,y+\mu\right\rangle )\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}y^{T}\Sigma^{-1}y\right)\left|\det(J_{\varphi}(y))\right|dy\tag{let \ensuremath{x=\varphi(y)=y+\mu}}\\
 & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\underbrace{\int_{\mathbb{R}^{n}}\exp\left(i\left\langle t,y\right\rangle -\frac{1}{2}y^{T}\Sigma^{-1}y\right)}_{:=I_{1}(t)}dy\tag{1}.
\end{align*}

\end_inset

Note that since 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is symmetric (by Problem 2.22), it follows that 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 has an eigen-decomposition in the form of 
\begin_inset Formula $\Sigma^{-1}=V\Lambda V^{*}$
\end_inset

 and 
\begin_inset Formula $\Lambda=V^{*}\Sigma^{-1}V$
\end_inset

, where 
\begin_inset Formula $\Lambda$
\end_inset

 is a diagonal matrix containing eigen values which are real and 
\begin_inset Formula $V\in\mathrm{O}(n)$
\end_inset

 according to Problem 2.18.
 Now we make a change of variable as follows: 
\begin_inset Formula 
\[
\varphi(x):\mathbb{R}^{n}\rightarrow\mathbb{R}^{n},x\mapsto Vx.\implies\left|\det J_{\varphi}(g)\right|=\left|\det V\right|=1.
\]

\end_inset

Then apply this change of variable to the 
\begin_inset Formula $I_{1}(t)$
\end_inset

 and we get 
\begin_inset Formula 
\begin{align*}
I_{1}(t) & =\int_{\mathbb{R}^{n}}\exp\left(i\left\langle t,Vx\right\rangle -\frac{1}{2}x^{T}V^{T}\Sigma^{-1}Vx\right)dx=\int_{\mathbb{R}^{n}}\exp\left(i\left\langle V^{*}t,x\right\rangle -\frac{1}{2}x^{T}\Lambda x\right)dx.
\end{align*}

\end_inset

Now, we bring this result back to Eq.(1) and get 
\begin_inset Formula 
\begin{align*}
\varphi(t) & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\int_{\mathbb{R}^{n}}\exp\left(i\left\langle s,x\right\rangle -\frac{1}{2}x^{T}\Lambda x\right)dx\tag{where \ensuremath{s=V^{*}t}}\\
 & =\frac{\exp(i\left\langle t,\mu\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\int_{\mathbb{R}^{n}}\exp\left(i\sum_{i=1}^{n}s_{i}x_{i}-\frac{1}{2}\sum_{i=1}^{n}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx\\
 & =\frac{\exp(i\left\langle t,u\right\rangle )}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\prod_{i=1}^{n}\int_{\mathbb{R}^{n}}\exp\left(is_{i}x_{i}-\frac{1}{2}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx_{i}\tag{by Fubini's theroem}\\
 & =\exp(i\left\langle t,u\right\rangle )\prod_{i=1}^{n}\int_{\mathbb{R}^{n}}\exp(its_{i})\frac{1}{\sqrt{2\pi}\lambda_{i}^{1/2}}\exp\left(-\frac{1}{2}\frac{x_{i}^{2}}{\lambda_{i}}\right)dx_{i}\\
 & =\exp(i\left\langle t,u\right\rangle )\prod_{i=1}^{n}\varphi_{X_{i}\sim\mathrm{N}(0,\lambda_{i})}(s_{i})=\exp(i\left\langle t,\mu\right\rangle )\exp\left(\sum_{i=1}^{n}-\frac{\lambda_{i}s_{i}^{2}}{2_{i}}\right)\\
 & =\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}t^{*}\Sigma t\right)=\exp(i\left\langle t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right),
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
Now since set of characteristic functions is an isomorphic to the set of
 probability distributions (cf.???), we can alternatively define Gaussian
 distribution using it's characteristic function.
 One advantage of this characterization is the following lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Then 
\begin_inset Formula $X=_{d}\Sigma^{1/2}Z+\mu,$
\end_inset

 where 
\begin_inset Formula $Z\sim\mathrm{MVN}(0,I).$
\end_inset


\end_layout

\begin_layout Proof
This is a standard result.
 For the sake of completeness, we provide a complete proof here.
 Recall a useful lemma: 
\end_layout

\begin_deeper
\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable.
 Then the characteristic function for 
\begin_inset Formula $AX+b$
\end_inset

 where 
\begin_inset Formula $A\in\mathrm{Mat_{\mathbb{K}}}(n,m)$
\end_inset

 and 
\begin_inset Formula $b\in\mathbb{R}^{m}$
\end_inset

 can be characterized as 
\begin_inset Formula $\varphi_{AX+b}=e^{i\left\langle t,b\right\rangle }\varphi_{X}(A^{*}t).$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: characteristic function of AX+b"

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\end_inset

 Following the definition, we have 
\begin_inset Formula 
\begin{align*}
\varphi_{AX+b}(t) & =\int\exp\left(i\left\langle t,AX+b\right\rangle \right)d\Omega=\exp(i\left\langle t,b\right\rangle )\int\exp(i\left\langle t,Ax\right\rangle )d\Omega=\exp(i\left\langle t,b\right\rangle )\int\exp(i\left\langle A^{*}t,x\right\rangle )d\Omega\\
 & =\exp(i\left\langle t,b\right\rangle \varphi_{X}(A^{*}t),
\end{align*}

\end_inset

as desired.
 
\end_layout

\end_deeper
\begin_layout Proof
Now in view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
\varphi_{\Sigma^{1/2}Z+\mu} & =\exp(i\left\langle t,b\right\rangle )\varphi_{Z}(\Sigma^{1/2}t)=\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma^{1/2}t,\Sigma^{1/2}t\right\rangle \right)\\
 & =\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma^{1/2}\Sigma^{1/2}t,t\right\rangle \right)=\exp(i\left\langle t,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\\
 & =\varphi_{X}(t).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $X=_{d}\Sigma^{1/2}Z+\mu$
\end_inset

 as desired.
 
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\end_inset

 In view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: characteristic function of AX+b"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula 
\begin{align*}
\varphi_{AX+b}(t) & =\exp(i\left\langle t,b\right\rangle )\varphi_{X}(A^{*}t)\\
 & =\exp(i\left\langle t,b\right\rangle )\exp(i\left\langle A^{*}t,\mu\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma A^{*}t,A^{*}t\right\rangle \right)\\
 & =\exp(i\left\langle t,A\mu,b\right\rangle )\exp\left(-\frac{1}{2}\left\langle A\Sigma A^{*}t,t\right\rangle \right)\\
 & =\varphi_{Y\sim\mathrm{MVN}(A\mu+b,A\Sigma A^{*})}(t).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $AX+b=_{d}\mathrm{MVN}(A\mu+b,A\Sigma A^{*})$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Now we go back to the problem itself.
 Instead of 
\begin_inset Formula $x_{1},x_{2}$
\end_inset

, we use 
\begin_inset Formula $X_{1},X_{2}$
\end_inset

 to denote the designated r.v, i.e., 
\begin_inset Formula $X_{1}\sim\mathrm{N}(\mu_{1},\tau_{1}^{-1})$
\end_inset

 and 
\begin_inset Formula $X_{2}\sim\mathrm{N}(\mu_{2},\tau_{2}^{-1})$
\end_inset

 and 
\begin_inset Formula $X_{1}\perp X_{2}.$
\end_inset

 Then it follows that the random vector 
\begin_inset Formula $\widetilde{X}=\begin{bmatrix}X_{1}\\
X_{2}
\end{bmatrix}\sim\mathrm{MVN}\left(\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix},\begin{bmatrix}\tau_{1}^{-1} & 0\\
0 & \tau_{2}^{-1}
\end{bmatrix}\right)$
\end_inset

.
 Note that since 
\begin_inset Formula $X=\mathbf{1}^{T}\widetilde{X}$
\end_inset

 , an application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have that 
\begin_inset Formula 
\[
X\sim\mathrm{N}\left(\mathbf{1}^{T}\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix},\mathbf{1}^{T}\Sigma\mathbf{1}\right)=\mathrm{N}(\mu_{1}+\mu_{2},\tau_{1}^{-1}+\tau_{2}^{-1}).
\]

\end_inset

Then, by Problem 1.35, it follows that 
\begin_inset Formula $H(X_{1}+X_{2})=\frac{1}{2}(1+\ln(2\pi(\tau_{1}^{-1}+\tau_{2}^{-1}))).$
\end_inset


\end_layout

\begin_layout Paragraph
Alternative derivation of gaussian mean, covariance
\end_layout

\begin_layout Standard
In the textbook, the mean and covariance matrix of MVN are derived using
 change of variables techniques when evaluating the integral.
 Since we have mentioned that we can characterize a distribution using its
 characteristic function.
 Naturally it comes the question of deriving moments of random variables
 from their characteristic function.
 Here, we provide a general solution to this problem and apply it to the
 Gaussian case.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable with 
\begin_inset Formula $\E[\left\Vert X\right\Vert ^{N}]<\infty.$
\end_inset

 Then 
\begin_inset Formula 
\[
\mathrm{D}^{\alpha}\varphi_{X}(t)=i^{\left|\alpha\right|}\int X^{\alpha}e^{i\left\langle t,X\right\rangle }d\Omega,
\]

\end_inset

where 
\begin_inset Formula $\alpha=(\alpha_{1},\alpha_{2},...,\alpha_{n})$
\end_inset

 denote the multi-index such that 
\begin_inset Formula $\left|\alpha\right|:=\sum_{i=1}^{n}\alpha_{i}\leq N$
\end_inset

 and 
\begin_inset Formula $X^{\alpha}:=X_{1}^{\alpha_{1}}X_{2}^{\alpha_{2}}\cdots X_{n}^{\alpha_{n}}.$
\end_inset

 As a result, 
\begin_inset Formula 
\[
i^{\left|\alpha\right|}\E[X^{\alpha}]=\mathrm{D}^{\alpha}\varphi_{X}(0).
\]

\end_inset


\end_layout

\begin_layout Proof
To prove this, we induct on 
\begin_inset Formula $N$
\end_inset

.
 Now for the base of 
\begin_inset Formula $N=1,$
\end_inset

 we note that 
\begin_inset Formula $\mathrm{D}^{\alpha}\varphi_{X}(t)$
\end_inset

 then is reduces to 
\begin_inset Formula $\frac{\partial}{\partial t_{i}}\varphi_{X}(t)$
\end_inset

 for some 
\begin_inset Formula $i\in\{1,...,n\}.$
\end_inset

 Note 
\begin_inset Formula $\frac{\partial}{\partial t_{i}}\varphi_{X}(t)=\frac{\partial}{\partial t_{i}}\int e^{i\left\langle t,X\right\rangle }d\Omega$
\end_inset

 and that 
\begin_inset Formula 
\[
\left|\frac{\partial}{\partial t_{i}}\left(\exp(i\left\langle t,X\right\rangle \right)\right|=\left|iX_{i}\exp\left(i\left\langle t,X\right\rangle \right)\right|\leq\left|X_{i}\right|.
\]

\end_inset

We claim that 
\begin_inset Formula $\left|X_{i}\right|\in L^{1}(\Omega).$
\end_inset

 To see this, first we note that by Jensen's inequality 
\begin_inset Formula $(\E[\left\Vert X\right\Vert ])^{N}\leq\E[\left\Vert X\right\Vert ^{N}]<\infty.$
\end_inset

 Take the 
\begin_inset Formula $N$
\end_inset

-th root, we see that 
\begin_inset Formula $\left\Vert X\right\Vert \in L^{1}.$
\end_inset

 Clearly, we have 
\begin_inset Formula $|X_{i}|\leq(\sum_{i=1}^{n}X_{i}^{2})^{1/2}$
\end_inset

.
 Hence, chaining these inequalities shows that 
\begin_inset Formula $\left|X_{i}\right|\in L^{1}(\Omega).$
\end_inset

 Then by a variant of DCT, we can move the differentiation inside and get
 
\begin_inset Formula 
\[
\frac{\partial}{\partial t_{i}}\varphi_{X}(t)=\int\frac{\partial}{\partial t_{i}}\exp(i\left\langle t,X\right\rangle )d\Omega=i\int X_{i}\exp(i\left\langle t,X\right\rangle )d\Omega.
\]

\end_inset

Now assume that the claim holds for 
\begin_inset Formula $N=n-1.$
\end_inset

 Then for 
\begin_inset Formula $N=n,$
\end_inset

 we first note that for some 
\begin_inset Formula $\alpha$
\end_inset

 with 
\begin_inset Formula $\left|\alpha\right|=n,$
\end_inset

 
\begin_inset Formula $\mathrm{D}^{\alpha}\varphi_{X}(t)=\frac{\partial}{\partial t_{i}}(\mathrm{D}^{\beta}\varphi_{X}(t))$
\end_inset

 for some multi-index 
\begin_inset Formula $\beta$
\end_inset

 such that 
\begin_inset Formula $\left|\beta\right|=n-1,$
\end_inset

 and some 
\begin_inset Formula $i\in\{1,...,n\}.$
\end_inset

 Then, we note that first by inductive hypothesis, 
\begin_inset Formula $\mathrm{D}\beta\varphi_{X}(t)=i^{\left|\beta\right|}\int X^{\beta}\exp^{i\left\langle t,X\right\rangle }d\Omega$
\end_inset

 and second, 
\begin_inset Formula 
\[
\left|\frac{\partial}{\partial t_{i}}X^{\beta}\exp(i\left\langle t,X\right\rangle )\right|=\left|iX_{i}X^{\beta}\exp(i\left\langle t,X\right\rangle )\right|\leq\left|X^{\alpha}\right|=\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}.
\]

\end_inset

Now, we claim that 
\begin_inset Formula $\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}\in L^{1}(\Omega)$
\end_inset

.
 To see this we note that for since 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}=N,$
\end_inset

 it follows that 
\begin_inset Formula $\alpha_{i}\leq N.$
\end_inset

 As a result, 
\begin_inset Formula $\left|X_{i}\right|^{\alpha_{i}}\leq\left\Vert X\right\Vert ^{\alpha_{i}}.$
\end_inset

 Therefore, 
\begin_inset Formula $\prod_{i=1}^{n}\left|X_{i}\right|^{\alpha_{i}}\leq\prod_{i=1}^{n}\left\Vert X\right\Vert ^{\alpha_{i}}=\left\Vert X\right\Vert ^{\sum_{i=1}^{n}\alpha_{i}}=\left\Vert X\right\Vert ^{N}\in L^{1}(\Omega).$
\end_inset

 Again, by the variant of DCT, we have 
\begin_inset Formula 
\begin{align*}
\mathrm{D}^{\alpha}\varphi_{X}(t) & =\frac{\partial}{\partial t_{i}}i^{\left|\beta\right|}\int X^{\beta}\exp(i\left\langle t,X\right\rangle )d\Omega=i^{\left|\beta\right|}\int\frac{\partial}{\partial t_{i}}\left(X^{\beta}\exp(i\left\langle t,X\right\rangle )\right)d\Omega\\
 & =i^{\left|\beta\right|}\int iX^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega=i^{\left|\beta\right|+1}\int X^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega\\
 & =i^{\left|\alpha\right|}\int X^{\alpha}\exp(i\left\langle t,X\right\rangle )d\Omega,
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
Now we use this result to derive the mean and covariance of a MVN random
 variable, which we denote as 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma).$
\end_inset

 Recall that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle ).$
\end_inset

 Now we find the Frechet derivative w.r.t 
\begin_inset Formula $t$
\end_inset

: note that by the chain rule:
\begin_inset Formula 
\begin{align*}
\mathrm{D}\varphi_{X}(t) & =\mathrm{D}\left(\exp(t)\right)\circ\left(t\mapsto i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\circ\mathrm{D}\bigg(\underbrace{i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle }_{:=H_{1}(t)}\bigg).
\end{align*}

\end_inset

Note that 
\begin_inset Formula 
\begin{align*}
H_{1}(t+h) & =i\left\langle t+h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma(t+h),t+h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle \Sigma t,h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle t,\Sigma^{*}h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+i\left\langle h,\mu\right\rangle -\left\langle \Sigma h,t\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle \\
 & =H_{1}(t)+\left\langle i\mu-\Sigma t,h\right\rangle -\frac{1}{2}\left\langle \Sigma h,h\right\rangle .
\end{align*}

\end_inset

Since 
\begin_inset Formula $\left\langle \Sigma h,h\right\rangle \leq\left\Vert \Sigma\right\Vert _{\infty}\left\Vert h\right\Vert ^{2}\rightarrow0$
\end_inset

 at 
\begin_inset Formula $\left\Vert h\right\Vert \rightarrow0,$
\end_inset

 it follows that 
\begin_inset Formula $\frac{1}{2}\left\langle \Sigma h,h\right\rangle =o(\left\Vert h\right\Vert ).$
\end_inset

 As 
\begin_inset Formula $h\mapsto\left\langle i\mu-\Sigma t,h\right\rangle \in\text{Hom}(\mathbb{R}^{n},\mathbb{R}),$
\end_inset

 it follows that 
\begin_inset Formula $\mathrm{D}H_{1}(t)=i\mu-\Sigma t.$
\end_inset

 Since 
\begin_inset Formula $\mathrm{D}(\exp(t))=\exp(t)$
\end_inset

 by elementary calculus, it follows that 
\begin_inset Formula 
\[
\mathrm{D}\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle )(i\mu-\Sigma t).
\]

\end_inset

Therefore, 
\begin_inset Formula $\E[X]=\mathrm{D}\varphi_{X}(0)=i^{-1}i\mu=\mu.$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Now to find the covariance, we can either partial differentiate term by
 term or use the same notion of Frechet derivative.
 We adopt the second method since it is consistent with our previous method,
 and also yields the total derivative in its matrix form directly, which
 is more elegant than piecing together terms.
 Before we go into the calculation, we prepare ourselves with a tool to
 facilitate the calculation - generalized product rule.
 
\end_layout

\begin_layout Lemma
Suppose the mapping 
\begin_inset Formula $B:X_{1}\times X_{2}\rightarrow Y$
\end_inset

 is bilinear and bounded, i.e., 
\begin_inset Formula 
\[
\left\Vert B(x_{1},x_{2})\right\Vert \leq C\left\Vert x_{1}\right\Vert \left\Vert x_{2}\right\Vert \text{ for all }x_{1}\in X_{1},x_{2}\in X_{2}
\]

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 is fixed and 
\begin_inset Formula $B$
\end_inset

 linear in each argument.
 Suppose further that the maps 
\begin_inset Formula $f_{i}:X\rightarrow X_{i},$
\end_inset

 
\begin_inset Formula $i=1,2$
\end_inset

 are Frechet differentiable at 
\begin_inset Formula $x$
\end_inset

, and there exist and open set 
\begin_inset Formula $U$
\end_inset

 such that 
\begin_inset Formula $x\in U$
\end_inset

 and 
\begin_inset Formula $U\subseteq\mathcal{D}_{f_{i}}.$
\end_inset

 Then the function 
\begin_inset Formula $H(x)=B(f_{1}(x),f_{2}(x))$
\end_inset

 is differentiable at 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset CommandInset label
LatexCommand label
name "lem: prod rule"

\end_inset


\begin_inset Formula 
\[
\mathrm{D}H(x)(h)=B(\mathrm{D}f_{1}(x)(h),f_{2}(x))+B(f_{1}(x),\mathrm{D}f_{2}'(x)(h)).
\]

\end_inset

Note: 
\begin_inset Formula $X_{1},X_{2},X,Y$
\end_inset

 are all assumed to be Banach spaces.
 
\end_layout

\begin_layout Proof
We follow the definition and write out 
\begin_inset Formula $H(x+h)-H(x)$
\end_inset

 for later analysis.
 To facilitate notation, we let 
\begin_inset Formula $f_{i}^{x}:=f_{i}(x)$
\end_inset

 for 
\begin_inset Formula $i=1,2.$
\end_inset

 
\begin_inset Formula 
\begin{align*}
H(x+h)-H(x) & =B(f_{1}^{x+h},f_{2}^{x+h})-B(f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x+h},f_{2}^{x+h})-B(f_{1}^{x+h},f_{2}^{x})+B(f_{1}^{x+h},f_{2}^{x})-B(f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x+h},f_{2}^{x+h}-f_{2}^{x})+B(f_{1}^{x+h}-f_{1}^{x},f_{2}^{x})\\
 & =B(f_{1}^{x}+\mathrm{D}f_{1}^{x}(h)+\left\Vert h\right\Vert r_{1}(h),\mathrm{D}f_{2}^{x}(h)+\left\Vert h\right\Vert r_{2}(h))+B(\mathrm{D}f_{1}^{x}(h)+\left\Vert h\right\Vert r_{1}(h),f_{2}^{x})\\
 & =T_{x}(h)+R_{x}(h),
\end{align*}

\end_inset

where 
\begin_inset Formula 
\[
\begin{cases}
\begin{split}T_{x}(h)= & B(f_{1}^{x},\mathrm{D}f_{2}^{x}(h))+B(\mathrm{D}f_{1}^{x}(h),f_{2}^{x})\\
R_{x}(h)= & B(f_{1}^{x},\left\Vert h\right\Vert r_{2}(h))+B(\mathrm{D}f_{1}^{x}(h),\mathrm{D}f_{2}^{x}(h))+B(\mathrm{D}f_{1}^{x}(h),\left\Vert h\right\Vert \\
 & +B(\left\Vert h\right\Vert r_{1}(h),\left\Vert h\right\Vert r_{2}(h))+B(\left\Vert h\right\Vert r_{1}(h),f_{2}^{x}).
\end{split}
r_{2}(h))+B(\left\Vert h\right\Vert _{1}r_{1}(h),\mathrm{D}f_{2}^{x}(h))\end{cases}
\]

\end_inset

In order to show that 
\begin_inset Formula $T_{x}(h)=\mathrm{D}H(x)\circ h.$
\end_inset

 We first need to show that 
\begin_inset Formula $T(h)\in\mathrm{Hom}(X,Y).$
\end_inset

 Indeed, 
\begin_inset Formula 
\begin{align*}
T_{x}(\alpha h+\beta g) & =B(f_{1}^{x},\mathrm{D}f_{2}(\alpha h+\beta g))+B(\mathrm{D}f_{1}^{x}(\alpha h+\beta g),f_{2}^{x})\\
 & =\alpha B(f_{1}^{x},\mathrm{D}f_{2}(h))+\beta B(f_{1}^{x},\mathrm{D}f_{2}(g))+\alpha B(\mathrm{D}f_{1}^{x}(h),f_{2}^{x})+\beta B(\mathrm{D}f_{1}^{x}(g),f_{2}^{x})\\
 & =\alpha T_{x}(h)+\beta T_{x}(g).
\end{align*}

\end_inset

Next, we need to show that 
\begin_inset Formula $R_{x}(h)=o(\left\Vert h\right\Vert ).$
\end_inset

 We analyze 
\begin_inset Formula $R_{x}(h)$
\end_inset

 term by term as follows 
\begin_inset Formula 
\[
\begin{cases}
\begin{alignedat}{1} & B(f_{1}^{x},\left\Vert h\right\Vert r_{2}(h))\leq C\left\Vert f_{1}^{x}\right\Vert \left\Vert h\right\Vert \left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\mathrm{D}f_{1}^{x}(h),\mathrm{D}f_{2}^{x}(h))\leq C\left\Vert \mathrm{D}f_{1}^{x}\right\Vert \left\Vert \mathrm{D}f_{2}^{x}\right\Vert \left\Vert h\right\Vert ^{2}=o(\left\Vert h\right\Vert )\\
 & B(\mathrm{D}f_{1}^{x}(h),\left\Vert h\right\Vert r_{2}(h)\leq C\left\Vert \mathrm{D}f_{1}^{x}\right\Vert \left\Vert h\right\Vert ^{2}\left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert _{1}r_{1}(h),\mathrm{D}f_{2}^{x}(h))\leq C\left\Vert \mathrm{D}f_{2}^{x}\right\Vert \left\Vert h\right\Vert ^{2}\left\Vert r_{1}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert r_{1}(h),\left\Vert h\right\Vert r_{2}(h))\leq C\left\Vert h\right\Vert ^{2}\left\Vert r_{1}(h)\right\Vert \left\Vert r_{2}(h)\right\Vert =o(\left\Vert h\right\Vert )\\
 & B(\left\Vert h\right\Vert r_{1}(h),f_{2}^{x})\leq C\left\Vert f_{2}^{x}\right\Vert \left\Vert h\right\Vert \left\Vert r_{1}(h)\right\Vert =o(\left\Vert h\right\Vert ).
\end{alignedat}
.\end{cases}
\]

\end_inset

Since 
\begin_inset Formula $R_{x}(h)$
\end_inset

 is the sum of these terms, it follows that 
\begin_inset Formula $R_{x}(h)=o(\left\Vert h\right\Vert )$
\end_inset

.
 Hence, it follows that 
\begin_inset Formula $T_{x}(h)=\mathrm{D}H(x)\circ h.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Now, observe that 
\begin_inset Formula $\mathrm{D}\varphi_{X}(t)=\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle )(i\mu-\Sigma t)=B(\exp(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle ),i\mu-\Sigma t),$
\end_inset

 where 
\begin_inset Formula $B\in\mathrm{Hom}(\mathbb{R},\mathbb{R}^{n};\mathbb{R}^{n})$
\end_inset

 is defined by 
\begin_inset Formula $(x,y)\mapsto xy.$
\end_inset

 Hence, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: prod rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it follows that 
\begin_inset Formula 
\begin{align*}
\mathrm{D}^{2}\varphi_{X}(t)(h) & =B\left(\left\langle \exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)(i\mu-\Sigma t),h\right\rangle ,i\mu-\Sigma t\right)+B\left(\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right),-\Sigma h\right)\\
 & =\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)(i\mu-\Sigma t)(i\mu-\Sigma t)^{T}h-\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)\Sigma h\\
 & =\exp\left(i\left\langle t,\mu\right\rangle -\frac{1}{2}\left\langle \Sigma t,t\right\rangle \right)((i\mu-\Sigma t)(i\mu-\Sigma t)^{T}h-\Sigma h).
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula 
\[
\mathrm{D}\varphi_{X}(0)h=(i^{2}\mu\mu^{T}-\Sigma)h=(-\mu\mu^{T}-\Sigma)h\implies\E[XX^{T}]=-\mathrm{D}^{2}\varphi_{X}(0)=\mu\mu^{T}+\Sigma.
\]

\end_inset

 As a result, we have according to definition the covariance matrix is 
\begin_inset Formula $\E[XX^{T}]-\mu\mu^{T}=\Sigma.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.17 - Suffices to assume the parameter 
\begin_inset Formula $\Sigma$
\end_inset

 in Gaussian to be symmetric
\end_layout

\end_inset

 Consider the multivariate Gaussian distribution given by (2.43).
 By writing the precision matrix (inverse covariance matrix) 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 as the sum of a symmetric and an anti-symmetric matrix, show that the anti-symm
etric term does not appear in the exponent of the Gaussian, and hence that
 the precision matrix may be taken to be symmetric without loss of generality.
 Because the inverse of a symmetric matrix is also symmetric (see Exercise
 2.22), it follows that the covariance matrix may also be chosen to be symmetric
 without loss of generality.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is an direct application of Problem 1.14.
 Recall the MVN in 
\begin_inset Formula $n$
\end_inset

-dimensional space has density in the following form:
\begin_inset Formula 
\[
\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right).
\]

\end_inset

Since 
\begin_inset Formula $(x-\mu)^{T}\Sigma(x-\mu)$
\end_inset

 is a bilinear form.
 In problem 1.14, we showed that it suffices to assume 
\begin_inset Formula $\Sigma^{-1}=\Sigma_{S}^{^{-1}}=\frac{1}{2}(\Sigma^{-1}+(\Sigma^{-1})^{T}),$
\end_inset

 which is symmetric since 
\begin_inset Formula $(x-\mu)^{T}\Sigma^{-1}(x-\mu)=(x-\mu)^{T}\Sigma_{S}^{-1}(x-\mu)$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{R}^{n}.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.18 - Eigen-decomposition for symmetric matrices 
\end_layout

\end_inset

 Consider a real, symmetric matrix Σ whose eigenvalue equation is given
 by (2.45).
 By taking the complex conjugate of this equation and subtracting the original
 equation, and then forming the inner product with eigenvector ui, show
 that the eigenvalues λi are real.
 Similarly, use the symmetry property of Σ to show that two eigenvectors
 ui and uj will be orthogonal provided λj à= λi.
 Finally, show that without loss of generality, the set of eigenvectors
 can be chosen to be orthonormal, so that they satisfy (2.46), even if some
 of the eigenvalues are zero.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before go into the proof, a lemma.
 This lemma is usually known as Gram-Schmidt orthogonalization.
 We prove it in the context of Hilbert space.
 Proof of this result at various level of generality can be found in any
 standard linear algebra textbook.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\mathcal{H}$
\end_inset

 be an Hilbert space.
 Suppose 
\begin_inset Formula $\mathcal{U}=\{u_{1},u_{2},\ldots,u_{n}\}$
\end_inset

 is a set of linearly independent vectors of 
\begin_inset Formula $V.$
\end_inset

 Then there exists a set 
\begin_inset Formula $\mathcal{V=}\{v_{1},v_{2},\ldots,v_{n}\}$
\end_inset

 of elements of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 such that 
\begin_inset CommandInset label
LatexCommand label
name "lem: gram schimt"

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\left\Vert v_{i}\right\Vert =1$
\end_inset

 for 
\begin_inset Formula $1\leq i\leq n.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left\langle v_{i},v_{j}\right\rangle =0$
\end_inset

 for 
\begin_inset Formula $1\leq i,j,\leq n$
\end_inset

 with 
\begin_inset Formula $i\neq j$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathrm{span}(\mathcal{V})=\mathrm{span}(\mathcal{U}).$
\end_inset


\end_layout

\end_deeper
\begin_layout Proof
We prove this constructively.
 The construction goes as follows: first we let 
\begin_inset Formula $v_{1}=\frac{u_{1}}{\left\Vert u_{1}\right\Vert }$
\end_inset

 and 
\begin_inset Formula $v_{2}=\frac{w_{2}}{\left\Vert w_{2}\right\Vert },$
\end_inset

 where 
\begin_inset Formula $w_{2}=u_{2}-\left\langle u_{2},v_{1}\right\rangle v_{1}$
\end_inset

 , and inductively 
\begin_inset Formula $v_{i}=\frac{w_{i}}{\left\Vert w_{i}\right\Vert },$
\end_inset

 where 
\begin_inset Formula $w_{i}=u_{i}-\sum_{j=1}^{i-1}\left\langle u_{i},v_{j}\right\rangle v_{j},$
\end_inset

 assuming 
\begin_inset Formula $v_{1},...,v_{i-1}$
\end_inset

 have already been defined.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

To prove the correctness of our construction, we induct on 
\begin_inset Formula $n.$
\end_inset

 For the base case where 
\begin_inset Formula $k=1.$
\end_inset

 We see that (2) and (3) is trivially satisfied for 
\begin_inset Formula $v_{1}=u_{1}/\left\Vert u_{1}\right\Vert .$
\end_inset

 Also, we have 
\begin_inset Formula $\left\Vert v_{1}\right\Vert =\left\Vert v_{1}\right\Vert /\left\Vert v_{1}\right\Vert =1.$
\end_inset

 Now suppose, the construction yields the desired set of vectors 
\begin_inset Formula $\mathcal{V}_{k=n-1}$
\end_inset

 that satisfies condition (1)-(3) for a given set of vectors 
\begin_inset Formula $\mathcal{U}_{n-1}$
\end_inset

.
 Then for 
\begin_inset Formula $k=n,$
\end_inset

we are given a set of elements in 
\begin_inset Formula $\mathcal{H}$
\end_inset

, 
\begin_inset Formula $\mathcal{U}_{n}=\{u_{1},\ldots,u_{n}\}$
\end_inset

.
 By induction hypothesis, we can construct a set of vectors 
\begin_inset Formula $\mathcal{V}_{n-1}=\{v_{1},\ldots,v_{n-1}\}$
\end_inset

 from the set 
\begin_inset Formula $\mathcal{U}_{n}-\{u_{n}\}$
\end_inset

 that satisfies the conditions (1)-(3) stipulated above with 
\begin_inset Formula $\mathcal{V}=\mathcal{V}_{n-1}$
\end_inset

 and 
\begin_inset Formula $\mathcal{U}=\mathcal{U}_{n-1}$
\end_inset

.
 Now let 
\begin_inset Formula $v_{n}=w_{n}/\left\Vert w_{n}\right\Vert ,$
\end_inset

 where 
\begin_inset Formula $w_{n}=u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}.$
\end_inset

 First, we show that 
\begin_inset Formula $v_{n}$
\end_inset

 is well-defined.
 To prove this, we show that 
\begin_inset Formula $w_{n}\notin\mathrm{span}(\mathcal{V}_{n-1}).$
\end_inset

 Suppose otherwise, then 
\begin_inset Formula $w_{n}=\sum_{i=1}^{n-1}\alpha_{i}v_{n}$
\end_inset

 for some scalars 
\begin_inset Formula $\{\alpha_{i}\}_{i=1}^{n-1}.$
\end_inset

 Then we have 
\begin_inset Formula 
\[
w_{n}=\sum_{i=1}^{n-1}\alpha_{i}v_{i}=u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\implies u_{n}=\sum_{i=1}^{n-1}(\alpha_{i}+\left\langle u_{n},v_{i}\right\rangle )v_{i}\in\mathrm{span}(\mathcal{V}_{n-1}).
\]

\end_inset

Since 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1})=\mathrm{span}(\mathcal{U}_{n-1})$
\end_inset

, it follows that 
\begin_inset Formula $u_{n}\in\mathrm{span}(\mathcal{U}_{n-1}).$
\end_inset

 This is a contradiction since then 
\begin_inset Formula $u_{n}$
\end_inset

 are independent of 
\begin_inset Formula $u_{1},...,u_{n-1}$
\end_inset

 by assumption.
 We claim that 
\begin_inset Formula $\mathcal{V}_{n-1}\cup\{v_{n}\}$
\end_inset

 satisfies conditions (1)-(3) with 
\begin_inset Formula $\mathcal{U}=\mathcal{U}_{n}$
\end_inset

 and 
\begin_inset Formula $\mathcal{V}=\mathcal{V}_{n-1}\cup\{v_{n}\}$
\end_inset

.
 Note that by construction 
\begin_inset Formula $\left\Vert v_{n}\right\Vert =1.$
\end_inset

 And since for 
\begin_inset Formula $j\in\{1,...,n-1\}$
\end_inset


\begin_inset Formula 
\begin{align*}
\left\langle w_{n},v_{j}\right\rangle  & =\left\langle u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i},v_{j}\right\rangle \\
 & =\left\langle u_{n},v_{j}\right\rangle -\sum_{i=1}^{n-1}(\left\langle u_{n},v_{i}\right\rangle \left\langle v_{i},v_{j}\right\rangle )\\
 & =\left\langle u_{n}-v_{j}\right\rangle -\left\langle u_{n}-v_{j}\right\rangle =0,
\end{align*}

\end_inset

it follows that 
\begin_inset Formula $\left\langle w_{n}/\left\Vert w_{n}\right\Vert ,v_{j}\right\rangle =\left\langle v_{n},v_{j}\right\rangle =0$
\end_inset

 for all 
\begin_inset Formula $j=1,...,n$
\end_inset

.
 What's left to prove is that 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\}\}=\mathrm{span}(\mathcal{U}_{n}).$
\end_inset

 Pick 
\begin_inset Formula $\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\})\ni x=\sum_{i=1}^{n}\alpha_{i}v_{i}.$
\end_inset

 If 
\begin_inset Formula $\alpha_{n}=0,$
\end_inset

 then 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{V}_{n-1})=\mathrm{span}(\mathcal{U}_{n-1})\subset\mathrm{span}(\mathcal{U}_{n}).$
\end_inset

 If 
\begin_inset Formula $\alpha_{n}\neq0,$
\end_inset

 then 
\begin_inset Formula 
\begin{align*}
x & =\sum_{i=1}^{n-1}\alpha_{i}v_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }\left(u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right)\\
 & =\sum_{i=1}^{n-1}(\alpha_{i}-\left\langle u_{i},v_{i}\right\rangle )v_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }u_{n}\tag{combining coefficients}\\
 & =\sum_{i=1}^{n-1}\beta_{i}u_{i}+\frac{\alpha_{n}}{\left\Vert u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\right\Vert }u_{n}.
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n})$
\end_inset

.
 On the other hand, suppose 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n}),$
\end_inset

 i.e.
 
\begin_inset Formula $x=\sum_{i=1}^{n}\alpha_{i}u_{i}$
\end_inset

.
 If 
\begin_inset Formula $\alpha_{n}=0,$
\end_inset

 then 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{U}_{n-1})=\mathrm{span}(\mathcal{V}_{n-1})$
\end_inset

 by induction hypothesis.
 If 
\begin_inset Formula $\alpha_{n}\neq0,$
\end_inset

 then we have 
\begin_inset Formula 
\begin{align*}
x & =\sum_{i=1}^{n-1}\alpha_{i}u_{i}+u_{n}=\sum_{i=1}^{n-1}\beta_{i}v_{i}+u_{n}-\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}+\sum_{i=1}^{n-1}\left\langle u_{n},v_{i}\right\rangle v_{i}\\
 & =\sum_{i=1}^{n-1}(\beta_{i}+\left\langle u_{n},v_{i}\right\rangle )v_{i}+v_{n},
\end{align*}

\end_inset

whence 
\begin_inset Formula $x\in\mathrm{span}(\mathcal{V}_{n-1}\cup\{v_{n}\}).$
\end_inset

 
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

 We first show that the eigenvalues are real.
 First, we note that by definition, the pair 
\begin_inset Formula $(\lambda_{i},\mu_{i})$
\end_inset

 is an eigenvector, eigenvalue pair iff 
\begin_inset Formula $\Sigma\mu_{i}=\lambda_{i}\mu.$
\end_inset

 Now we fix one such pair 
\begin_inset Formula $(\mu_{i},\lambda_{i})$
\end_inset

.
 Multiplying 
\begin_inset Formula $\mu_{i}^{*}$
\end_inset

 on the left on both side of the equation yields 
\begin_inset Formula $\mu^{*}\Sigma\mu_{i}=\lambda_{i}\mu_{i}^{*}\mu_{i}.$
\end_inset

 Now since 
\begin_inset Formula $\Sigma$
\end_inset

 is symmetric, it follows that 
\begin_inset Formula $(\mu^{*}\Sigma\mu)^{*}=\mu^{*}\Sigma\mu=\lambda_{i}^{*}\mu_{i}^{*}\mu_{i}.$
\end_inset

 Hence, it follows that 
\begin_inset Formula 
\[
\lambda_{i}^{*}\mu_{i}^{*}\mu_{i}=\lambda_{i}\mu_{i}^{*}\mu_{i}\iff(\lambda_{i}^{*}-\lambda_{i})\mu_{i}^{*}\mu_{i}=0\iff\lambda_{i}^{*}=\lambda_{i},
\]

\end_inset

since we assume 
\begin_inset Formula $\mu_{i}\neq0.$
\end_inset

 Therefore, 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is real.
 Since 
\begin_inset Formula $(\mu_{i},\lambda_{i})$
\end_inset

 is chosen to be arbitrary, it follows that all eigen values in this case
 are real.
 
\end_layout

\begin_layout Enumerate
Next, we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with 
\begin_inset Formula $\lambda_{i}\neq\lambda_{j},$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},\lambda_{j}\neq0$
\end_inset

, we have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0.$
\end_inset

 First, note the following identity: 
\begin_inset Formula 
\[
\lambda_{j}\mu_{j}^{*}\mu_{i}=(\Sigma\mu_{j})^{*}\mu_{i}=(\Sigma^{*}\mu_{j})^{*}\mu_{i}=\mu_{j}^{*}\Sigma\mu_{i}=\mu_{j}^{*}\lambda_{i}\mu_{i}=\lambda_{i}\mu_{j}^{*}\mu_{i},
\]

\end_inset

which implies 
\begin_inset Formula $\lambda_{i}\mu_{j}^{*}\mu_{i}=\lambda_{j}\mu_{j}^{*}\mu_{i}\Leftrightarrow\mu_{j}^{*}\mu_{i}(\lambda_{i}-\lambda_{j})=0.$
\end_inset

 Since 
\begin_inset Formula $\lambda_{i}\neq\lambda_{j}$
\end_inset

 by assumption, it follows that 
\begin_inset Formula $\left\langle \mu_{j},\mu_{i}\right\rangle =\mu_{j}^{*}\mu_{i}=0$
\end_inset

, as desired.
 
\end_layout

\begin_layout Enumerate
Now we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with 
\begin_inset Formula $\lambda_{i}=\lambda_{j}$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},\lambda_{j}\neq0,$
\end_inset

 we can still have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0$
\end_inset

.
 For better notation, suppose 
\begin_inset Formula $\lambda_{1}=\lambda_{2}:=\lambda\in\mathbb{R}.$
\end_inset

 First, we show that any linear combination of 
\begin_inset Formula $\mu_{i}$
\end_inset

 and 
\begin_inset Formula $\mu_{j}$
\end_inset

 is also an eigen vector with the same eigen value, i.e., 
\begin_inset Formula $(\lambda,\alpha\mu_{i}+\beta\mu_{2})$
\end_inset

 is a valid eigen pair as well for any 
\begin_inset Formula $\alpha,\beta\in\mathbb{R}-\{0\}.$
\end_inset

 Indeed, we have 
\begin_inset Formula 
\[
\Sigma(\alpha\mu_{i}+\beta\mu_{j})=\alpha\Sigma\mu_{i}+\beta\Sigma\mu_{j}=\lambda\alpha\mu_{i}+\lambda\beta\mu_{j}=\lambda(\alpha\mu_{i}+\beta\mu_{j}).
\]

\end_inset

Therefore, in view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gram schimt"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can orthonormalize 
\begin_inset Formula $\mu_{i}$
\end_inset

 and 
\begin_inset Formula $\mu_{j}$
\end_inset

 to 
\begin_inset Formula $\widetilde{\mu}_{i}$
\end_inset

 and 
\begin_inset Formula $\widetilde{\mu}_{j}$
\end_inset

 such that 
\begin_inset Formula $(\lambda,\tilde{\mu}_{i})$
\end_inset

 and 
\begin_inset Formula $(\lambda,\widetilde{\mu}_{j})$
\end_inset

 are eigen-pairs as well (
\begin_inset Formula $\widetilde{\mu}_{1},\widetilde{\mu}_{2}$
\end_inset

 are linear combination of 
\begin_inset Formula $\mu_{1}$
\end_inset

 and 
\begin_inset Formula $\mu_{2}$
\end_inset

).
 
\end_layout

\begin_layout Enumerate
Now we show that for eigen-pair 
\begin_inset Formula $(\lambda_{i},\mu_{i}),(\lambda_{j},\mu_{j})$
\end_inset

 with at least one of 
\begin_inset Formula $\lambda_{i}$
\end_inset

 and 
\begin_inset Formula $\lambda_{j}$
\end_inset

 being equal to 0, we can still have 
\begin_inset Formula $\left\langle \mu_{i},\mu_{j}\right\rangle =0$
\end_inset

.
 Without loss of generality, suppose 
\begin_inset Formula $\lambda_{i}=0.$
\end_inset

 Then, this means that 
\begin_inset Formula $\Sigma\mu_{i}=0.$
\end_inset

 Now suppose 
\begin_inset Formula $\lambda_{j}\neq0,$
\end_inset

 then we have that 
\begin_inset Formula $\Sigma\mu_{j}=\lambda_{j}\mu_{j}\implies\mu_{j}=\Sigma\mu_{j}/\lambda_{j}.$
\end_inset

 Then it follows that 
\begin_inset Formula 
\[
\left\langle \mu_{i},\mu_{j}\right\rangle =\frac{1}{\lambda_{j}}\mu_{i}^{T}\Sigma\mu_{j}=\frac{1}{\lambda_{j}}(\Sigma\mu_{i})^{T}\mu_{j}=0.
\]

\end_inset

Suppose otherwise that 
\begin_inset Formula $\lambda_{j}=0.$
\end_inset

 Then 
\begin_inset Formula $\mu_{i},\mu_{j}\in\ker(\Sigma),$
\end_inset

 which is a subspace.
 Therefore, we can orthonormalize 
\begin_inset Formula $\mu_{i},\mu_{j}$
\end_inset

 by applying 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gram schimt"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.19 - Characterization of 
\begin_inset Formula $\Sigma,\Sigma^{-1}$
\end_inset

 in Gaussian distribution
\end_layout

\end_inset

 Show that a real, symmetric matrix 
\begin_inset Formula $\Sigma$
\end_inset

 having the eigenvector equation (2.45) can be expressed as an expansion
 in the eigenvectors, with coefficients given by the eigenvalues, of the
 form (2.48).
 Similarly, show that the inverse matrix 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 has a representation of the form (2.49).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\text{Eq.(2.45)}=\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{*}=U\Lambda U^{*},$
\end_inset

 where 
\begin_inset Formula $U=[u_{i}]$
\end_inset

 are vertical stack of eigen vectors of 
\begin_inset Formula $\Sigma.$
\end_inset

 On the other hand, note that 
\begin_inset Formula $\Sigma U=U\Lambda,$
\end_inset

 which implies 
\begin_inset Formula $\Lambda=U^{*}\Sigma U.$
\end_inset

 Therefore, substitute back we get 
\begin_inset Formula $U\Lambda U^{*}=UU^{*}\Sigma UU^{*}=\Sigma$
\end_inset

 as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.20 - Positive definite has positive eigenvalues
\end_layout

\end_inset

 A positive definite matrix 
\begin_inset Formula $\Sigma$
\end_inset

 can be defined as one for which the quadratic form 
\begin_inset Formula 
\[
a^{T}\Sigma a
\]

\end_inset

is positive for any real value of the vector a.
 Show that a necessary and sufficient condition for 
\begin_inset Formula $\Sigma$
\end_inset

 to be positive definite is that all of the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 of 
\begin_inset Formula $\Sigma$
\end_inset

, defined by (2.45), are positive.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is an important result.
 Hence, we will prove a stronger version by extending the matrix to the
 
\begin_inset Formula $\mathbb{C}.$
\end_inset

 For later use, we pack the result in the following lemma.
\end_layout

\begin_layout Lemma
A matrix 
\begin_inset Formula $M\in\mathrm{Mat}_{\mathbb{C}}(n,n)$
\end_inset

 for some 
\begin_inset Formula $n\in\mathbb{N}$
\end_inset

 is symmetric positive definite
\begin_inset Foot
status open

\begin_layout Plain Layout
Although there are matrices that are postivie definite but not symmetric.
 It suffices for us to assume that 
\begin_inset Formula $M$
\end_inset

 is symmetric in view of Problem 1.14.
 
\end_layout

\end_inset

 if and only if all of the eigen values of 
\begin_inset Formula $M_{i}$
\end_inset

 are positive.
 
\end_layout

\begin_layout Proof
\begin_inset Formula $\Leftarrow$
\end_inset

 Suppose all of the eigen values of 
\begin_inset Formula $M$
\end_inset

, denoted by 
\begin_inset Formula $\{\lambda_{i}\}_{i=1}^{n}$
\end_inset

 are positive.
 Note that in view of Problem 2.18, we have 
\begin_inset Formula $M=V\Lambda V^{*},$
\end_inset

 where 
\begin_inset Formula $V$
\end_inset

 is the matrix containing eigen vectors and 
\begin_inset Formula $\Lambda$
\end_inset

 is the diagonal matrix of eigen values.
 So we have for any 
\begin_inset Formula $x\in\mathbb{C}^{n}$
\end_inset

 that 
\begin_inset Formula $x^{*}Mx=(x^{*}V)\Lambda(V^{*}x)=\sum_{i=1}^{n}s_{i}^{2}/\lambda_{i},$
\end_inset

 where 
\begin_inset Formula $s_{i}$
\end_inset

 is the 
\begin_inset Formula $i$
\end_inset

-th term in the vector 
\begin_inset Formula $V^{*}x.$
\end_inset

 Sicne 
\begin_inset Formula $\lambda_{i}>0$
\end_inset

 for all 
\begin_inset Formula $i\in\{1,...,n\},$
\end_inset

 it follows that 
\begin_inset Formula $x^{*}Mx>0$
\end_inset

 as desired.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $\Rightarrow$
\end_inset

 On the other hand, suppose 
\begin_inset Formula $M$
\end_inset

 is positive definite, i.e.
 
\begin_inset Formula $x^{*}Mx>0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{C}^{n}$
\end_inset

.
\begin_inset Foot
status open

\begin_layout Plain Layout
One detail we left out here is to show that 
\begin_inset Formula $x^{*}Mx$
\end_inset

 is real for any 
\begin_inset Formula $x\in\mathbb{C}^{n}.$
\end_inset

 To see this, we observe that 
\begin_inset Formula $(x^{*}Mx)^{*}=x^{*}M^{*}x=x^{*}Mx,$
\end_inset

 whence is real since its complex conjugate is equal to itself.
 
\end_layout

\end_inset

 Let 
\begin_inset Formula $(\lambda,v)$
\end_inset

 be an eigen-pair of 
\begin_inset Formula $M$
\end_inset

.
 We show that 
\begin_inset Formula $\lambda>0$
\end_inset

 by case analysis.
 Suppose 
\begin_inset Formula $\lambda\leq0,$
\end_inset

 then we have 
\begin_inset Formula $v^{*}Mv=v^{*}\lambda v=\lambda\left|v\right|^{2}\leq0,$
\end_inset

 which is a contradiction to the fact that 
\begin_inset Formula $M$
\end_inset

 is positive definite.
 As a result, 
\begin_inset Formula $\lambda>0$
\end_inset

.
 Since 
\begin_inset Formula $\lambda$
\end_inset

 is chosen arbitrarily, we have shown that any eigen value of 
\begin_inset Formula $M$
\end_inset

 is positive as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.21 - Independent parameter for symmetric matrix 
\end_layout

\end_inset

Show that a real, symmetric matrix of size 
\begin_inset Formula $D\times D$
\end_inset

 has 
\begin_inset Formula $D(D+1)/2$
\end_inset

 independent parameters.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clearly, once we have known the upper triangular part of the matrix plus
 the diagonal, we will have known the whole matrix.
 As a result, the number of independent parameters for symmetric matrix
 is 
\begin_inset Formula $\sum_{i=1}^{D}i=D(D+1)/2.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.22 - Inverse of symmetric matrix is symmetric
\end_layout

\end_inset

 Show that the inverse of a symmetric matrix is itself symmetric.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we fix some notations.
 Let 
\begin_inset Formula $M\in\mathrm{GL}_{\mathbb{R}}(n)$
\end_inset

 be a symmetric matrix.
 It suffices to show that 
\begin_inset Formula $(M^{-1})^{T}=M^{-1}.$
\end_inset

 Since 
\begin_inset Formula $M^{-1}M=MM^{-1}=I,$
\end_inset

 it follows that 
\begin_inset Formula 
\[
(MM^{-1})^{T}=((M^{-1})^{T}M^{T})=(M^{-1})^{T}M=I.
\]

\end_inset

Hence, it follows that 
\begin_inset Formula $(M^{-1})^{T}M=M^{-1}M.$
\end_inset

 Now multiplying both sides of the previous expression by 
\begin_inset Formula $M^{-1},$
\end_inset

 we get 
\begin_inset Formula $(M^{-1})^{T}MM^{-1}=M^{-1}MM^{-t},$
\end_inset

 which implies that 
\begin_inset Formula $(M^{-1})^{T}=M^{-1}.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.23 - Volume of hyperellipsoid in 
\begin_inset Formula $n$
\end_inset

-dimensional space 
\end_layout

\end_inset

 By diagonalizing the coordinate system using the eigenvector expansion
 (2.45), show that the volume contained within the hyperellipsoid corresponding
 to a constant Mahalanobis distance 
\begin_inset Formula $\Delta$
\end_inset

 is given by 
\begin_inset Formula 
\[
V_{D}\left|\Sigma\right|^{1/2}\Delta^{D}
\]

\end_inset

where 
\begin_inset Formula $V_{D}$
\end_inset

 is the volume of the unit sphere in 
\begin_inset Formula $D$
\end_inset

 dimensions, and the Mahalanobis distance is defined by (2.44).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The wording of this problem is a bit confusing.
 Here, we give a clarification: recall that the unit sphere in 
\begin_inset Formula $n$
\end_inset

-dimensional Euclidean space is given by 
\begin_inset Formula 
\[
V_{D}:=\int_{\mathbb{R}^{n}}\mathbbm{1}(\left\Vert x\right\Vert \leq1)dx=\int_{\mathbb{R}^{n}}\mathbbm{1}(x^{T}x\leq1)dx.
\]

\end_inset

Also recall that the solution to this integral has been worked out in Problem
 1.18.
 In the textbook, the hyperellipsoid with Mahalanobis distance 
\begin_inset Formula $\Delta$
\end_inset

 is defined to be the set of the form 
\begin_inset Formula $\{x\in\mathbb{R}^{n}:(x-\mu)^{T}\Sigma^{-1}(x-\mu)\leq\Delta^{2},\Sigma^{-1}\text{ is positive (semi)definite}\}.$
\end_inset

 So to show that the volumne of hyperellipsoid with Mahalanobis distance
 
\begin_inset Formula $\Delta$
\end_inset

 is equal to 
\begin_inset Formula $V_{D}\det\left|\Sigma\right|^{1/2}\Delta^{D}$
\end_inset

 is equivalent to showing the following: 
\begin_inset Formula 
\[
\int_{\mathbb{R}^{n}}\mathbbm{1}((x-\mu)^{T}\Sigma^{-1}(x-\mu))dx=V_{D}\det\left|\Sigma\right|^{1/2}\Delta^{D}.
\]

\end_inset

We show this using a series of change of variables(c.f.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset

) : note that 
\begin_inset Formula 
\begin{align*}
\int_{\mathbb{R}^{n}}\mathbbm{1}((x-\mu)^{T}\Sigma^{-1}(x-\mu)\leq\Delta^{2})dx & =\int_{\mathbb{R}^{n}}\mathbbm{1}(y^{T}\Sigma^{-1}y\leq\Delta^{2})dy\tag{by letting \ensuremath{x=y+\mu}}\\
 & =\int_{\mathbb{R}^{n}}\mathbbm{1}(y^{T}\Sigma^{-1/2}\Sigma^{-1/2}y\leq\Delta^{2})dy\tag{since \ensuremath{\Sigma^{-1}} is p.d.}\\
 & =\int_{\mathbb{R}^{n}}\mathbbm{1}(w^{T}w\leq\Delta^{2})\left|\det\Sigma^{1/2}\right|dw\tag{by letting \ensuremath{y=\Sigma^{1/2}w}}\\
 & =\int_{\mathbb{R}^{n}}\mathbbm{1}(z^{T}z\leq1)\left|\det\Sigma^{1/2}\right|\left|\det\Delta I\right|dz\tag{by letting \ensuremath{w=(\Delta I)z}}\\
 & =\left|\det\Sigma^{1/2}\right|\det\left|\Delta I\right|V_{D}\\
 & =\det\left|\Sigma\right|^{1/2}\Delta^{D}V_{D},
\end{align*}

\end_inset

where the last equality follows since 
\begin_inset Formula $\det(\Sigma^{1/2}\Sigma^{1/2})=\det(\Sigma^{1/2})\det(\Sigma^{1/2})=\det(\Sigma),$
\end_inset

 which implies that 
\begin_inset Formula $\det(\Sigma^{1/2})=(\det\Sigma)^{1/2}$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.24 - Block matrix inversion formula
\end_layout

\end_inset

 Prove the identity (2.76) by multiplying both sides by the matrix 
\begin_inset Formula 
\[
\begin{bmatrix}A & B\\
C & D
\end{bmatrix}
\]

\end_inset

and making use of the definition 
\begin_inset Formula $(2.77).$
\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To facilitate notation, we use 
\begin_inset Formula $F$
\end_inset

 to denote the matrix defined in the RHS of Eq.(2.76) and 
\begin_inset Formula $F$
\end_inset

 the LHS.
 To show the identity claimed in Eq.(2.76) holds, we need to show that 
\begin_inset Formula $FE=EF=I$
\end_inset

(we assume the dimensions match in all the partitions and parts of the parition
 in 
\begin_inset Formula $F$
\end_inset

 are necessarily invertible).
 Note that the following lemma shows that it suffices for us to show either
 
\begin_inset Formula $FE=I$
\end_inset

 or 
\begin_inset Formula $EF=I$
\end_inset

.
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{C}}(n,m)$
\end_inset

 and 
\begin_inset Formula $B\in\mathrm{Mat}_{\mathbb{C}}(m,n).$
\end_inset

 Then if 
\begin_inset Formula $AB=I$
\end_inset

, it follows that 
\begin_inset Formula $BA=I.$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: matrix left inverse in implies right inverse"

\end_inset


\end_layout

\begin_layout Proof
By assumption we have 
\begin_inset Formula $AB-I=0.$
\end_inset

 We multiply on the left by 
\begin_inset Formula $B$
\end_inset

 to get 
\begin_inset Formula $BAB-B=(BA-I)B=0.$
\end_inset

 Now we let 
\begin_inset Formula $\{e_{i}\}_{i=1}^{n}$
\end_inset

 be the standard basis of 
\begin_inset Formula $\mathbb{R}^{n}.$
\end_inset

 We claim that 
\begin_inset Formula $\{Be_{i}\}_{i=1}^{n}$
\end_inset

 is also a standard basis.
 To see this, suppose 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}Be_{i}=0.$
\end_inset

 Multiplying both sides on the left by 
\begin_inset Formula $A$
\end_inset

 and we get 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}ABe_{i}=0,$
\end_inset

 which reduces to 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}e_{i}=0$
\end_inset

 since 
\begin_inset Formula $AB=I$
\end_inset

 by assumption.
 Since 
\begin_inset Formula $\{e_{i}\}_{i=1}^{n}$
\end_inset

 is the standard basis, it follows that 
\begin_inset Formula $\alpha_{i}=0$
\end_inset

 for all 
\begin_inset Formula $i\in\{1,...,n\}.$
\end_inset

 As a result 
\begin_inset Formula $\{Be_{i}\}_{i=1}^{n}$
\end_inset

 is a set of basis in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 as desired.
 Now we come back to the 
\begin_inset Formula $(BA-I)B=0.$
\end_inset

 Note that from which we can see that 
\begin_inset Formula $(BA-I)Be_{i}$
\end_inset

 for all 
\begin_inset Formula $i=1,...,n.$
\end_inset

 Since 
\begin_inset Formula $\{Be_{i}\}_{i=1}^{n}$
\end_inset

 is a set of basis, it follows that for any 
\begin_inset Formula $v\in\mathbb{R}^{n},$
\end_inset

 we have 
\begin_inset Formula 
\[
(BA-I)v=(BA-I)\left(\sum_{i=1}^{n}\alpha_{i}Be_{i}\right)=\sum_{i=1}^{n}\alpha_{i}(BA-I)Be_{i}=0,
\]

\end_inset

whence 
\begin_inset Formula $BA=I$
\end_inset

 as desired.
 
\end_layout

\begin_layout Standard
In view of previous lemma, we go ahead and show that 
\begin_inset Formula $FE=I.$
\end_inset

 Writing the terms out explicitly yields: 
\begin_inset Formula 
\begin{align*}
FE & =\begin{bmatrix}A & B\\
C & D
\end{bmatrix}\begin{bmatrix}M & -MBD^{-1}\\
-D^{-1}CM & D^{-1}+D^{-1}CMBD^{-1}
\end{bmatrix}\\
 & =\begin{bmatrix}AM-B(D^{-1}CM) & -AMBD^{-1}+BD^{-1}+BD^{-1}CMBD^{-1}\\
CM-DD^{-1}CM & -CMBD^{-1}+I+CMBD^{-1}
\end{bmatrix}\\
 & :=\begin{bmatrix}P_{11} & P_{12}\\
P_{21} & P_{22}
\end{bmatrix}.
\end{align*}

\end_inset

Now we evaluate the blocks 
\begin_inset Formula $P_{ij}$
\end_inset

 term by term: 
\begin_inset Formula 
\begin{align*}
P_{11} & =A(A-BD^{-1}C)^{-1}-BD^{-1}C(A-BD^{-1}C)^{-1}=(A-BD^{-1}C)(A-BD^{-1}C)^{-1}=I;\\
P_{12} & =(-AM+I+BD^{-1}CM)BD^{-1}=(-\underbrace{(A-BD^{-1}C)M}_{=I}+I)BD^{-1}=0;\\
P_{21} & =CM-CM=0;\\
P_{22} & =I.
\end{align*}

\end_inset

Therefore, the results follows.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.25 - Marginal and conditional expectation of multivariate gaussian
 
\end_layout

\end_inset

 In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distributio
ns for a multivariate Gaussian.
 More generally, we can consider a partitioning of the components of 
\begin_inset Formula $X$
\end_inset

 into three groups 
\begin_inset Formula $X_{a}$
\end_inset

, 
\begin_inset Formula $B_{b}$
\end_inset

, and 
\begin_inset Formula $X_{c}$
\end_inset

, with a corresponding partitioning of the mean vector 
\begin_inset Formula $\mu$
\end_inset

 and of the covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 in the form 
\begin_inset Formula 
\[
\mu=\begin{bmatrix}\mu_{a}\\
\mu_{b}\\
\mu_{c}
\end{bmatrix},\ \ \Sigma=\begin{bmatrix}\Sigma_{aa} & \Sigma_{ab} & \Sigma_{ac}\\
\Sigma_{ba} & \Sigma_{bb} & \Sigma_{bc}\\
\Sigma_{ca} & \Sigma_{cb} & \Sigma_{cc}
\end{bmatrix}
\]

\end_inset

By making use of the results of Section 2.3, find an expression for the condition
al distribution 
\begin_inset Formula $\P(X_{a}\vert X_{b})$
\end_inset

 in which 
\begin_inset Formula $X_{c}$
\end_inset

 has been marginalized out.
\end_layout

\begin_layout Remark

\shape italic
Although this textbook has derived the result for marginal and conditional
 distribution for multivariate gaussian distributions using completing squares.
 It is not done in the most rigiorous manner and left out many of the calculatio
ns.
 Hence, we rederive the results here.
 However, we will be using a slightly different approach in the derivation,
 which is more rigorous and slightly more general.
 Details of the derivation proposed by the book will also be discussed in
 the reading notes.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To begin with, we introduce a few auxiliary lemmas to help proving later
 results.
\end_layout

\begin_layout Lemma
Suppose 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{R}}(n,n)$
\end_inset

 is positive definite(symmetric) and partitioned as 
\begin_inset Formula $A=\begin{bmatrix}A_{11} & A_{12}\\
A_{21} & A_{22}
\end{bmatrix},$
\end_inset

 then 
\begin_inset Formula $A_{11}$
\end_inset

 and 
\begin_inset Formula $A_{22}$
\end_inset

 are positive definite as well.
 
\end_layout

\begin_layout Proof
It suffices to show that 
\begin_inset Formula $A_{11}$
\end_inset

 and 
\begin_inset Formula $A_{22}$
\end_inset

 are p.d since p.d.
 matrices are invertible.
 Without loss of generality, we assume 
\begin_inset Formula $A_{11}$
\end_inset

 is of dimension 
\begin_inset Formula $n_{1}\times n_{1}$
\end_inset

 and 
\begin_inset Formula $A_{22}$
\end_inset

 of 
\begin_inset Formula $n_{2}\times n_{2}$
\end_inset

 such that 
\begin_inset Formula $n_{1}+n_{2}=n.$
\end_inset

 Note that since 
\begin_inset Formula $A$
\end_inset

 is p.d., 
\begin_inset Formula $x^{T}Ax>0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{R}^{n}.$
\end_inset

 Then for 
\begin_inset Formula $x=\begin{bmatrix}x_{1}\\
0
\end{bmatrix}$
\end_inset

 in block form, where 
\begin_inset Formula $x_{1}\in\mathbb{R}^{n_{1}}$
\end_inset

 is arbitrarily chosen, it follows that 
\begin_inset Formula $x^{T}Ax=x_{11}^{T}A_{11}x_{11}>0$
\end_inset

.
 Therefore, 
\begin_inset Formula $A_{11}$
\end_inset

 is p.d.
 as well.
 On the other hand, if we let 
\begin_inset Formula $x=\begin{bmatrix}0\\
x_{2}
\end{bmatrix}$
\end_inset

 for some arbitrarily chosen 
\begin_inset Formula $x_{2}\in\mathbb{R}^{n_{2}},$
\end_inset

 it also follows that 
\begin_inset Formula $x^{T}Ax=x_{2}^{T}A_{22}x_{2}>0.$
\end_inset

 Since 
\begin_inset Formula $x_{2}$
\end_inset

 is arbitrarily, 
\begin_inset Formula $A_{22}$
\end_inset

 is p.d.
 as well.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $A\in\mathrm{Mat}_{\mathbb{\mathbb{C}}}(n,n)$
\end_inset

, 
\begin_inset Formula $D\in\mathrm{Mat}_{\mathbb{C}}(m,m),B\in\mathrm{Mat}_{\mathbb{C}}(n,m)$
\end_inset

 for arbitrary 
\begin_inset Formula $n,m\in\mathbb{N}$
\end_inset

.
 Then it follows that 
\begin_inset CommandInset label
LatexCommand label
name "lem: block determinant"

\end_inset


\begin_inset Formula 
\[
\det\begin{bmatrix}A & B\\
0 & D
\end{bmatrix}=\det(A)\det(D).
\]

\end_inset


\end_layout

\begin_layout Proof
We layout our proof in several steps as below.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

We first prove the basic case where 
\begin_inset Formula $D=I.$
\end_inset

 We claim that 
\begin_inset Formula $\det\begin{bmatrix}A & B\\
0 & I
\end{bmatrix}=\det A.$
\end_inset

 To prove this claim, we induct on 
\begin_inset Formula $I'$
\end_inset

s size, denoted as 
\begin_inset Formula $m$
\end_inset

.
 For the base case where 
\begin_inset Formula $m=1$
\end_inset

, we have by Laplace expansion on the last row that 
\begin_inset Formula 
\[
\det\begin{bmatrix}A & B\\
0 & 1
\end{bmatrix}=(-1)^{(n+1)+(n+1)}\det(A)=\det A.
\]

\end_inset

Now suppose 
\begin_inset Formula $m=k$
\end_inset

 holds.
 Then we have that 
\begin_inset Formula 
\[
\det\begin{bmatrix}A & B\\
0 & I_{k+1}
\end{bmatrix}=\det\begin{bmatrix}A & B_{1} & B_{2}\\
0 & I_{k} & 0\\
0 & 0 & 1
\end{bmatrix}=(-1)^{(n+m+1)+(n+m+1)}\det\begin{bmatrix}A & B\\
0 & I_{k}
\end{bmatrix}=\det(A),
\]

\end_inset

where 
\begin_inset Formula $B_{1}\in\mathrm{Mat}_{\mathbb{C}}(n,m)$
\end_inset

 and that 
\begin_inset Formula $B_{2}\in\mathrm{Mat}_{\mathbb{C}}(n,1)$
\end_inset

 and the last equality follows from inductive hypoethesis.
 We also note that using the same argument we can also show that 
\begin_inset Formula 
\[
\det\begin{bmatrix}I & B\\
0 & D
\end{bmatrix}=\det(D).
\]

\end_inset


\end_layout

\begin_layout Enumerate
Now we go back to the proof of the lemma.
 We first deal the case where 
\begin_inset Formula $A$
\end_inset

 is inverstible, i.e.
 
\begin_inset Formula $\det A\neq0.$
\end_inset

 Recall that 
\begin_inset Formula $\det(MN)=\det(M)\det(N)$
\end_inset

 for abitrary compatible matrices 
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $N$
\end_inset

.
 Therefore, it follows that 
\begin_inset Formula 
\[
\det\begin{bmatrix}A & B\\
0 & D
\end{bmatrix}=\det\left(\begin{bmatrix}A & 0\\
0 & I_{m}
\end{bmatrix}\begin{bmatrix}I_{n} & A^{-1}B\\
0 & D
\end{bmatrix}\right)=\det\begin{bmatrix}A & 0\\
0 & I_{m}
\end{bmatrix}\det\begin{bmatrix}I_{n} & A^{-1}B\\
0 & D
\end{bmatrix}=\det(A)\det(D),
\]

\end_inset

where the last equality follows from step-1.
 
\end_layout

\begin_layout Enumerate
It remains to deal with the case where 
\begin_inset Formula $A$
\end_inset

 is not invertible.
 Note that if 
\begin_inset Formula $A$
\end_inset

 is not invertible, then the columns of 
\begin_inset Formula $A$
\end_inset

 are not linearly independent.
 From this we see that the first 
\begin_inset Formula $n$
\end_inset

 columns of 
\begin_inset Formula $\begin{bmatrix}A & B\\
0 & D
\end{bmatrix}$
\end_inset

 are not linearly independent as well since we are stacking 
\begin_inset Formula $0$
\end_inset

's under 
\begin_inset Formula $A$
\end_inset

 and as a result the spanning set of first 
\begin_inset Formula $n$
\end_inset

 columns is thus homeomorphic to that of 
\begin_inset Formula $A.$
\end_inset

 Therefore, 
\begin_inset Formula $\begin{bmatrix}A & B\\
0 & D
\end{bmatrix}$
\end_inset

 is not invertible.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
The following result is a classical theorem, whose proof can be easily found
 in any measure theoretic probability books.
 We state without proof.
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Uniqueness of fourier transform
\end_layout

\end_inset

The Fourier transform of a probability measure on 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 characterizes 
\begin_inset Formula $\mu,$
\end_inset

 that is if two probabilty measures on 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 admit the same Fourier transform, they are equal.
 
\begin_inset CommandInset label
LatexCommand label
name "thm: uniqueness of cf"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
In later results, we will construct some independent variables from family
 of distributions decided by random variables that are not necessarily independe
nt.
 The proof of this theorem is quite complicated.
 It's closely related to the Kolmogrov extension theorem.
 
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Creation of new, independent random variables
\end_layout

\end_inset

Let 
\begin_inset Formula $(X_{\alpha})_{\alpha\in A}$
\end_inset

 be a family of random variabels(not necessarily independent or finite),
 and let 
\begin_inset Formula $(\mu_{\beta})_{\beta\in B}$
\end_inset

 be a collection (not necessarily finite) of probability measures on measurable
 spaces 
\begin_inset Formula $(R_{\beta})_{\beta\in B}.$
\end_inset

 Then after extending the sample spaces if necessary, one can find a family
 
\begin_inset Formula $(Y_{\beta})_{\beta\in B}$
\end_inset

 of independent random variables such that each 
\begin_inset Formula $Y_{\beta}$
\end_inset

 has distribution 
\begin_inset Formula $\mu_{\beta}$
\end_inset

, and the two families 
\begin_inset Formula $(X_{\alpha})_{\alpha\in A}$
\end_inset

 and 
\begin_inset Formula $(Y_{\beta})_{\beta\in B}$
\end_inset

 are independent of each other.
\begin_inset CommandInset label
LatexCommand label
name "thm: construct independent r.v. "

\end_inset

 
\end_layout

\begin_layout Standard
One direct application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: uniqueness of cf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is the following lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 valued random variable that has partition of the form 
\begin_inset Formula $\begin{bmatrix}X_{1};X_{2}\end{bmatrix}$
\end_inset

, where 
\begin_inset Formula $X_{1}\in\mathbb{R}^{k}$
\end_inset

 and 
\begin_inset Formula $X_{2}\in\mathbb{R}^{n-k}.$
\end_inset

 Then 
\begin_inset Formula $X_{1}\perp X_{2}$
\end_inset

 iff 
\begin_inset Formula $\varphi_{X}(t)=\varphi_{X_{1}}(t_{1})\varphi_{X_{2}}(t_{2}).$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: cf independence"

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula $\Rightarrow$
\end_inset

 Suppose 
\begin_inset Formula $X_{1}\perp X_{2}.$
\end_inset

 Then by a well known result (cf.
 
\begin_inset CommandInset citation
LatexCommand cite
after "Exercise 4.15"
key "resnickProbabilityPath2014"
literal "false"

\end_inset

), we have that 
\begin_inset Formula 
\[
\varphi_{X}(t)=\E[e^{i\left\langle t,X\right\rangle }]=\E[e^{i\left\langle t_{1},X_{1}\right\rangle }e^{i\left\langle t_{2},X_{2}\right\rangle }]=\E[e^{i\left\langle t_{1},X_{1}\right\rangle }]\E[e^{i\left\langle t_{2},X_{2}\right\rangle }]=\varphi_{X_{1}}(t_{1})\varphi_{X_{2}}(t_{2}).
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset


\begin_inset Formula $\Leftarrow$
\end_inset

 We first construct 
\begin_inset Formula $\widetilde{X}_{1}$
\end_inset

 and 
\begin_inset Formula $\widetilde{X}_{2}$
\end_inset

 such that 
\begin_inset Formula $X_{1}=_{d}\widetilde{X}_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}=_{d}\widetilde{X}_{1}$
\end_inset

, as well as 
\begin_inset Formula $\widetilde{X}_{1}\perp\widetilde{X}_{2}$
\end_inset

 (the existence of 
\begin_inset Formula $\widetilde{X}_{1}$
\end_inset

 and 
\begin_inset Formula $\widetilde{X}_{2}$
\end_inset

 is guaranteed by 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: construct independent r.v. "
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Then we have 
\begin_inset Formula 
\begin{align*}
\varphi_{(X_{1},X_{2})}((t_{1},t_{2})) & =\varphi_{X_{1}}(t_{1})\varphi_{X_{2}}(t_{2})\\
 & =\varphi_{\tilde{X}_{1}}(t_{1})\varphi_{\tilde{X}_{2}}(t_{2})\tag{by definition of characteristic functions}\\
 & =\varphi_{(\widetilde{X}_{1},\widetilde{X}_{2})}((t_{1},t_{2})).
\end{align*}

\end_inset

Therefore, 
\begin_inset Formula $\P_{(X_{1},X_{2})}=\P_{(\widetilde{X}_{1},\widetilde{X}_{2})}.$
\end_inset

 Hence, as a result, for any 
\begin_inset Formula $A\in\mathcal{B}(\mathbb{R}^{k}),B\in\mathcal{B}(\mathbb{R}^{n-k}),$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
\P_{(X_{1},X_{2})}(X_{1}\in A,X_{2}\in B) & =\P_{(\widetilde{X}_{1},\widetilde{X}_{2})}(\widetilde{X}_{1}\in A,\widetilde{X}_{2}\in B)=\P(\tilde{X}_{1}\in A)\P(\tilde{X}_{2}\in B)\\
 & =\P(X_{1}\in A)\P(X_{2}\in B).
\end{align*}

\end_inset

Hence, 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
 
\end_layout

\begin_layout Standard
Now, we state and prove some useful result of multivariate gaussian distribution
 for later use.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 valued random variable such that 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu,\Sigma)$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is paritioned as (for some fixed 
\begin_inset Formula $k$
\end_inset

)
\begin_inset Formula 
\[
X=\begin{bmatrix}X_{1}\\
X_{2}
\end{bmatrix},\ \ \mu=\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix}\text{ and }\Sigma=\begin{bmatrix}\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix},
\]

\end_inset

where 
\begin_inset Formula $X_{1},\mu_{1}\in\mathbb{R}^{k}$
\end_inset

 and 
\begin_inset Formula $X_{2},\mu_{2}\in\mathbb{R}^{n-k}$
\end_inset

, for 
\begin_inset Formula $k=1,...,n-1$
\end_inset

 (
\begin_inset Formula $\Sigma_{11},\Sigma_{12},\Sigma_{21},\Sigma_{22}$
\end_inset

 are of dimension 
\begin_inset Formula $k\times k,k\times(n-k),(n-k)\times k,(n-k)\times(n-k)$
\end_inset

).
 Then the follow holds:
\begin_inset CommandInset label
LatexCommand label
name "lem: property of gaussian"

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $X_{1}\sim\mathrm{MVN}(\mu_{1},\Sigma_{11})$
\end_inset

 and 
\begin_inset Formula $X_{2}\sim\mathrm{MVN}(\mu_{2},\Sigma_{22});$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $X_{1}\perp X_{2}$
\end_inset

 iff 
\begin_inset Formula $\Sigma_{12}=0;$
\end_inset

 
\end_layout

\begin_layout Enumerate
the conditional distribution of 
\begin_inset Formula $X_{1}$
\end_inset

 given that 
\begin_inset Formula $X_{2}=x_{2}$
\end_inset

 is 
\begin_inset Formula $\mathrm{MVN}(\mu_{1\cdot2},\Sigma_{11\cdot2}),$
\end_inset

 where 
\begin_inset Formula $\mu_{1\cdot2}=\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2})$
\end_inset

, and 
\begin_inset Formula $\Sigma_{11\cdot2}=\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}.$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
To prove this, we take the characteristic function approach.
 We fist talk about the general case.
 If we don't make any distributional assumption about 
\begin_inset Formula $X$
\end_inset

 and only assume that it has some density function which we denote as 
\begin_inset Formula $f_{X}(x).$
\end_inset

 Then we have that 
\begin_inset Formula 
\begin{align*}
\varphi_{X_{1}}(t) & =\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}\exp\bigg(i\bigg(\sum_{i=1}^{n_{1}}t_{i}x_{i}\bigg)\bigg)f(x_{1},...,x_{n_{1}})dx_{1}dx_{2}\cdots dx_{n_{1}}\\
 & =\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}\exp\bigg(i\bigg(\sum_{i=1}^{n_{1}}t_{i}x_{i}\bigg)\left(\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}f(x_{1},\ldots,x_{n})dx_{n_{1}+1}\cdots dx_{n}\right)dx_{1}\cdots dx_{n}\\
 & =\int_{\mathbb{R}}\cdots\int_{\mathbb{R}}\exp\bigg(i\bigg(\sum_{i=1}^{n_{1}}t_{i}x_{i}+\sum_{i=n_{1+1}}^{n}0x_{i}\bigg)\bigg)f(x_{1},\ldots,x_{n})dx_{1}\cdots dx_{n}\\
 & =\varphi_{X}(t,0),
\end{align*}

\end_inset

where 
\begin_inset Formula $t\in\mathbb{R}^{n_{1}}.$
\end_inset

 Hence, by applying this fact to the gaussian case along with 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we see that 
\begin_inset Formula 
\begin{align*}
\varphi_{X_{1}}(t) & =\varphi_{X}\bigg(\begin{bmatrix}t\\
0
\end{bmatrix}\bigg)\\
 & =\exp\bigg((i\left\langle \begin{bmatrix}t\\
0
\end{bmatrix},\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix}\right\rangle \bigg)\exp\bigg(-\frac{1}{2}\begin{bmatrix}t\\
0
\end{bmatrix}^{T}\begin{bmatrix}\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}\begin{bmatrix}t\\
0
\end{bmatrix}\bigg)\\
 & =\exp(i\left\langle t,x\right\rangle )\exp\left(-\frac{1}{2}t^{T}\Sigma_{11}t\right).
\end{align*}

\end_inset

Hence, 
\begin_inset Formula $X_{1}\sim\mathrm{MVN}(\mu_{1},\Sigma_{11}).$
\end_inset

 That 
\begin_inset Formula $X_{2}\sim\mathrm{MVN}(\mu_{2},\Sigma_{22})$
\end_inset

 follows from a similar argument.
 
\end_layout

\begin_layout Enumerate
To prove this, note that 
\begin_inset Formula 
\[
\Sigma_{12}=0\iff t^{T}\Sigma t\iff t_{1}^{T}\Sigma_{11}t_{1}+t_{2}^{T}\Sigma_{22}t_{2},
\]

\end_inset

for any 
\begin_inset Formula $t\in\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $t_{1}\in\mathbb{R}^{k},t_{2}\in\mathbb{R}^{n-k}.$
\end_inset

 Then it follows that 
\begin_inset Formula 
\begin{align*}
\varphi_{X}(t) & =\exp(i\left\langle t,x\right\rangle )\exp\left(-\frac{1}{2}t^{T}\Sigma_{11}t\right)\\
 & =\exp(i\left\langle t_{1},x_{1}\right\rangle )\exp\left(-\frac{1}{2}t_{1}^{T}\Sigma_{11}t_{1}\right)\exp(i\left\langle t_{2},x_{2}\right\rangle )\exp\left(-\frac{1}{2}t_{2}^{T}\Sigma_{22}t_{2}\right)\\
 & =\varphi_{X_{1}}(t_{1})\varphi_{X_{2}}(t_{2}).
\end{align*}

\end_inset

Then the result follows by an application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf independence"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

First, we consider a linear transformation of 
\begin_inset Formula $X$
\end_inset

 in the form as 
\begin_inset Formula 
\[
CX=\begin{bmatrix}I_{k} & -B\\
0 & I_{n-k}
\end{bmatrix}\begin{bmatrix}X_{1}\\
X_{2}
\end{bmatrix}=\begin{bmatrix}X_{1}-BX_{2}\\
X_{2}
\end{bmatrix}:=\begin{bmatrix}Y_{1}\\
Y_{2}
\end{bmatrix}:=Y
\]

\end_inset

Then by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it follows that 
\begin_inset Formula $Y\sim\mathrm{MVN}(\mu_{Y},\Sigma_{Y}),$
\end_inset

 where 
\begin_inset Formula 
\begin{align*}
\mu_{Y} & =\begin{bmatrix}I_{k} & -B\\
0 & I_{n-k}
\end{bmatrix}\begin{bmatrix}\mu_{1}\\
\mu_{2}
\end{bmatrix}=\begin{bmatrix}\mu_{1}-B\mu_{2}\\
\mu_{2}
\end{bmatrix};\\
\Sigma_{Y} & =\begin{bmatrix}I_{k} & -B\\
0 & I_{n-k}
\end{bmatrix}\begin{bmatrix}\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}\begin{bmatrix}I_{k} & -B^{T}\\
0 & I_{n-k}
\end{bmatrix}=\begin{bmatrix}\Sigma_{11}-B\Sigma_{12}+\Sigma_{12}B^{T}-B\Sigma_{22}B^{T} & \Sigma_{12}-B\Sigma_{22}\\
\Sigma_{12}-B^{T}\Sigma_{22} & \Sigma_{22}
\end{bmatrix}.
\end{align*}

\end_inset

 In view of part-2, if we let 
\begin_inset Formula $B=\Sigma_{12}\Sigma_{22}^{-1}$
\end_inset

, we have that 
\begin_inset Formula 
\begin{align*}
Y & \sim\mathrm{MVN}\left(\begin{bmatrix}\mu_{1}-\Sigma_{12}\Sigma_{22}^{-1}\mu_{2}\\
\mu_{2}
\end{bmatrix},\begin{bmatrix}\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & 0\\
0 & \Sigma_{22}
\end{bmatrix}\right)=\mathrm{MVN}\left(\widetilde{\mu}=\begin{bmatrix}\mu_{1\cdot2}\\
\mu_{2}
\end{bmatrix},\widetilde{\Sigma}=\begin{bmatrix}\Sigma_{11\cdot2} & 0\\
0 & \Sigma_{22}
\end{bmatrix}\right)
\end{align*}

\end_inset

Then in view of part 1 and 2, it follows that 
\begin_inset Formula $Y_{1}\sim\mathrm{MVN}(\mu_{11\cdot2},\Sigma_{11\cdot2})\text{ and }Y_{2}\sim\mathrm{MVN}(\mu_{2},\Sigma_{22}),$
\end_inset

 and moreover 
\begin_inset Formula $Y_{1}\perp Y_{2}.$
\end_inset

 Therefore, 
\begin_inset Formula $Y$
\end_inset

 has the density function in the following form 
\begin_inset Formula 
\begin{align*}
f_{Y}(y) & =f_{(Y_{1},Y_{2})}((y_{1},y_{2}))\\
 & =\frac{1}{(2\pi)^{n/2}(\det\widetilde{\Sigma})^{1/2}}\exp\left(-\frac{1}{2}(y-\mu)^{T}\Sigma^{-1}(y-\mu)\right)\\
 & =\frac{1}{(2\pi)^{k/2}(\det\Sigma_{11\cdot2})}\exp\left(-\frac{1}{2}(y_{1}-\mu_{1\cdot2})^{T}\Sigma_{11\cdot2}^{-1}(y_{1}-\mu_{1\cdot2})\right)\tag{by \ref{lem: block determinant} }\\
 & \ \ \times\frac{1}{(2\pi)^{(n-k)/2}\det(\Sigma_{22})}\exp\left(-\frac{1}{2}(y_{2}-\mu_{2})^{T}\Sigma_{22}^{-1}(y_{2}-\mu_{2})\right)\\
 & =f_{Y_{1}}(y_{1})f_{Y_{2}}(y_{2}).
\end{align*}

\end_inset

Now note that since 
\begin_inset Formula 
\[
\begin{bmatrix}I_{k} & \Sigma_{12}\Sigma_{22}\\
0 & I_{n-k}
\end{bmatrix}Y=\begin{bmatrix}X_{1}\\
X_{2}
\end{bmatrix},\text{ and }\begin{bmatrix}I_{k} & \Sigma_{12}\Sigma_{22}\\
0 & I_{n-k}
\end{bmatrix}^{-1}=\begin{bmatrix}I_{k} & -\Sigma_{12}\Sigma_{22}^{-1}\\
0 & I_{n-k}
\end{bmatrix}:=M,
\]

\end_inset

it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula 
\begin{align*}
f_{(X_{1},X_{2})}(x_{1},x_{2}) & =f_{(Y_{1},Y_{2})}\left(\begin{bmatrix}I_{k} & -\Sigma_{12}\Sigma_{22}^{-1}\\
0 & I_{n-k}
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}\right)=f_{(Y_{1},Y_{2})}(x_{1}-\Sigma_{12}\Sigma_{22}^{-1}x_{2},x_{2})\\
 & =f_{Y_{1}}(x_{1}-\Sigma_{12}\Sigma_{22}^{-1}x_{2})f_{X_{2}}(x_{2}).
\end{align*}

\end_inset

On the other hand, since 
\begin_inset Formula $f_{(X_{1},X_{2})}(x_{1},x_{2})=f_{X_{1}\vert X_{2}}(x_{1}\vert x_{2})f_{X_{2}}(x_{2}),$
\end_inset

 the follows that 
\begin_inset Formula $f_{Y_{1}}(x_{1}-\Sigma_{12}\Sigma_{22}^{-1}x_{2})=f_{X_{1}\vert X_{2}}(x_{1}\vert x_{2}).$
\end_inset

 Since 
\begin_inset Formula 
\[
x_{1}-\Sigma_{12}\Sigma_{22}^{-1}x_{2}-\mu_{1}+\Sigma_{12}\Sigma_{22}\mu_{2}=x_{1}-(\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2})=\mu_{1\cdot2},
\]

\end_inset

we further expand the expression and get 
\begin_inset Formula 
\begin{align*}
f_{Y_{1}}(x_{1}-\Sigma_{12}\Sigma_{22}^{-1}x_{2}) & =\frac{1}{(2\pi)^{k/2}\left|\det\Sigma_{11\cdot2}\right|^{1/2}}\exp\bigg(-\frac{1}{2}(x_{1}-\mu_{1\cdot2})^{T}\Sigma_{11\cdot2}^{-1}(x_{1}-\mu_{1\cdot2})\bigg)\\
 & =f_{\mathrm{MVN}(\mu_{1\cdot2},\Sigma_{11\cdot2})}(x_{1}).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $X_{1}\vert X_{2}=x_{2}\sim\mathrm{MVN}(\mu_{1\cdot2},\Sigma_{11\cdot2})$
\end_inset

 as desired.
 
\end_layout

\end_deeper
\begin_layout Standard
Now we go back to the solution to the problem.
 Since 
\begin_inset Formula $(X_{a},X_{b},X_{c})$
\end_inset

 and 
\begin_inset Formula $((X_{a},X_{b}),X_{c})$
\end_inset

 are homeomorphic, it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: property of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that 
\begin_inset Formula 
\[
\begin{bmatrix}X_{a}\\
X_{b}
\end{bmatrix}\sim\mathrm{MVN}\left(\mu_{a\cdot b},\Sigma_{a\cdot b}\right),\text{where }\mu_{a\cdot b}=\begin{bmatrix}\mu_{a}\\
\mu_{b}
\end{bmatrix},\Sigma_{a\cdot b}=\begin{bmatrix}\Sigma_{aa} & \Sigma_{ab}\\
\Sigma_{ba} & \Sigma_{bb}
\end{bmatrix}.
\]

\end_inset

Another application of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: property of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

-(3), yields that 
\begin_inset Formula 
\[
f_{X_{a}\vert X_{b}}(x_{a}\vert x_{b})=\frac{1}{(2\pi)^{(\dim X_{a}+\dim X_{b})/2}(\det\Sigma_{aa\cdot2})}\exp\left(-\frac{1}{2}(x-\mu_{a\cdot2})^{T}\Sigma_{aa\cdot2}^{-1}(x-\mu_{a\cdot2})\right),
\]

\end_inset

where 
\begin_inset Formula $\mu_{a\cdot2}=\mu_{a}+\Sigma_{ab}\Sigma_{bb}^{-1}(x_{b}-\mu_{b}),\text{ and }\Sigma_{aa\cdot2}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.26 - Woodbury matrix inversion formula
\end_layout

\end_inset

 A very useful result from linear algebra is the Woodbury matrix inversion
 formula given by 
\begin_inset Formula 
\[
(A+BCD)^{-1}=A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}.
\]

\end_inset

By multiplying both sides by 
\begin_inset Formula $(A+BCD)$
\end_inset

 prove the correctness of this result.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: matrix left inverse in implies right inverse"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it suffices to show one of the left and right inverse.
 We show the left inverse here.
 Note
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & (A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1})(A+BCD)\\
=\ \  & I+A^{-1}B(C^{-1}+DA^{-1}B)^{-1}D-A^{-1}BCD-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}BCD\\
=\ \  & I+A^{-1}BCD-A^{-1}B(C^{-1}+DA^{-1}B)(DA^{-1}BC+I)D\\
=\ \  & I+A^{-1}BCD-A^{-1}B(C^{-1}+DA^{-1}B)(DA^{-1}B+C^{-1})CD\\
=\ \  & I+A^{-1}BCD-A^{-1}BCD\\
=\ \  & I.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.27 - Linearity of expectation and covariance (multivariate case)
\end_layout

\end_inset

 Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 be two independent random vectors, so that 
\begin_inset Formula $f(x,z)=f(x)f(z)$
\end_inset

.
 Show that the mean of their sum 
\begin_inset Formula $Y=X+Z$
\end_inset

 is given by the sum of the means of each of the variable separately.
 Similarly, show that the covariance matrix of 
\begin_inset Formula $Y$
\end_inset

 is given by the sum of the covariance matrices of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

.
 Confirm that this result agrees with that of Exercise 1.10.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
leftmargin=*
\end_layout

\end_inset

For expectation, we note that 
\begin_inset Formula 
\begin{align*}
\E[X+Y] & =\int\int(x+y)f_{(X,Y)}(x,y)dxdy\\
 & =\int\int(x+y)f_{X}(x)f_{Y}(y)dxdy\\
 & =\int_{\mathrm{supp}(X)}xf(x)\left(\int_{\mathrm{supp}(Y)}f(y)dy\right)dx+\int_{\mathrm{supp}(Y)}yf(y)\left(\int_{\mathrm{supp}(X)}f(x)dx\right)dy\\
 & =\E[X]+\E[Y].
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
For covariance, note that 
\begin_inset Formula 
\begin{align*}
\mathrm{Cov}[X+Y] & =\E[(X+Y-\E[X+Y])(X+Y-\E[X+Y])^{T}]\\
 & =\E[(X-\E[X]+Y-\E[Y])(X-\E[X]+Y-\E[Y])^{T}]\\
 & =\mathrm{Cov}[X]+\mathrm{Cov}[Y]+\E[(X-\E[X])(Y-\E[Y])^{T}]+\E[(Y-\E[Y])(X-\E[X])^{T}]\\
 & =\mathrm{Cov}[X]+\mathrm{Cov}[Y]+\E[(X-\E[X])\E[(Y-\E[Y])^{T}]+\E[(Y-\E[Y]]\E[X-\E[X]]^{T}\tag{since \ensuremath{X\perp Y}}\\
 & =\mathrm{Cov}[X]+\mathrm{Cov}[Y].
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.28 - Conditional distribution from joint gaussian
\end_layout

\end_inset

 Consider a joint distribution over the variable 
\begin_inset Formula 
\[
z=\begin{bmatrix}x\\
y
\end{bmatrix}
\]

\end_inset

whose mean and covariance are given by (2.108) and (2.105) respectively.
 By making use of the results (2.92) and (2.93) show that the marginal distributio
n 
\begin_inset Formula $f(x)$
\end_inset

 is given (2.99).
 Similarly, by making use of the results (2.81) and (2.82) show that the condition
al distribution 
\begin_inset Formula $f(y\vert x)$
\end_inset

 is given by (2.100).
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
This problem can be solved using the hint provided by the book which adopts
 the technique of completing squares.
 However, here we will solve it using the theory we have developed in a
 few previous exercises.
 But first, we would like to rephrase this problem to make things clearer:
 given a joint normal variable 
\begin_inset Formula $Z$
\end_inset

 such that 
\begin_inset Formula 
\[
Z=\begin{bmatrix}X\\
Y
\end{bmatrix}\sim\mathrm{MVN}\left(\mu_{Z}:=\begin{bmatrix}\mu\\
A\mu+b
\end{bmatrix},\Sigma_{Z}:=\begin{bmatrix}\Lambda^{-1} & \Lambda^{-1}A^{T}\\
A\Lambda^{-1} & L^{-1}+A\Lambda^{-1}A^{T}
\end{bmatrix}\right),
\]

\end_inset

find the conditional distribution of 
\begin_inset Formula $Y\vert X,$
\end_inset

 which we denote as 
\begin_inset Formula $f_{Y\vert X}(y\vert x).$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\shape default

\begin_inset Newline newline
\end_inset

Without loss generality, assume that 
\begin_inset Formula $X\in\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $Y\in\mathbb{R}^{m}$
\end_inset

.
 Since 
\begin_inset Formula $\begin{bmatrix}0 & I_{m}\\
I_{n} & 0
\end{bmatrix}\begin{bmatrix}X\\
Y
\end{bmatrix}=\begin{bmatrix}Y\\
X
\end{bmatrix},$
\end_inset

 it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that 
\begin_inset Formula 
\begin{align*}
\begin{bmatrix}Y\\
X
\end{bmatrix} & \sim\mathrm{MVN}\left(\widehat{\mu}=\begin{bmatrix}0 & I_{m}\\
I_{n} & 0
\end{bmatrix}\begin{bmatrix}\mu\\
A\mu+b
\end{bmatrix},\widetilde{\Sigma}=\begin{bmatrix}0 & I_{m}\\
I_{n} & 0
\end{bmatrix}\begin{bmatrix}\Lambda^{-1} & \Lambda^{-1}A^{T}\\
A\Lambda^{-1} & L^{-1}+A\Lambda^{-1}A^{T}
\end{bmatrix}\begin{bmatrix}0 & I_{m}\\
I_{n} & 0
\end{bmatrix}^{T}\right)\\
 & =\mathrm{MVN}\left(\widetilde{\mu}=\begin{bmatrix}A\mu+b\\
\mu
\end{bmatrix},\widetilde{\Sigma}=\begin{bmatrix}A\Lambda^{-1} & L^{-1}+A\Lambda^{-1}A^{T}\\
\Lambda^{-1} & \Lambda^{-1}A^{T}
\end{bmatrix}\begin{bmatrix}0 & I_{n}\\
I_{m} & 0
\end{bmatrix}\right)\\
 & =\mathrm{MVN}\left(\widetilde{\mu}=\begin{bmatrix}A\mu+b\\
\mu
\end{bmatrix},\widetilde{\Sigma}=\begin{bmatrix}L^{-1}+A\Lambda^{-1}A^{T} & A\Lambda^{-1}\\
\Lambda^{-1}A^{T} & \Lambda^{-1}
\end{bmatrix}\right).
\end{align*}

\end_inset

Then we can apply 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: property of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

-(3) and get 
\begin_inset Formula 
\begin{align*}
Y\vert X=x & \sim\mathrm{MVN}(\mu_{Y\vert X=x}=A\mu_{1}+b+(A\Lambda^{-1})\Lambda(x-\mu),\Sigma_{Y\vert X}=L^{-1}+A\Lambda^{-1}A^{T}-A\Lambda^{-1}\Lambda\Lambda^{-1}A^{T})\\
 & =\mathrm{MVN}(\mu_{Y\vert X=x}=Ax+b,\Sigma_{Y\vert X}=L^{-1}.
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.29 - Verify Eq.(2.105) 
\end_layout

\end_inset

 Using the partitioned matrix inversion formula (2.76), show that the inverse
 of the precision matrix (2.104) is given by the covariance matrix (2.105).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Recall from Problem 2.24, for an arbitrary block matrix of the form 
\begin_inset Formula $\begin{bmatrix}A & B\\
C & D
\end{bmatrix}$
\end_inset

 has inverse of form 
\begin_inset Formula 
\[
\begin{bmatrix}A & B\\
C & D
\end{bmatrix}^{-1}=\begin{bmatrix}M & -MBD^{-1}\\
-D^{-1}CM & D^{-1}+D^{-1}CMBD^{-1}
\end{bmatrix},
\]

\end_inset

where 
\begin_inset Formula $M=(A-BDC^{-1})^{-1}.$
\end_inset

 Therefore, we have 
\begin_inset Formula 
\[
\begin{bmatrix}\Lambda+A^{T}LA & -A^{T}L\\
-LA & L
\end{bmatrix}^{-1}=\begin{bmatrix}\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix},
\]

\end_inset

where 
\begin_inset Formula 
\begin{align*}
\Sigma_{11} & =(\Lambda+A^{T}LA-A^{T}LL^{-1}LA)=\Lambda;\\
\Sigma_{12} & =-\Lambda^{-1}A^{T}LL^{-1}=-\Lambda^{-1}A^{T};\\
\Sigma_{21} & =-L^{-1}LA\Lambda=A\Lambda;\\
\Sigma_{22} & =L^{-1}+L^{-1}(-LA)\Lambda(-A^{T}L)L^{-1}=L^{-1}+A\Lambda A^{T}.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.30 - Verify Eq.(2.108) 
\end_layout

\end_inset

 By starting from (2.107) and making use of the result (2.105), verify the
 result (2.108).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that we have 
\begin_inset Formula 
\begin{align*}
\E[Z] & =R^{-1}\begin{bmatrix}\Lambda\mu-A^{T}Lb\\
Lb
\end{bmatrix}=\begin{bmatrix}\Lambda^{-1} & \Lambda^{-1}A^{T}\\
A\Lambda^{-1} & L^{-1}+A\Lambda^{-1}A
\end{bmatrix}\begin{bmatrix}\Lambda\mu-A^{T}Lb\\
Lb
\end{bmatrix}\\
 & =\begin{bmatrix}\Lambda^{-1}\Lambda\mu-\Lambda^{-1}A^{T}Lb+\Lambda^{-1}A^{T}Lb\\
A\Lambda^{-1}\Lambda\mu-A\Lambda^{-1}A^{T}Lb+L^{-1}Lb+A\Lambda^{-1}ALb
\end{bmatrix}\\
 & =\begin{bmatrix}\mu\\
A\mu+b
\end{bmatrix}
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.31 - Sum of multivariate gaussian
\end_layout

\end_inset

 Consider two multidimensional random vectors x and z having Gaussian distributi
ons 
\begin_inset Formula $f(x)=\mathrm{N}(x\vert\mu_{x},\Sigma_{x})$
\end_inset

 and 
\begin_inset Formula $f(z)=\mathrm{N}(z\vert\mu_{z},\Sigma_{z})$
\end_inset

 respectively, together with their sum 
\begin_inset Formula $y=x+z$
\end_inset

.
 Use the results (2.109) and (2.110) to find an expression for the marginal
 distribution 
\begin_inset Formula $f(y)$
\end_inset

 by considering the linear-Gaussian model comprising the product of the
 marginal distribution 
\begin_inset Formula $f(x)$
\end_inset

 and the conditional distribution 
\begin_inset Formula $f(y\vert x)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we give a lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X_{1},X_{2}$
\end_inset

 be two independent 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variables.
 The characteristic function of 
\begin_inset Formula $X_{1}+X_{2}$
\end_inset

 can be represented as 
\begin_inset CommandInset label
LatexCommand label
name "lem: cf of two independent r.v."

\end_inset


\begin_inset Formula 
\[
\varphi_{X_{1}+X_{2}}(t)=\varphi_{X_{1}}(t)\varphi_{X_{2}}(t).
\]

\end_inset


\end_layout

\begin_layout Proof
We follow the definition and write 
\begin_inset Formula 
\[
\varphi_{X_{1}+X_{2}}(t)=\E[e^{i\left\langle t,X_{1}+X_{2}\right\rangle }]=\E[e^{i\left\langle t,X_{1}\right\rangle }e^{i\left\langle t,X_{2}\right\rangle }]=\E[e^{i\left\langle t,X_{1}\right\rangle }]\E[e^{i\left\langle t,X_{2}\right\rangle }]=\varphi_{X_{1}}(t)\varphi_{X_{2}}(t)
\]

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
The solution to this problem is an direct application of this lemma.
 Since this is an important property, we pack it in a lemma below.
 In addition to having only two r.v., we also generalize it to an arbitrary
 number of r.v.s.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\{X_{i}\}_{i=1}^{m}$
\end_inset

 be 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued independent random variables with 
\begin_inset Formula $X_{i}\sim\mathrm{MVN}(\mu_{i},\Sigma_{i})$
\end_inset

.
 Then it follows that 
\begin_inset Formula $\sum_{i=1}^{m}X_{i}\sim\mathrm{MVN}(\sum_{i=1}^{m}\mu_{i},\sum_{i=1}^{m}\Sigma_{i})$
\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "lem: sum of independent MVN"

\end_inset


\end_layout

\begin_layout Proof
To prove the claim, we induct on 
\begin_inset Formula $m.$
\end_inset

 For the base case 
\begin_inset Formula $m=2,$
\end_inset

 we note that since 
\begin_inset Formula $X_{1},X_{2}$
\end_inset

 are independent, it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of two independent r.v."
plural "false"
caps "false"
noprefix "false"

\end_inset

 that 
\begin_inset Formula 
\begin{align*}
\varphi_{X_{1}+X_{2}}(t) & =\varphi_{X_{1}}(t)\varphi_{X_{2}}(t)\\
 & =\exp(i\left\langle t,\mu_{1}\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma_{1}t,t\right\rangle \right)\exp(i\left\langle t,\mu_{2}\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma_{2}t,t\right\rangle \right)\\
 & =\exp(i\left\langle t,\mu_{1}+\mu_{2}\right\rangle )\exp\left(-\frac{1}{2}\left\langle (\Sigma_{1}+\Sigma_{2})t,t\right\rangle \right).
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula $X_{1}+X_{2}\sim\mathrm{MVN}(\mu_{1}+\mu_{2},\Sigma_{1}+\Sigma_{2}).$
\end_inset

 Now suppose the claim holds for 
\begin_inset Formula $m=k.$
\end_inset

 Then for 
\begin_inset Formula $m=k+1,$
\end_inset

 we first note that by inductive hypothesis 
\begin_inset Formula $\sum_{i=1}^{k}X_{i}\sim\mathrm{MVN}(\sum_{i=1}^{k}\mu_{i},\sum_{i=1}^{k}\Sigma_{i}).$
\end_inset

 By (??QUOTE), 
\begin_inset Formula $(\sum_{i=1}^{k}X_{i})\perp X_{k+1},$
\end_inset

 and as a result 
\begin_inset Formula 
\begin{align*}
\varphi_{\sum_{i=1}^{k+1}X_{i}}(t) & =\varphi_{\sum_{i=1}^{k}X_{i}}(t)\varphi_{X_{k+1}}(t)\\
 & =\exp\left(i\left\langle t,\sum_{i=1}^{k}\mu_{i}\right\rangle \right)\exp\left(-\frac{1}{2}\left\langle \left(\sum_{i=1}^{k}\Sigma_{k}\right)t,t\right\rangle \right)\exp(i\left\langle t,\mu_{k+1}\right\rangle )\exp\left(-\frac{1}{2}\left\langle \Sigma_{k+1}t,t\right\rangle \right)\\
 & =\exp\left(i\left\langle t,\sum_{i=1}^{k+1}\mu_{i}\right\rangle \right)\exp\left(-\frac{1}{2}\left\langle \left(\sum_{i=1}^{k+1}\Sigma_{k}t,t\right)\right\rangle \right)
\end{align*}

\end_inset

and as a result 
\begin_inset Formula $\sum_{i=1}^{k+1}X_{i}\sim\mathrm{MVN}(\sum_{i=1}^{k+1}\mu_{i},\sum_{i=1}^{k+1}\Sigma_{i}).$
\end_inset

 By the principle of mathematical induction, we conclude that the desired
 result follows.
\end_layout

\begin_layout Proof
Therefore, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: sum of independent MVN"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $X+Z\sim\mathrm{MVN}(\mu_{x}+\mu_{z},\Sigma_{x}+\Sigma_{z}).$
\end_inset

 
\end_layout

\begin_layout Subsection*
Extensions 
\end_layout

\begin_layout Standard
From this problem, we have shown that sum of independent Gaussians is also
 Gaussian.
 There are some further implications can be derived, which are also useful.
 The first result is a necessary and sufficient condition for a random vector
 to be multivariate gaussian.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X=(X_{1},X_{2},\ldots,X_{n})$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued random variable.
 Then 
\begin_inset Formula $X$
\end_inset

 is multivariate gaussian if and only if 
\begin_inset Formula $\left\langle t,X\right\rangle $
\end_inset

 is univariate gaussian for any 
\begin_inset Formula $t\in\mathbb{R}^{n}-\{0\}.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "lem: necessary and sufficent condition to be MVN"

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula $\Rightarrow$
\end_inset

 Suppose that 
\begin_inset Formula $X$
\end_inset

 is multivariate gaussian.
 Then by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: distribution of linear transformation of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\left\langle t,X\right\rangle =t^{T}X$
\end_inset

, being a linear transformation of 
\begin_inset Formula $X,$
\end_inset

 is also gaussian.
 Moreover, it is univariate gaussian since 
\begin_inset Formula $t^{T}X\in\mathbb{R}$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $\Leftarrow$
\end_inset

 On the other hand, suppose 
\begin_inset Formula $\left\langle t,X\right\rangle $
\end_inset

 is univariate gaussian for any 
\begin_inset Formula $t\in\mathbb{R}^{n}-\{0\}.$
\end_inset

 Then it follows that 
\begin_inset Formula $X_{i}\sim\mathrm{N}(\mu_{i},\Sigma_{i})$
\end_inset

 by letting 
\begin_inset Formula $t$
\end_inset

 be 
\begin_inset Formula $e_{i}$
\end_inset

 for 
\begin_inset Formula $i=1,...,n,$
\end_inset

 where 
\begin_inset Formula $e_{i}$
\end_inset

 is the standard basis vector in 
\begin_inset Formula $\mathbb{R}^{n}.$
\end_inset

 Therefore, 
\begin_inset Formula $\E\left|X_{i}\right|<\infty,\E[X_{i}^{2}]<\infty$
\end_inset

 and by Cauchy Schwartz 
\begin_inset Formula $\E[X_{i}X_{j}]\leq\E\left|X_{i}X_{j}\right|\leq\E\left|X_{i}\right|^{1/2}\E\left|X_{j}\right|^{1/2}<\infty$
\end_inset

 for any 
\begin_inset Formula $i,j=1,...,n$
\end_inset

.
 Therefore, 
\begin_inset Formula $\E[X]$
\end_inset

 and 
\begin_inset Formula $\mathrm{Cov}[X]$
\end_inset

 (the variance covariance matrix) is finite.
 We denote 
\begin_inset Formula $\E[X]=\mu_{X}$
\end_inset

 and 
\begin_inset Formula $\mathrm{Cov}[X]=\Sigma_{X}.$
\end_inset

 Note that with the notation defined above, 
\begin_inset Formula $t^{T}X\sim\mathrm{N}(t^{T}\mu_{X},t^{T}\Sigma_{X}t)$
\end_inset

 for any 
\begin_inset Formula $t\in\mathbb{R}^{n}-\{0\}$
\end_inset


\begin_inset Formula 
\[
\varphi_{t^{T}X}(r)=\E[\exp(irt^{T}X)]=\exp(irt^{T}\mu_{X})\exp\left(-\frac{r^{2}}{2}t^{T}\Sigma_{X}t\right).
\]

\end_inset

Also note that 
\begin_inset Formula $\varphi_{X}(t)=\E[\exp(i\left\langle t,X\right\rangle )]=\E[\exp(it^{T}X)]=\varphi_{t^{T}X}(1).$
\end_inset

 It follows that 
\begin_inset Formula 
\[
\varphi_{X}(t)=\exp(it^{T}\mu_{X})\exp\left(-\frac{1}{2}t^{T}\Sigma_{X}t\right).
\]

\end_inset

Hence, 
\begin_inset Formula $X\sim\mathrm{MVN}(\mu_{X},\Sigma_{X})$
\end_inset

 in view of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: cf of gaussian"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
An direct application of the previous lemma is that fact that if we stack
 up independent multi dimensional gaussians, the stacked up vector will
 also be random gaussian, which we state formally as a corollary below.
 It should be noted as this result can also be shown using characteristic
 functions directly.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\{X_{i}\}_{i=1}^{m}$
\end_inset

 be a collection of independent random variables such that 
\begin_inset Formula $X_{i}\in\mathbb{R}^{n_{i}}$
\end_inset

 and 
\begin_inset Formula $X_{i}\sim\mathrm{MVN}(\mu_{i},\Sigma_{i}).$
\end_inset

 Then 
\begin_inset Formula 
\[
X=(X_{1},...,X_{m})\sim\mathrm{MVN}\left(\mu_{X}=\begin{bmatrix}\mu_{1}\\
\vdots\\
\mu_{m}
\end{bmatrix},\Sigma_{X}=\begin{bmatrix}\Sigma_{1} & 0 & 0 & 0\\
0 & \Sigma_{2} & 0 & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & 0 & \Sigma_{n}
\end{bmatrix}\right).
\]

\end_inset


\end_layout

\begin_layout Proof
For any 
\begin_inset Formula $t\in\mathbb{R}^{\sum_{i=1}^{m}n_{i}}$
\end_inset

, we note that by triangularizing it follows that 
\begin_inset Formula 
\[
\left\langle t,X\right\rangle =\sum_{i=1}^{\sum_{i=1}^{m}n_{i}}t_{i}X_{i}=\sum_{i=1}^{m}\bigg(\sum_{j=1}^{n_{i}}t_{ij}X_{ij}\bigg).
\]

\end_inset

Since by assumption 
\begin_inset Formula $X_{i}=(X_{i1},X_{i2},...,X_{in_{i}})$
\end_inset

 is gaussian, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: necessary and sufficent condition to be MVN"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it follow that 
\begin_inset Formula $\sum_{j=1}^{n_{i}}t_{ij}X_{ij}$
\end_inset

 is univariate gaussian.
 By well a well known result (for example c.f.
 QUOTE independence), 
\begin_inset Formula $Y_{i}:=\sum_{j=1}^{n_{i}}t_{ij}X_{ij}\perp Y_{i'}=\sum_{j=1}^{n_{i'}}t_{i'j}X_{i'j}$
\end_inset

 for 
\begin_inset Formula $i\neq i'.$
\end_inset

 Then by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: sum of independent MVN"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it follows that 
\begin_inset Formula $\left\langle t,X\right\rangle =\sum_{i=1}^{m}(\sum_{j=1}^{n_{i}}t_{ij}X_{ij})$
\end_inset

 is gaussian.
 Therefore, 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: necessary and sufficent condition to be MVN"
plural "false"
caps "false"
noprefix "false"

\end_inset

 implies that 
\begin_inset Formula $X$
\end_inset

 is multivariate gaussian.
 That 
\begin_inset Formula $\mu_{X}$
\end_inset

 and 
\begin_inset Formula $\Sigma_{X}$
\end_inset

 are of the form claimed can be verified by direct computation, which we
 left an easy exercise.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.32 - Completing the squares trick for gaussian - 1
\end_layout

\end_inset

 This exercise and the next provide practice at manipulating the quadratic
 forms that arise in linear-Gaussian models, as well as giving an independent
 check of results derived in the main text.
 Consider a joint distribution 
\begin_inset Formula $f(x,y)$
\end_inset

 defined by the marginal and conditional distributions given by (2.99) and
 (2.100).
 By examining the quadratic form in the exponent of the joint distribution,
 and using the technique of "completing the square" discussed in Section
 2.3, find expressions for the mean and covariance of the marginal distribution
 
\begin_inset Formula $f(y)$
\end_inset

 in which the variable 
\begin_inset Formula $x$
\end_inset

 has been integrated out.
 To do this, make use of the Woodbury matrix inversion formula (2.289).
 Verify that these results agree with (2.109) and (2.110) obtained using the
 results of Chapter 2.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we give a lemma that formalize the process of "completing the squares"
 for gaussian distribution.
 Note that most of the derivation of the results about Gaussian distribution
 in the book can be attributed to this lemma, although it is not mentioned
 explicitly in the book.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

-valued absolutely continuous random variable with density function denoted
 as 
\begin_inset Formula $f_{X}.$
\end_inset

 If 
\begin_inset Formula $f_{X}(x)\propto\exp(-\frac{1}{2}q(x)),$
\end_inset

 where 
\begin_inset Formula $q(x)=x^{T}\Lambda x-2b^{T}x+c$
\end_inset

, in which 
\begin_inset Formula $\Lambda\in\mathrm{Mat}_{\mathbb{R}}(n,n)$
\end_inset

 and 
\begin_inset Formula $\Lambda$
\end_inset

 is positive definite, 
\begin_inset Formula $b\in\mathbb{R}^{n},c\in\mathbb{R},$
\end_inset

 then 
\begin_inset Formula $X\sim\mathrm{MVN}(\Lambda^{-1}b,\Lambda^{-1}).$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "lem: gaussian completion of squares"

\end_inset


\end_layout

\begin_layout Proof
First, note that by completing square we have that 
\begin_inset Formula 
\[
q(x)=x^{T}\Lambda x-2b^{T}x+c=(x-\Lambda^{-1}b)^{T}\Lambda(x-\Lambda^{-1}b)-b^{T}\Lambda^{-1}b+c.
\]

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\exp\left(-\frac{1}{2}q(x)\right) & =\exp\left(-\frac{1}{2}(x-\Lambda^{-1}b)^{T}\Lambda(x-\Lambda^{-1}b)\right)\exp\left(-\frac{1}{2}(b^{T}\Lambda^{-1}b-c)\right)\\
 & \propto\exp\left(-\frac{1}{2}(x-\Lambda^{-1}b)\Lambda(x-\Lambda^{-1}b)\right).
\end{align*}

\end_inset

Now note that 
\begin_inset Formula $f_{X}(x)$
\end_inset

, being a density should integrate to 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula 
\[
\int\frac{1}{(2\pi)^{n/2}\left|\det\Lambda^{-1}\right|^{1/2}}\exp\left(-\frac{1}{2}(x-\Lambda^{-1}b)\Lambda(x-\Lambda^{-1}b)\right)dx=1,\tag{1}
\]

\end_inset

it follows that 
\begin_inset Formula $f_{X}(x)$
\end_inset

 should be the gaussian density proposed in Eq.(1).
 Hence, it follows that 
\begin_inset Formula $X\sim\mathrm{MVN}(\Lambda^{-1}b,\Lambda^{-1}).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
With this lemma, we now solve this problem.
 Note that 
\begin_inset Formula 
\begin{align*}
f_{(X,Y)}(x,y) & =f_{X}(x)f_{Y\vert X}(y\vert x)\\
 & =C\exp\bigg(\underbrace{-\frac{1}{2}(x-\mu)^{T}\Lambda(x-\mu)-\frac{1}{2}(y-(Ax+b))L^{T}(y-(Ax+b))}_{:=Q(x,y)}\bigg),
\end{align*}

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 is some normalizing constant.
 Now we message 
\begin_inset Formula $Q(x,y)$
\end_inset

 a little bit by grouping all terms related to 
\begin_inset Formula $x$
\end_inset

 together: 
\begin_inset Formula 
\begin{align*}
Q(x,y) & =-\frac{1}{2}\left[(x-\mu)^{T}\Lambda(x-\mu)+(y-(Ax+b))^{T}L(y-(Ax+b))\right]\\
 & =-\frac{1}{2}\left[x^{T}\Lambda x-2x^{T}\Lambda\mu+\mu^{T}\Lambda\mu+y^{T}Ly-2y^{T}L(Ax+b)+(Ax+b)^{T}L(Ax+b)\right]\\
 & =-\frac{1}{2}\left[x^{T}\Lambda x+x^{T}A^{T}LAx+2x^{T}A^{T}Lb-2x^{T}A^{T}Ly+2x^{T}\Lambda\mu-2y^{T}Lb+y^{T}Ly+\mu^{T}\Lambda\mu+b^{T}Lb\right]\\
 & =-\frac{1}{2}\left[x^{T}(\Lambda+A^{T}LA)x+2x^{T}(\Lambda\mu+A^{T}Lb-A^{T}Ly)\right]+R(y)\\
 & =-\frac{1}{2}\left[x^{T}(\Lambda+A^{T}LA)x-2x^{T}(\Lambda\mu+A^{T}L(y-b))\right]+R(y)\tag{1}\\
 & =-\frac{1}{2}\left[(x-\widetilde{\mu})^{T}(\Lambda+A^{T}LA)(x-\widetilde{\mu})+(\Lambda\mu+A^{T}L(y-b))^{T}(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))\right]+R(y)\\
 & =-\frac{1}{2}(x-\widetilde{\mu})^{T}(\Lambda+A^{T}LA)(x-\widetilde{\mu})\\
 & \ \ +\frac{1}{2}(\Lambda\mu+A^{T}L(y-b))^{T}(\Lambda+A^{T}LA)^{-1}(\Lambda+A^{T}LA)(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))+R(y)\\
 & =\underbrace{-\frac{1}{2}(x-\widetilde{\mu})^{T}(\Lambda+A^{T}LA)(x-\widetilde{\mu})}_{:=\mathcal{H}_{1}(x)}+\underbrace{\frac{1}{2}\widetilde{\mu}^{T}(\Lambda+A^{T}LA)\widetilde{\mu}+R(y)}_{:=\mathcal{H}_{2}(y)}.
\end{align*}

\end_inset

where 
\begin_inset Formula $\widetilde{\mu}=(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))$
\end_inset

 and 
\begin_inset Formula $R(y)=-\frac{1}{2}\mu^{T}\Lambda\mu-\frac{1}{2}b^{T}Lb-\frac{1}{2}y^{T}Ly+y^{T}Lb.$
\end_inset

 Therefore, we have that 
\begin_inset Formula 
\begin{align*}
f_{Y}(y) & =\int f_{(X,Y)}(x,y)dx=\int\frac{1}{(2\pi)^{n/2}\left|\det(\Lambda+A^{T}LA)\right|^{1/2}}\exp\left(\mathcal{H}_{1}(x)\right)dx\cdot\widetilde{C}\exp(\mathcal{H}_{2}(y))\\
 & =\widetilde{C}\exp(\mathcal{H}_{2}(y)).
\end{align*}

\end_inset

where 
\begin_inset Formula $\widetilde{C}$
\end_inset

 a new normalizing constant.
 Now we do the completion of the square again for 
\begin_inset Formula $f_{Y}(y)$
\end_inset

 as follows: 
\begin_inset Formula 
\begin{align*}
 & f_{Y}(y)\\
=\ \  & \widetilde{C}\exp(\widetilde{\mu}^{T}(\Lambda+A^{T}LA)\widetilde{\mu}+R(y))\\
\propto\ \  & \exp\left(\frac{1}{2}[(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))]^{T}(\Lambda+A^{T}LA)[(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))]\right)\\
 & \times\exp\left(-\frac{1}{2}\mu^{T}\Lambda\mu-\frac{1}{2}b^{T}Lb-\frac{1}{2}y^{T}Ly+2y^{T}Lb\right)\\
=\ \  & \exp\left(\frac{1}{2}(\Lambda\mu+A^{T}L(y-b)){}^{T}(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b))\right)\\
 & \times\exp\left(-\frac{1}{2}\mu^{T}\Lambda\mu-\frac{1}{2}b^{T}Lb-\frac{1}{2}y^{T}Ly+y^{T}Lb\right)\\
\propto\ \  & \exp\bigg(-\frac{1}{2}y^{T}Ly+y^{T}Lb+\frac{1}{2}y^{T}LA(\Lambda+A^{T}LA)^{-1}A^{T}Ly+(\Lambda\mu)^{T}(\Lambda+A^{T}LA)^{-1}A^{T}Ly\\
 & \ \ \ \ \ \ -y^{T}LA(\Lambda+A^{T}LA)^{-1}A^{T}Lb\bigg)\\
=\ \  & \exp\left(-\frac{1}{2}y^{T}(L-LA(\Lambda+A^{T}LA)^{-1}A^{T}L)y+y^{T}[(L-LA(\Lambda+A^{T}LA)^{-1}A^{T}L)b+LA(\Lambda+A^{T}LA)^{-1}\Lambda\mu]\right).
\end{align*}

\end_inset

Therefore, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it follows that 
\begin_inset Formula $Y\sim\mathrm{MVN}(\mu_{Y},\Sigma_{Y})$
\end_inset

, where 
\begin_inset Formula 
\begin{align*}
\Sigma_{Y} & =(L-LA(\Lambda+A^{T}LA)^{-1}A^{T}L)^{-1}\\
 & =L^{-1}-L^{-1}(-LA)[(\Lambda+A^{T}LA)+A^{T}LL^{-1}(-LA)]^{-1}A^{T}LL^{-1}\tag{by Woodbury inversion formula}\\
 & =L^{-1}+A[\Lambda+A^{T}LA-A^{T}LA]^{-1}A^{T}\\
 & =L^{-1}+A\Lambda^{-1}A^{T},
\end{align*}

\end_inset

and 
\begin_inset Formula 
\begin{align*}
\mu_{Y} & =(L-LA(\Lambda+A^{T}LA)^{-1}A^{T}L)^{-1}[(L-LA(\Lambda+A^{T}LA)^{-1}A^{T}L)b+LA(\Lambda+A^{T}LA)^{-1}\Lambda\mu]\\
 & =(L^{-1}+A\Lambda^{-1}A^{T})[(L^{-1}+A\Lambda^{-1}A^{T})^{-1}b+LA(\Lambda+A^{T}LA)^{-1}\Lambda\mu]\\
 & =b+(L^{-1}+A\Lambda^{-1}A^{T})LA(\Lambda+A^{T}LA)^{-1}\Lambda\mu\\
 & =b+(A+A\Lambda^{-1}A^{T}LA)(\Lambda+A^{T}LA)^{-1}\Lambda\mu=b+A(I+\Lambda^{-1}A^{T}LA)(I+\Lambda^{-1}A^{T}LA)^{-1}\Lambda^{-1}\Lambda\mu\\
 & =A\mu+b.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.33 - Completing the squares trick for gaussian - 2 
\end_layout

\end_inset

 Consider the same joint distribution as in Exercise 2.32, but now use the
 technique of completing the square to find expressions for the mean and
 covariance of the conditional distribution 
\begin_inset Formula $f(x\vert y)$
\end_inset

.
 Again, verify that these agree with the corresponding expressions (2.111)
 and (2.112).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, using the same argument as in Problem 2.32 we see that 
\begin_inset Formula 
\[
f_{(X,Y)}(x,y)=C\exp\bigg(\underbrace{-\frac{1}{2}(x-\mu)^{T}\Lambda(x-\mu)-\frac{1}{2}(y-(Ax+b))L^{T}(y-(Ax+b))}_{:=Q(x,y)}\bigg).
\]

\end_inset

And following Eq.(1) in Problem 2.33, we have 
\begin_inset Formula 
\begin{align*}
Q(x,y) & =-\frac{1}{2}\left[x^{T}(\Lambda+A^{T}LA)x-2x^{T}(\Lambda\mu+A^{T}L(y-b))\right]+R(y).
\end{align*}

\end_inset

Then it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that 
\begin_inset Formula 
\[
X\vert Y=y\sim\mathrm{MVN}(\mu_{X|Y}=(\Lambda+A^{T}LA)^{-1}(\Lambda\mu+A^{T}L(y-b)),\Sigma_{X|Y}=(\Lambda+A^{T}LA)^{-1}.
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.34 - MLE of covariance matrix for multivariate gaussian 
\end_layout

\end_inset

 To find the maximum likelihood solution for the covariance matrix of a
 multivariate Gaussian, we need to maximize the log likelihood function
 (2.118) with respect to 
\begin_inset Formula $\Sigma$
\end_inset

, noting that the covariance matrix must be symmetric and positive definite.
 Here we proceed by ignoring these constraints and doing a straightforward
 maximization.
 Using the results (C.21), (C.26), and (C.28) from Appendix C, show that the
 covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 that maximizes the log likelihood function (2.118) is given by the sample
 covariance (2.122).
 We note that the final result is necessarily symmetric and positive definite
 (provided the sample covariance is nonsingular).
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\shape italic
This problem involves differentiating the log-determinant of a positive
 definite matrix.
 Although relevant formulas have been provided in the back of the book,
 they are somehow not very rigorous in its presentation.
 Hence, through the solution to this problem we would develop some results
 w.r.t to the determinant of the matrix determinants.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset

 
\series bold
Solution.

\series default
 Before we start proving results, we stop and recall the definition of determina
nt.
 The most generalized definition of determinant involves the notion of manifold
 and Lie groups.
 For practical purposes it suffices to define determinant in the way that
 characterizes it as a function as follows:
\end_layout

\begin_layout Theorem
There exists a unique function 
\begin_inset Formula $D:(\mathbb{R}^{n})^{n}\rightarrow\mathbb{R}$
\end_inset

 such that 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $D$
\end_inset

 is multilinear, i.e., 
\begin_inset Formula $D$
\end_inset

 is linear w.r.t to each of its arguments.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D$
\end_inset

 is anti-symmetric: Exchanging any two arguments changes its sign.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D$
\end_inset

 is normalized: 
\begin_inset Formula $D(e_{1},\ldots,e_{n})=1$
\end_inset

 the set of the standard basis vectors 
\begin_inset Formula $\{e_{i}\}_{i=1}^{n}$
\end_inset

 in 
\begin_inset Formula $\mathbb{R}^{n}.$
\end_inset


\end_layout

\begin_layout Standard
The determinant of an 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $A=[a_{1},a_{2},\ldots,a_{n}]$
\end_inset

 is defined as 
\begin_inset Formula $\det A=D(a_{1},\ldots,a_{n})$
\end_inset

 where 
\begin_inset Formula $a_{1},\ldots a_{n}\in\mathbb{R}^{n}$
\end_inset

 are the columns of the matrix 
\begin_inset Formula $A$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
In order the find the Frechet derivative of the determinant function, we
 need a specific characterization of the determinant function that is easy
 to work with.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $A=[a_{ij}]_{ij}\in\mathrm{Mat}_{\mathbb{R}}(n,n).$
\end_inset

 Then 
\begin_inset CommandInset label
LatexCommand label
name "lem: characterization of determinant"

\end_inset


\begin_inset Formula 
\[
\det A=\sum_{\sigma\in S_{n}}\mathrm{sgn}(\sigma)a_{1,\sigma(1)}\ldots a_{n,\sigma(n)},
\]

\end_inset

where 
\begin_inset Formula $S_{n}$
\end_inset

 is the permutation group.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
With the previous two well-known results, we are ready the calculate the
 derivative of the determinant function.
 
\end_layout

\begin_layout Lemma
The following results about the determinant function are true:
\begin_inset CommandInset label
LatexCommand label
name "lem: log-det differential"

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The determinant function is Frechet differentiable.
\end_layout

\begin_layout Enumerate
The derivative of the determinant at the identity is given by 
\begin_inset Formula $[\mathrm{D}\det(I)]B=\mathrm{tr}(B).$
\end_inset

 
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $\det A\neq0,$
\end_inset

 then 
\begin_inset Formula $[\mathrm{D}\det(A)]B=\det A\cdot\mathrm{tr}(A^{-1}B).$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Proof
We first find the Frechet differential of the determinant function at the
 identity matrix, 
\begin_inset Formula $I$
\end_inset

.
 Since all norms are equivalent, it suffices to find the Frechet derivative
 w.r.t to an arbitrarily chosen norm.
 For convenience, we choose this norm to be the max norm, 
\begin_inset Formula $\left\Vert A\right\Vert _{\infty}=\max_{i,j}|a_{i,j}|.$
\end_inset

 Now consider an arbitrary matrix 
\begin_inset Formula $H,$
\end_inset

 we see that 
\begin_inset Formula 
\begin{align*}
\det(I+H) & =\det\begin{bmatrix}1+h_{1,1} & h_{1,2} & \cdots & h_{1,n}\\
h_{2,1} & 1+h_{2,2} & \cdots & h_{2,n}\\
\vdots & \cdots & \ddots & \vdots\\
h_{n,1} & h_{n,2} & \cdots & 1+h_{n,n}
\end{bmatrix}\\
 & =\sum_{\sigma\in S_{n}}\mathrm{sgn}(\sigma)\prod_{i=1}^{n}(I+H)_{i,\sigma(i)}\\
 & =\mathrm{sgn}(\mathrm{id})\prod_{i=1}^{n}(I+H)_{i,i}+\sum_{\sigma\in S_{n},\sigma\neq\mathrm{id}}\prod_{i=1}^{n}(I+H)_{i,\sigma(i)}\\
 & =\prod_{i=1}^{n}(1+h_{i,i})+\sum_{\sigma\in S_{n},\sigma\neq\mathrm{id}}\prod_{i=1}^{n}(I+H)_{i,\sigma(i).}\tag{1}
\end{align*}

\end_inset

Now we analysis the terms in Eq.(1) term by term: 
\begin_inset Formula 
\begin{align*}
\prod_{i=1}^{n}(1+h_{i,i}) & =1+\sum_{i=1}^{n}h_{i,i}+\underbrace{\sum_{i_{1},i_{2}\in\{1,...,n\},i_{1}\neq i_{2}}h_{i_{1}}h_{i_{2}}}_{\leq\binom{n}{2}\left\Vert H\right\Vert _{\infty}^{2}=o(\left\Vert H\right\Vert _{\infty})}+\cdots+\underbrace{h_{i_{1}}h_{i_{2}}\ldots h_{i_{n}}}_{\leq\left\Vert H\right\Vert ^{n}=o(\left\Vert H\right\Vert _{\infty})}\\
 & =\det I+\mathrm{tr}(H)+o(\left\Vert H\right\Vert _{\infty}).
\end{align*}

\end_inset

On the other hand, note that for 
\begin_inset Formula $\sigma\notin\mathrm{id}$
\end_inset

, there are at least two terms of 
\begin_inset Formula $(I+H)_{i,\sigma(i)}$
\end_inset

 are off diagonal since every permutation can be written as a product of
 disjoint cycles and identity element is the only element in the permutation
 group that has a representation as product of 
\begin_inset Formula $1$
\end_inset

-cycles
\begin_inset Foot
status open

\begin_layout Plain Layout
this is because group identity is unique.
 
\end_layout

\end_inset

; so 
\begin_inset Formula $\sigma$
\end_inset

 has at least one 
\begin_inset Formula $2$
\end_inset

 cycle in its product representation, which means there exists at least
 one pair 
\begin_inset Formula $(i,j)$
\end_inset

 with 
\begin_inset Formula $i\neq j$
\end_inset

 such that 
\begin_inset Formula $(I+H)_{i,\sigma(i)}=(I+H)_{i,j}=h_{i,j}$
\end_inset

 and similarly 
\begin_inset Formula $(I+H)_{j,\sigma(j)}=h_{j,i}$
\end_inset

.
 As a result, 
\begin_inset Formula 
\[
\prod_{i=1}^{n}(I+H)_{i,\sigma(i)}=h_{i}h_{j}\prod_{k\in\{1,...,n\}-\{i,j\}}(I+H)_{i,\sigma(i)}\leq\left\Vert H\right\Vert _{\infty}^{2}\prod_{k\in\{1,...,n\}-\{i,j\}}(I+H)_{i,\sigma(i)}=o(\left\Vert H\right\Vert _{\infty})
\]

\end_inset

Therefore, it follows that 
\begin_inset Formula 
\[
\mathrm{Eq}.(1)=\det I+\mathrm{tr}(H)+o(\left\Vert H\right\Vert _{\infty}).
\]

\end_inset

Hence, by definition we have 
\begin_inset Formula $[\mathrm{D}\det I](H)=\mathrm{tr}(H)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Now we find the Frechet differential of the determinant function at any
 invertible matrix 
\begin_inset Formula $A$
\end_inset

.To start with, we note that 
\begin_inset Formula 
\begin{align*}
\det(A+H) & =\det A\det(I+A^{-1}H)=\det A(\det I+\mathrm{tr}(A^{-1}H)+o(\left\Vert A^{-1}H\right\Vert ))\\
 & =\det A+\det A\cdot\mathrm{tr}(A^{-1}H)+o(\left\Vert A^{-1}H\right\Vert )\\
 & =\det A+\det(A)\cdot\mathrm{tr}(A^{-1}H)+o(\left\Vert H\right\Vert ),
\end{align*}

\end_inset

where 
\begin_inset Formula $\left\Vert \cdot\right\Vert $
\end_inset

 is any consistent norm of our choosing.
 Since 
\begin_inset Formula $\det(A)\cdot\mathrm{tr}(A^{-1}H)=\left\langle \det A\cdot A^{-1},H\right\rangle ,$
\end_inset

 it follow that 
\begin_inset Formula $[\mathrm{D}\det(A)](H)=\det$
\end_inset


\begin_inset Formula $\nabla\det(A)=\det A\cdot A^{-1}.$
\end_inset

 
\end_layout

\begin_layout Standard
We have an intermediate corollary.
 
\end_layout

\begin_layout Corollary
Let 
\begin_inset Formula $A\in\mathrm{GL}_{\mathbb{R}}(n,n).$
\end_inset

 Then 
\begin_inset Formula $\nabla\log\det A=A^{-1}.$
\end_inset

 
\end_layout

\begin_layout Proof
By the chain rule, 
\begin_inset Formula 
\[
\mathrm{D}(\log\det A)(H)=\frac{1}{\det A}\circ\mathrm{D}(\det A)=\frac{1}{\det A}\det A\left\langle A^{-1},H\right\rangle =\left\langle A^{-1},H\right\rangle .
\]

\end_inset

Therefore, it follows that 
\begin_inset Formula $\nabla\log\det A=A^{-1}.$
\end_inset


\end_layout

\begin_layout Standard
We still need one more result to reach fully rigorous solution to this problem;
 that is the differential of the function 
\begin_inset Formula $\mathrm{GL}_{\mathbb{R}}(n)\rightarrow\mathrm{GL}_{\mathbb{R}}(n)$
\end_inset

 defined by 
\begin_inset Formula $A\mapsto A^{-1}.$
\end_inset

 First, a lemma coupled with a definition.
 
\end_layout

\begin_layout Definition
A Banach algebra is an associative algebra with unit 
\begin_inset Formula $1$
\end_inset

 over complex (or real) numbers that is at the same time a Banach space,
 and so that the norm is sub-multiplicative and 
\begin_inset Formula $\left\Vert 1\right\Vert =1.$
\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem: neumann series"

\end_inset

Let 
\begin_inset Formula $\mathcal{A}$
\end_inset

 be a unital Banach algebra, and 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

 with 
\begin_inset Formula $\left\Vert a\right\Vert <1.$
\end_inset

 Then 
\begin_inset Formula $1-a$
\end_inset

 is invertible with inverse 
\begin_inset Formula $(1-a)^{-1}=\sum_{i=0}^{\infty}a^{n}.$
\end_inset


\end_layout

\begin_layout Proof
For each 
\begin_inset Formula $n\in\mathbb{N},$
\end_inset

 define the partial sums 
\begin_inset Formula $S_{n}=\sum_{i=0}^{n}a_{i}.$
\end_inset

 Note that 
\begin_inset Formula $\left\Vert a\right\Vert <1$
\end_inset

; so 
\begin_inset Formula $\left\Vert a^{n}\right\Vert \leq\left\Vert a\right\Vert ^{n}<1$
\end_inset

 and it follows that 
\begin_inset Formula $\sum_{i=0}^{\infty}\left\Vert a^{n}\right\Vert \leq\sum_{i=0}^{\infty}\left\Vert a\right\Vert ^{n}<\infty,$
\end_inset

 whence 
\begin_inset Formula $\sum_{i=0}^{\infty}a^{n}$
\end_inset

 absolutely convergent and thus convergent.
 We then denote 
\begin_inset Formula $S=\lim_{i=0}^{\infty}a_{i}=\lim_{n}S_{n}$
\end_inset

.
 Now we note that 
\begin_inset Formula 
\[
(1-a)S_{n}=(1-a)\left(\sum_{i=0}^{n}a^{i}\right)=1-a^{n+1}.
\]

\end_inset

Now we take the limit, it follows that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}(1-a)S_{n}=(1-a)\lim_{n\rightarrow\infty}S_{n}=(1-a)S=1-\lim_{n\rightarrow\infty}a^{n+1}=1.
\]

\end_inset

And similarly, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}[S_{n}(1-a)]=(\lim_{n\rightarrow}S_{n})(1-a)=S(1-a)=1-\lim_{n\rightarrow\infty}a^{n+1}=1.
\]

\end_inset

As a result, 
\begin_inset Formula $S=a^{-1}.$
\end_inset

 
\end_layout

\begin_layout Remark
A direct application of the this lemma is that in the context of matrix
 inversion.
 Let 
\begin_inset Formula $A\in\mathrm{GL}_{\mathbb{R}}(n);$
\end_inset

 if 
\begin_inset Formula $\left\Vert A\right\Vert <1$
\end_inset

, then it follows that 
\begin_inset Formula $(I-A)$
\end_inset

 is invertible and 
\begin_inset Formula $(I-A)^{-1}=\sum_{i=0}^{\infty}A^{i}.$
\end_inset

 
\end_layout

\begin_layout Standard
Now with the Neumann series, have the following lemma.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $f$
\end_inset

 be a function 
\begin_inset Formula $\mathrm{GL}_{\mathbb{R}}(n)\rightarrow\mathrm{GL}_{\mathbb{R}}(n):A\mapsto A^{-1}.$
\end_inset

 Then 
\begin_inset Formula $(\mathrm{D}f(A))(H)=-A^{-1}HA^{-1}.$
\end_inset


\end_layout

\begin_layout Proof
We fix 
\begin_inset Formula $\left\Vert \cdot\right\Vert $
\end_inset

 to be any consistent matrix norm.
 First, we note that for 
\begin_inset Formula $H\in\mathrm{GL}_{\mathbb{R}}(n)$
\end_inset

 such that 
\begin_inset Formula $\left\Vert A^{-1}H\right\Vert <\frac{1}{2}.$
\end_inset


\begin_inset Formula 
\begin{align*}
f(A+H) & =(A+H)^{-1}-A^{-1}=A(I-(-(A^{-1}H)^{-1})\\
 & =\left[A\left(\sum_{i=0}^{\infty}(-A^{-1}H)^{-i}\right)\right]^{-1}\\
 & =\left[\left(I-A^{-1}H+\sum_{i=2}^{\infty}(-1)^{i}(A^{-1}H)^{i}\right)A^{-1}\right]\\
 & =A^{-1}+(-A^{-1}HA^{-1})+\sum_{i=2}^{\infty}(-1)^{i}(A^{-1}H)^{i}A^{-1}.\tag{1}
\end{align*}

\end_inset

Next, note that 
\begin_inset Formula 
\begin{align*}
\left\Vert \sum_{i=2}^{\infty}(-1)^{i}(A^{-1}H)^{i}A^{-1}\right\Vert  & \leq\sum_{i=2}^{\infty}\left\Vert (A^{-1}H)^{i}A^{-1}\right\Vert \leq\left\Vert A^{-1}\right\Vert \sum_{i=2}^{\infty}\left\Vert (A^{-1}H)^{i}\right\Vert \leq\left\Vert A^{-1}\right\Vert \sum_{i=2}^{\infty}\left\Vert A^{-1}H\right\Vert ^{i}\\
 & =\left\Vert A^{-1}\right\Vert \left\Vert A^{-1}H\right\Vert \sum_{i=1}^{\infty}\left\Vert A^{-1}H\right\Vert ^{i}=\frac{\left\Vert A^{-1}H\right\Vert \left\Vert A^{-1}\right\Vert \left\Vert A^{-1}H\right\Vert }{1-\left\Vert A^{-1}H\right\Vert }\leq\frac{\left\Vert A^{-1}\right\Vert ^{3}\left\Vert H\right\Vert ^{2}}{1-\left\Vert A^{-1}H\right\Vert }\\
 & \leq2\left\Vert A^{-1}\right\Vert ^{3}\left\Vert H\right\Vert ^{2}\tag{since \ensuremath{\left\Vert A^{-1}H\right\Vert \leq\frac{1}{2}} by assumption}.
\end{align*}

\end_inset

Hence, it follows that 
\begin_inset Formula 
\[
\frac{\left\Vert \sum_{i=2}^{\infty}(-1)^{i}(A^{-1}H)^{i}A^{-1}\right\Vert }{\left\Vert H\right\Vert }\leq\frac{2\left\Vert A^{-1}\right\Vert ^{3}\left\Vert H\right\Vert ^{2}}{\left\Vert H\right\Vert }=2\left\Vert A^{-1}\right\Vert ^{3}\left\Vert H\right\Vert \xrightarrow{\left\Vert H\right\Vert \rightarrow0}0.
\]

\end_inset

It follows that 
\begin_inset Formula 
\[
f(A+H)=A^{-1}+(-A^{-1}HA^{-1})+o(\left\Vert H\right\Vert ).
\]

\end_inset

Since the function 
\begin_inset Formula $\mathrm{GL}_{\mathbb{R}}(n)\ni H\mapsto-A^{-1}HA^{-1}$
\end_inset

 is in 
\begin_inset Formula $\mathrm{Hom}(\mathrm{GL}_{\mathbb{R}}(n),\mathrm{GL}_{\mathbb{R}}(n)),$
\end_inset

 it follows that 
\begin_inset Formula $[\mathrm{D}f(A)]\circ H=-A^{-1}HA^{-1}.$
\end_inset

 
\end_layout

\begin_layout Standard
Now we come back to solve this problem.
 First, we write out the likelihood equation as follows: 
\begin_inset Formula 
\[
\mathcal{L}(\mu,\Sigma)=\prod_{i=1}^{d}\frac{1}{(2\pi)^{n/2}\det\Sigma^{1/2}}\exp\left(-\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)\right).
\]

\end_inset

Take the logarithm and we get 
\begin_inset Formula 
\begin{align*}
\ell(\mu,\Sigma) & =\sum_{i=1}^{d}\left[\log\frac{1}{(2\pi)^{n/2}\det\Sigma^{1/2}}-\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)\right]\\
 & =-\sum_{i=1}^{d}\log\frac{1}{(2\pi)^{n/2}}-\sum_{i=1}^{d}\log\frac{1}{\det\Sigma^{1/2}}-\sum_{i=1}^{d}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)\\
 & =-\frac{nd}{2}\log2\pi-\frac{d}{2}\log\det\Sigma-\sum_{i=1}^{d}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu).
\end{align*}

\end_inset

To find the critical points, we set the gradient to zero.
 First, we find the gradient.
 Note that by the product rule, we have that 
\begin_inset Formula 
\[
\mathrm{D}_{\Sigma}\ell(\mu,\Sigma)=-\frac{d}{2}(\mathrm{D}_{\Sigma^{-1}}\log\det\Sigma)-\frac{1}{2}\sum_{i=1}^{d}\mathrm{D}_{\Sigma^{-1}}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu).\tag{2 }
\]

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: log-det differential"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it follows that 
\begin_inset Formula $\mathrm{D}\log\det\Sigma=\Sigma^{-1}.$
\end_inset

 For the term 
\begin_inset Formula $(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu),$
\end_inset

 we see that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: prod rule"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset Formula 
\begin{align*}
\mathrm{\mathrm{D}}_{\Sigma}[(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)]\circ H & =\mathrm{D}_{\Sigma}\left\langle \Sigma^{-1}(x_{i}-\mu),(x_{i}-\mu)\right\rangle \circ H\\
 & =\mathrm{D}_{\Sigma}\left\langle \Sigma^{-1},(x_{i}-\mu)(x_{i}-\mu)^{T}\right\rangle \circ H\\
 & =\left\langle (\mathrm{D}_{\Sigma}\Sigma^{-1})\circ H,(x_{i}-\mu)(x_{i}-\mu)^{T}\right\rangle \\
 & =\left\langle (x_{i}-\mu)(x_{i}-\mu)^{T},-\Sigma^{-1}H\Sigma^{-1}\right\rangle \\
 & =\left\langle -\Sigma^{-1}(x_{i}-\mu)(x_{i}-\mu)^{T}\Sigma^{-1},H\right\rangle .
\end{align*}

\end_inset

Therefore, it follows that 
\begin_inset Formula $\mathrm{D}_{\Sigma}[(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)]=\Sigma^{-1}(x_{i}-\mu)(x_{i}-\mu)^{T}\Sigma^{-1}.$
\end_inset

 Now substitute these result back into Eq.(2) and we get 
\begin_inset Formula 
\[
\nabla_{\Sigma}\ell(\mu,\Sigma)=-\frac{d}{2}\Sigma^{-1}+\frac{1}{2}\sum_{i=1}^{d}\Sigma^{-1}(x_{i}-\mu)(x_{i}-\mu)^{T}\Sigma^{-1}.
\]

\end_inset

 Setting it to zero yields 
\begin_inset Formula 
\begin{align*}
\frac{1}{2}\sum_{i=1}^{d}\Sigma^{-1}(x_{i}-\mu)(x_{i}-\mu)^{T}\Sigma^{-1}=\frac{d}{2}\Sigma^{-1} & \implies I=\frac{1}{d}\Sigma^{-1}\sum_{i=1}^{d}(x_{i}-\mu)(x_{i}-\mu)^{T}\\
 & \implies\Sigma=\frac{1}{d}\sum_{i=1}^{d}(x_{i}-\mu)(x_{i}-\mu)^{T}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.35 - Expectation of 
\begin_inset Formula $\Sigma_{MLE}$
\end_inset

 in multivariate gaussian
\end_layout

\end_inset

 Use the result (2.59) to prove (2.62).
 Now, using the results (2.59), and (2.62), show that 
\begin_inset Formula 
\[
\E[x_{n}x_{m}]=\mu\mu^{T}+I_{nm}\Sigma
\]

\end_inset

where 
\begin_inset Formula $x_{n}$
\end_inset

 denotes a data point sampled from a Gaussian distribution with mean 
\begin_inset Formula $\mu$
\end_inset

 and covariance 
\begin_inset Formula $\Sigma$
\end_inset

 and 
\begin_inset Formula $I_{nm}$
\end_inset

 denotes the 
\begin_inset Formula $(n,m)$
\end_inset

 element of the identity matrix.
 Hence, prove the result (2.124).
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Recall in Problem 3.34, we have that 
\begin_inset Formula $\Sigma_{MLE}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu_{MLE})(X_{i}-\mu_{MLE})^{T}.$
\end_inset

 Then we have 
\begin_inset Formula 
\begin{align*}
\E[\Sigma_{MLE}] & =\frac{1}{n}\sum_{i=1}^{n}\E[(X_{i}-\mu_{MLE})(X_{i}-\mu_{MLE})^{T}]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\underbrace{\E[X_{i}X_{i}^{T}-X_{i}\mu_{MLE}^{T}-\mu_{MLE}X_{i}^{T}+\mu_{MLE}\mu_{MLE}^{T}]}_{:=\mathcal{H}(X_{i})}.
\end{align*}

\end_inset

Now we note that 
\begin_inset Formula 
\begin{align*}
\mathcal{H}(X_{i}) & =\E\bigg[X_{i}X_{i}^{T}-X_{i}\bigg(\frac{1}{n}\sum_{j=1}^{n}X_{j}\bigg)^{T}-\bigg(\sum_{j=1}^{n}X_{i}\bigg)X_{i}^{T}+\bigg(\sum_{i=1}^{n}X_{i}\bigg)\bigg(\sum_{i=1}^{n}X_{i}\bigg)^{T}\bigg]\\
 & =\E\bigg[X_{i}X_{i}^{T}-\frac{2}{n}\sum_{j=1}^{n}X_{i}X_{j}+\bigg(\sum_{i=1}^{n}X_{i}\bigg)\bigg(\sum_{i=1}^{n}X_{i}\bigg)^{T}\bigg]\\
 & =\mu\mu^{T}+\Sigma-\frac{2}{n}\bigg(\sum_{j=1}^{n}\mu\mu^{T}+\Sigma\bigg)+\frac{1}{n^{2}}\bigg[\sum_{i,j=1}^{n}\mu\mu^{T}+\sum_{i=1}^{n}\Sigma\bigg]\\
 & =\mu\mu^{T}+\Sigma-2\mu\mu^{T}-\frac{2}{n}\Sigma+\mu\mu^{T}+\frac{1}{n}\Sigma\\
 & =\left(1-\frac{1}{n}\right)\Sigma.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.36 - Sequential estimation of gaussian covariance - univariate
 case
\end_layout

\end_inset

 Using an analogous procedure to that used to obtain (2.126), derive an expressio
n for the sequential estimation of the variance of a univariate Gaussian
 distribution, by starting with the maximum likelihood expression 
\begin_inset Formula 
\[
\sigma_{ML}^{2}=\frac{1}{n}\sum_{i=1}^{n}(x_{n}-\mu)^{2}
\]

\end_inset

Verify that substituting the expression for a Gaussian distribution into
 the Robbins-Monro sequential estimation formula (2.135) gives a result of
 the same form, and hence obtain an expression for the corresponding coefficient
s 
\begin_inset Formula $a_{n}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 First, we note that 
\begin_inset Formula 
\begin{align*}
\sigma_{n}^{2} & =\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu)^{2}=\frac{n-1}{n}\frac{1}{n-1}\bigg[\sum_{i=1}^{n-1}(x_{i}-\mu)^{2}+(x_{n}-\mu)^{2}\bigg]\\
 & =\left(1-\frac{1}{n}\right)\sigma_{n-1}^{2}+\frac{1}{n}(x_{n}-\mu)^{2}\\
 & =\sigma_{n-1}^{2}+\frac{1}{n}[(x_{n}-\mu)^{2}-\sigma_{n-1}^{2}].\tag{1}
\end{align*}

\end_inset

On the other hand, using the sequential estimation formula, Eq.(2.135), we
 have that 
\begin_inset Formula 
\begin{align*}
\sigma_{n}^{2} & =\sigma_{n-1}^{2}+a_{n-1}\frac{\partial}{\partial\sigma_{n-1}^{2}}\log\left[\frac{1}{(2\pi\sigma_{n-1}^{2})^{1/2}}\exp\left\{ -\frac{(x_{n}-\mu)^{2}}{2\sigma_{n-1}^{2}}\right\} \right]\\
 & =\sigma_{n-1}^{2}+a_{n-1}\left[\frac{(x_{n}-\mu)^{2}}{2\sigma_{n-1}^{4}}-\frac{1}{2\sigma_{n-1}^{2}}\right]\\
 & =\sigma_{n-1}^{2}+\frac{a_{n-1}}{2\sigma_{n-1}^{4}}[(x_{n}-\mu)^{2}-\sigma_{n-1}^{2}].\tag{2}
\end{align*}

\end_inset

Compared Eq.(1) and Eq.(2), we see that 
\begin_inset Formula 
\[
\frac{1}{n}=\frac{a_{n-1}}{2\sigma_{n-1}^{4}}\implies a_{n-1}=\frac{2\sigma_{n-1}^{4}}{n}.
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.37 - Sequential estimation of gaussian covariance - multivariate
 case
\end_layout

\end_inset

 Using an analogous procedure to that used to obtain (2.126), derive an expressio
n for the sequential estimation of the covariance of a multivariate Gaussian
 distribution, by starting with the maximum likelihood expression (2.122).
 Verify that substituting the expression for a Gaussian distribution into
 the Robbins-Monro sequential estimation formula (2.135) gives a result of
 the same form, and hence obtain an expression for the corresponding coefficient
s 
\begin_inset Formula $a_{n}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Using the result in Problem 2.34 we have 
\begin_inset Formula 
\begin{align*}
\Sigma_{n} & =\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{T}=\frac{n-1}{n}\frac{1}{n-1}\left[\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{T}\right]\\
 & =\left(1-\frac{1}{n}\right)\frac{1}{n-1}\left[\sum_{i=1}^{n-1}(x_{i}-\mu)(x_{i}-\mu)^{T}+(x_{n}-\mu)(x_{n}-\mu)^{T}\right]\\
 & =\left(1-\frac{1}{n}\right)\Sigma_{n-1}+\frac{1}{n}(x_{n}-\mu)(x_{n}-\mu)^{T}\\
 & =\Sigma_{n-1}+\frac{1}{n}\left[(x_{n}-\mu)(x_{n}-\mu)^{T}-\Sigma_{n-1}\right].\tag{1 }
\end{align*}

\end_inset

On the other hand, using the sequential estimation formula, Eq.(2.135) , we
 see that 
\begin_inset Formula 
\begin{align*}
\Sigma_{n} & =\Sigma_{n-1}+a_{n-1}\frac{\partial}{\partial\Sigma_{n-1}}\log\left[\frac{1}{(2\pi)^{d/2}(\det\Sigma_{n-1})^{1/2}}\exp\left(-\frac{1}{2}(x_{n}-\mu)^{T}\Sigma_{n-1}^{-1}(x_{n}-\mu)\right)\right]\\
 & =\Sigma_{n-1}+a_{n-1}\mathrm{D}_{\Sigma_{n-1}}\left(-\frac{d}{2}\log2\pi-\frac{1}{2}\log\det\Sigma_{n-1}-\frac{1}{2}(x_{n}-\mu)^{T}\Sigma_{n-1}^{-1}(x_{n}-\mu)\right)\\
 & =\Sigma_{n-1}-\frac{1}{2}a_{n-1}(\mathrm{D}_{\Sigma_{n-1}}\log\det\Sigma_{n-1}+\mathrm{D}_{\Sigma_{n-1}}\left\langle \Sigma_{n-1}^{-1}(x_{n}-\mu)^{T},(x_{n}-\mu)\right\rangle )\\
 & =\Sigma_{n-1}-\frac{1}{2}a_{n-1}(\Sigma_{n-1}^{-1}-\mathrm{D}_{\Sigma_{n-1}}\left\langle \Sigma_{n-1}^{-1},(x_{n}-\mu)(x_{n}-\mu)^{T}\right\rangle )\\
 & =\Sigma_{n-1}-\frac{1}{2}a_{n-1}(\Sigma_{n-1}^{-1}-\Sigma_{n-1}^{-1}(x_{n}-\mu)(x_{n}-\mu)^{T}\Sigma_{n-1}^{-1})\\
 & =\Sigma_{n-1}+\frac{1}{2}a_{n-1}(\Sigma_{n-1}^{-2}(x_{n}-\mu)(x_{n}-\mu)^{T}-\Sigma_{n-1}^{-1})\\
 & =\Sigma_{n-1}+\frac{a_{n-1}}{2}\Sigma_{n-1}^{-2}((x_{n}-\mu)(x_{n}-\mu)^{T}-\Sigma_{n-1}).\tag{2 }
\end{align*}

\end_inset

Compare Eq.(1) and Eq.(2) and we get that 
\begin_inset Formula 
\[
\frac{a_{n-1}}{2}\Sigma_{n-1}^{-2}=\frac{1}{n}\implies a_{n-1}=\frac{2\Sigma_{n-1}^{2}}{n}.
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.38 - Completion the square for Gaussian bayesian update
\end_layout

\end_inset

 Use the technique of completing the square for the quadratic form in the
 expo- nent to derive the results (2.141) and (2.142).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Recall that we are given 
\begin_inset Formula 
\begin{align*}
f(x_{1},\dots,x_{n}\vert\mu) & =\prod_{i=1}^{n}f(x_{n}\vert\mu)=\frac{1}{(2\pi)^{n/2}\sigma^{2}}\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right);\\
f(\mu\vert\mu_{0},\sigma_{0}) & =\frac{1}{(2\pi)^{1/2}\sigma_{0}^{2}}\exp\left(-\frac{1}{2\sigma_{0}^{2}}(\mu-\mu_{0})^{2}\right).
\end{align*}

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
f(\mu\vert x_{1},\ldots,x_{n}) & \propto f(x_{1},\ldots,x_{n}\vert\mu)f(\mu\vert\mu_{0},\sigma_{0})\\
 & \propto\exp\left(-\frac{1}{2}\left(\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}+\frac{1}{\sigma_{0}^{2}}(\mu-\mu_{0})^{2}\right)\right)\\
 & =\exp\left(-\frac{1}{2}\left(\frac{\mu^{2}}{\sigma_{0}^{2}}-\frac{2\mu\mu_{0}}{\sigma_{0}^{2}}+\frac{\mu_{0}^{2}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}^{2}-\frac{2}{\sigma^{2}}\sum_{i=1}^{n}x_{i}\mu+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\mu^{2}\right)\right)\\
 & =\exp\left(-\frac{1}{2}\left[\mu^{2}\left(\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}\right)-2\mu\left(\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}\right)+\frac{\mu_{0}^{2}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}^{2}\right]\right)\\
 & \propto\exp\left(-\frac{1}{2}\left[\mu^{2}\left(\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}\right)-2\mu\left(\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}\right)\right]\right).
\end{align*}

\end_inset

Therefore, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it follows that 
\begin_inset Formula $\mu\sim\mathrm{N}((\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}})^{-1}(\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}),(\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}})^{-1}).$
\end_inset

 And we can get the desired result by noticing that 
\begin_inset Formula 
\begin{align*}
\left(\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}\right)^{-1}\left(\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}\right) & =\frac{\sigma_{0}^{2}\sigma^{2}}{\sigma^{2}+n\sigma_{0}^{2}}\frac{\mu_{0}\sigma^{2}+\sigma_{0}\sum_{i=1}^{n}x_{i}}{\sigma_{0}^{2}\sigma^{2}}=\frac{\mu_{0}\sigma^{2}}{\sigma^{2}+n\sigma_{0}^{2}}+\frac{\sigma_{0}\sum_{i=1}^{n}x_{i}}{\sigma^{2}+n\sigma_{0}^{2}}\\
 & =\frac{\mu_{0}\sigma^{2}}{\sigma^{2}+n\sigma_{0}^{2}}+\frac{n\sigma_{0}}{\sigma^{2}+n\sigma_{0}^{2}}\mu_{ML},
\end{align*}

\end_inset

and that 
\begin_inset Formula 
\[
\left(\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}\right)^{-1}=\frac{\sigma_{0}^{2}\sigma^{2}}{\sigma^{2}+n\sigma_{0}^{2}}.
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.39 - Sequential bayesian for univariate gaussian
\end_layout

\end_inset

Starting from the results (2.141) and (2.142) for the posterior distribution
 of the mean of a Gaussian random variable, dissect out the contributions
 from the first 
\begin_inset Formula $n-1$
\end_inset

 data points and hence obtain expressions for the sequential update of 
\begin_inset Formula $\mu_{n}$
\end_inset

 and 
\begin_inset Formula $\sigma_{n}^{2}$
\end_inset

 .
 Now derive the same results starting from the posterior distribution 
\begin_inset Formula $f(\mu\vert x_{1},\ldots,x_{n-1})=\mathrm{N}(\mu\vert\mu_{n-1},\sigma_{n-1}^{2})$
\end_inset

 and multiplying by the likelihood function 
\begin_inset Formula $f(x_{n}\vert\mu)=\mathrm{N}(x_{n}\vert\mu,\sigma^{2})$
\end_inset

 and then completing the square and normalizing to obtain the posterior
 distribution after 
\begin_inset Formula $n$
\end_inset

 observations.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Recall that by Eq.(2.141) and Eq.(2.142), we have 
\begin_inset Formula 
\[
\mu_{n}=\frac{\sigma^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\mu_{0}+\frac{n\sigma_{0}^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\mu_{ML}\ \text{and }\frac{1}{\sigma_{n}^{2}}=\frac{1}{\sigma_{0}^{2}}+\frac{n}{\sigma^{2}}=\frac{\sigma^{2}+n\sigma_{0}^{2}}{\sigma_{0}^{2}\sigma^{2}}.
\]

\end_inset

First, we note that since for any 
\begin_inset Formula $i=1,...,n$
\end_inset

, 
\begin_inset Formula $\frac{1}{\sigma_{i}^{2}}=\frac{1}{\sigma_{i}^{2}}+\frac{i}{\sigma^{2}},$
\end_inset

 then it follows that 
\begin_inset Formula 
\[
\frac{1}{\sigma_{n}^{2}}=\frac{1}{\sigma_{0}^{2}}+\frac{n-1}{\sigma^{2}}+\frac{1}{\sigma^{2}}=\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}}=\frac{\sigma^{2}+\sigma_{n-1}^{2}}{\sigma_{n-1}^{2}\sigma^{2}}.
\]

\end_inset

Next, note that 
\begin_inset Formula 
\[
\frac{\sigma_{n}^{2}}{\sigma_{n-1}^{2}}=\frac{\sigma^{2}+(n-1)\sigma_{0}^{2}}{\sigma_{0}^{2}\sigma^{2}}\frac{\sigma_{0}^{2}\sigma^{2}}{\sigma^{2}+n\sigma_{0}^{2}}=\frac{\sigma^{2}+(n-1)\sigma_{0}^{2}}{\sigma^{2}+n\sigma_{0}^{2}}.
\]

\end_inset

Hence, it follows that 
\begin_inset Formula 
\begin{align*}
\mu_{n-1} & =\frac{1}{n\sigma_{0}^{2}+\sigma^{2}}(\sigma^{2}\mu_{0}+n\sigma_{0}^{2}\mu_{ML})=\frac{1}{n\sigma_{0}^{2}+\sigma^{2}}\left(\sigma^{2}\mu_{0}+\sigma_{0}^{2}\sum_{i=1}^{n}x_{i}\right)\\
 & =\frac{1}{n\sigma_{0}^{2}+\sigma^{2}}\left(\sigma^{2}\mu_{0}+\sigma_{0}^{2}\sum_{i=1}^{n-1}x_{i}+\sigma_{0}^{2}x_{n}\right)\\
 & =\frac{(n-1)\sigma_{0}^{2}+\sigma^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\frac{1}{(n-1)\sigma_{0}^{2}+\sigma^{2}}\left(\sigma^{2}\mu_{0}+\sigma_{0}^{2}\sum_{i=1}^{n-1}x_{i}\right)+\frac{\sigma_{0}^{2}x_{n}}{n\sigma_{0}^{2}+\sigma^{2}}\\
 & =\frac{\sigma_{n}^{2}}{\sigma_{n-1}^{2}}\underbrace{\frac{1}{(n-1)\sigma_{0}^{2}+\sigma^{2}}\left(\sigma^{2}\mu_{0}+\sigma_{0}^{2}\sum_{i=1}^{n-1}x_{i}\right)}_{:=\mu_{n-1}}+\frac{\sigma_{0}^{2}x_{n}}{n\sigma_{0}^{2}+\sigma^{2}}\\
 & =\frac{\sigma_{n}^{2}}{\sigma_{n-1}^{2}}\mu_{n-1}+\frac{\sigma^{2}\sigma_{0}^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\frac{x_{n}}{\sigma^{2}}\\
 & =\frac{\sigma_{n}^{2}}{\sigma_{n-1}^{2}}\mu_{n-1}+\frac{\sigma_{n}x_{n}}{\sigma^{2}}.
\end{align*}

\end_inset

On the other hand, using the sequential update formula we have that 
\begin_inset Formula 
\begin{align*}
f(\mu|x_{1},\ldots,x_{n}) & =f(\mu)f(x_{n}\vert\mu)=f(\mu\vert x_{1},\ldots,x_{n-1})f(x_{n}\vert\mu)=f_{\mathrm{N}(\mu_{n-1},\sigma_{n-1}^{2})}(\mu)f_{\mathrm{N}(\mu,\sigma^{2})}(x_{n})\tag{1}\\
 & \propto\exp\left(-\frac{1}{2\sigma_{n-1}^{2}}(\mu-\mu_{n-1})^{2}-\frac{1}{2\sigma^{2}}(x_{n}-\mu)^{2}\right)\\
 & =\exp\left(-\frac{1}{2}\left(\frac{\mu^{2}-2\mu_{n-1}\mu+\mu_{n-1}^{2}}{\sigma_{n-1}^{2}}+\frac{x_{n}^{2}-2\mu x_{n}+\mu^{2}}{\sigma^{2}}\right)\right)\\
 & =\exp\left(-\frac{1}{2}\left(\mu^{2}\left(\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}}\right)-2\mu\left(\frac{\mu_{n-1}}{\sigma_{n-1}^{2}}+\frac{x_{n}}{\sigma^{2}}\right)\right)-\frac{\mu_{n-1}^{2}}{2\sigma_{n-1}^{2}}-\frac{x_{n}^{2}}{2\sigma^{2}}\right)\\
 & \propto\exp\left(-\frac{1}{2}\left(\mu^{2}\left(\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}}\right)-2\mu\left(\frac{\mu_{n-1}}{\sigma_{n-1}^{2}}+\frac{x_{n}}{\sigma^{2}}\right)\right)\right).
\end{align*}

\end_inset

Therefore, by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\mu_{n}|x_{1},...,x_{n}\sim\mathrm{N}(\mu_{n}=(\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}})^{-1}(\frac{\mu_{n-1}}{\sigma_{n-1}^{2}}+\frac{x_{n}}{\sigma^{2}}),\sigma_{n}=(\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}})^{-1})$
\end_inset

.
 We can conclude by noting 
\begin_inset Formula 
\begin{align*}
\mu_{n} & =\left(\frac{\sigma_{n-1}^{2}\sigma^{2}}{\sigma_{n-1}^{2}+\sigma^{2}}\right)\left(\frac{\sigma^{2}\mu_{n-1}+\sigma_{n-1}^{2}x_{n}}{\sigma_{n-1}^{2}\sigma^{2}}\right)=\frac{\sigma^{2}\mu_{n-1}+\sigma_{n-1}^{2}x_{n}}{\sigma_{n-1}^{2}+\sigma^{2}}=\frac{\mu_{n-1}}{\sigma_{n-1}^{2}}+\frac{\sigma_{n}x_{n}}{\sigma^{2}}.\\
\frac{1}{\sigma_{n}} & =\frac{1}{\sigma_{n-1}^{2}}+\frac{1}{\sigma^{2}}=\frac{\sigma_{n-1}^{2}+\sigma^{2}}{\sigma_{n-1}^{2}\sigma^{2}},
\end{align*}

\end_inset

which is the same as what we derived previously.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.40 - Bayesian update for multivariate gaussian 
\end_layout

\end_inset

 Consider a 
\begin_inset Formula $D$
\end_inset

 dimensional Gaussian random variable 
\begin_inset Formula $x$
\end_inset

 with distribution 
\begin_inset Formula $\mathrm{N}(x\vert\mu,\Sigma)$
\end_inset

 in which the covariance 
\begin_inset Formula $\Sigma$
\end_inset

 is known and for which we wish to infer the mean 
\begin_inset Formula $\mu$
\end_inset

 from a set of observations 
\begin_inset Formula $X=(x_{1},\ldots,x_{n}).$
\end_inset

 Given a prior distribution 
\begin_inset Formula $f(\mu)=\mathrm{N}(\mu\vert\mu_{0},\Sigma_{0})$
\end_inset

, find the corresponding posterior distribution 
\begin_inset Formula $f(\mu\vert X)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Note that 
\begin_inset Formula 
\begin{align*}
f(\mu\vert x_{1},...,x_{n}) & \propto f(\mu\vert\mu_{0},\Sigma_{0})\prod_{i=1}^{n}f(x_{i}\vert\mu,\Sigma)\\
 & \propto\exp\left(-\frac{1}{2}(\mu-\mu_{0})^{T}\Sigma_{0}^{-1}(\mu-\mu_{0})-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)\right)\\
 & =\exp\left(-\frac{1}{2}\left(\mu^{T}\Sigma_{0}^{-1}u-2\mu^{T}\Sigma_{0}^{-1}\mu_{0}+\mu_{0}^{T}\Sigma_{0}^{-1}\mu_{0}+\sum_{i=1}^{n}(x_{i}^{T}\Sigma^{-1}x_{i}-2\mu^{T}\Sigma^{-1}x_{i}+\mu^{T}\Sigma^{-1}\mu)\right)\right)\\
 & =\exp\left(-\frac{1}{2}\left(\mu^{T}(\Sigma_{0}^{-1}+n\Sigma^{-1})\mu-2\mu^{T}\bigg(\Sigma_{0}^{-1}\mu_{0}-\Sigma^{-1}\sum_{i=1}^{n}x_{i}\bigg)+\mu_{0}^{T}\Sigma_{0}^{-1}\mu_{0}+\sum_{i=1}^{n}(x_{i}^{T}\Sigma^{-1}x_{i})\right)\right).
\end{align*}

\end_inset

Therefore, it follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that 
\begin_inset Formula 
\begin{align*}
\mu\vert x_{1},\ldots,x_{n} & \sim\mathrm{MVN}\bigg((\Sigma_{0}^{-1}+n\Sigma^{-1})^{-1}\bigg(\Sigma_{0}^{-1}\mu_{0}-\Sigma^{-1}\sum_{i=1}^{n}x_{i}\bigg),(\Sigma_{0}^{-1}+n\Sigma^{-1})^{-1}\bigg)\\
 & \sim\mathrm{MVN}\left((\Sigma_{0}^{-1}+n\Sigma^{-1})^{-1}(\Sigma_{0}^{-1}\mu_{0}-n\Sigma^{-1}\mu_{ML}),(\Sigma_{0}^{-1}+n\Sigma^{-1})^{-1}\right),
\end{align*}

\end_inset

where 
\begin_inset Formula $\mu_{ML}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.41 - Gamma density is normalized
\end_layout

\end_inset

 Use the definition of the gamma function (1.141) to show that the gamma
 distribution (2.146) is normalized.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Recall that the Gamma density with parameter 
\begin_inset Formula $a,b$
\end_inset

 is given by 
\begin_inset Formula 
\[
f(x)=\frac{1}{\Gamma(a)}b^{a}x^{a-1}e^{-bx}\mathbbm{1}_{\{x\geq0\}}.
\]

\end_inset

Therefore, we have by 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset Formula 
\begin{align*}
\int_{[0,\infty)}f(x)dx & =\int_{[0,\infty)}f(T(y))\left|\det J_{T}\right|dy\tag{where \ensuremath{T(y)=\frac{y}{b}}}\\
 & =\frac{1}{\Gamma(a)}\int_{0}^{\infty}b^{a}\frac{y^{a-1}}{b^{a-1}}e^{-b\frac{1}{b}y}\cdot\frac{1}{b}dy=\frac{1}{\Gamma(a)}\underbrace{\int_{0}^{\infty}y^{a-1}e^{-y}dy}_{=\Gamma(a)}=1.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.42 - Gamma distribution's mean, mode, variance
\end_layout

\end_inset

 Evaluate the mean, variance, and mode of the gamma distribution (2.146).
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 To find the mean, note that 
\begin_inset Formula 
\begin{align*}
\E[X] & =\frac{1}{\Gamma(a)}\int_{0}^{\infty}xb^{a}x^{a-1}e^{-bx}dx=\frac{1}{\Gamma(a)}\int_{0}^{\infty}b^{a}x^{a}e^{-bx}dx=\frac{1}{\Gamma(a)}\int_{0}^{\infty}b^{a}\frac{y^{a}}{b^{a}}e^{-y}\frac{1}{b}dy\\
 & =\frac{1}{b\Gamma(a)}\int_{0}^{\infty}y^{a+1-1}e^{-y}dy=\frac{\Gamma(a+1)}{b\Gamma(a)}=\frac{a}{b}.
\end{align*}

\end_inset

To find the variance, we first note find 
\begin_inset Formula 
\begin{align*}
\E[X^{2}] & =\frac{1}{\Gamma(a)}\int_{0}^{\infty}x^{2}b^{a}x^{a-1}e^{-bx}dx=\frac{1}{\Gamma(a)}\int_{0}^{\infty}x^{a+1}b^{a}e^{-bx}dx=\frac{1}{\Gamma(a)}\int_{0}^{\infty}\frac{y^{a+1}}{b^{a+1}}b^{a}e^{-y}\frac{1}{b}dy\\
 & =\frac{1}{b^{2}\Gamma(a)}\int_{0}^{\infty}y^{a+2-1}e^{-y}dy=\frac{\Gamma(a+2)}{b^{2}\Gamma(a)}=\frac{(a+1)a}{b^{2}}.
\end{align*}

\end_inset

Hence, we have 
\begin_inset Formula 
\[
\mathrm{Var}[X]=\E[X^{2}]-(\E[X])^{2}=\frac{a^{2}+a}{b^{2}}-\frac{a^{2}}{b^{2}}=\frac{a}{b^{2}}.
\]

\end_inset

To find the mode, we differentiate the density function and set its derivative
 to 0: 
\begin_inset Formula 
\[
\frac{d}{dx}f(x)=\frac{d}{dx}\left(\frac{1}{\Gamma(a)}x^{a-1}b^{a}e^{-bx}\right)=(a-1)x^{a-2}e^{-bx}-bx^{a-1}e^{-bx}=0.
\]

\end_inset

Rearranging it a bit we get 
\begin_inset Formula $x^{a-1}\left(\frac{a-1}{x}-b\right)e^{-bx}=0$
\end_inset

.
 Since 
\begin_inset Formula $x>0$
\end_inset

 by assumption, we can further reduce to 
\begin_inset Formula $\frac{a-1}{x}-b=0,$
\end_inset

 which implies that 
\begin_inset Formula $x=\frac{a-1}{b}.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.43 - Generalized univariate Gaussian distribution 
\end_layout

\end_inset

The following distribution 
\begin_inset Formula 
\[
p(x\vert\sigma^{2},q)=\frac{q}{2(2\sigma^{2})^{1/q}\Gamma(1/q)}\exp\left(-\frac{\left|x\right|^{q}}{2\sigma^{2}}\right)\tag{1}
\]

\end_inset

is a generalization of the univariate Gaussian distribution.
 Show that this distribution is normalized so that 
\begin_inset Formula 
\[
\int_{-\infty}^{\infty}p(x\vert\sigma^{2},q)dx=1
\]

\end_inset

and that it reduces to the Gaussian when 
\begin_inset Formula $q=2.$
\end_inset

 Consider a regression model in which the target variable is given by 
\begin_inset Formula $t=y(x,w)+\varepsilon$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 is a random noise varaible drawn from distribution Eq.(1).
 Show that the log-likelihood function over 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

, for an observed date set of input vectors 
\begin_inset Formula $X=(x_{1},\ldots,x_{n})$
\end_inset

 and corresponding target variables 
\begin_inset Formula $t=(t_{1},\dots,t_{n})$
\end_inset

 is given by 
\begin_inset Formula 
\[
\ln p(t\vert X,w,\sigma^{2})=-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left|y(x_{n},w)-t_{n}\right|^{q}-\frac{n}{q}\ln(2\sigma^{2})+\mathrm{const.}
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that by change of variable of 
\begin_inset Formula $u=\frac{x^{q}}{2\sigma^{2}},$
\end_inset

 which implies that 
\begin_inset Formula $x=(2\sigma^{2}u)^{1/q}$
\end_inset

 and 
\begin_inset Formula $du=\frac{qx^{q-1}}{2\sigma^{2}}dx$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\int f(x\vert\sigma^{2},q)dx & =\frac{q}{2(2\sigma^{2})^{1/q}\Gamma(1/q)}\int_{\mathbb{R}}\exp\left(-\frac{\left|x\right|^{q}}{2\sigma^{2}}\right)dx\\
 & =\frac{q}{(2\sigma^{2})^{1/q}\Gamma(1/q)}\int_{0}^{\infty}\exp\left(-\frac{x^{q}}{2\sigma^{2}}\right)dx\\
 & =\frac{q}{(2\sigma^{2})^{1/q}\Gamma(1/q)}\int_{0}^{\infty}\exp(-u)\frac{2\sigma^{2}}{qx^{q-1}}du\\
 & =\frac{q}{(2\sigma^{2})^{1/q}\Gamma(1/q)}\int_{0}^{\infty}\exp(-u)\frac{2\sigma^{2}}{q((2\sigma^{2}u)^{1/q})^{q-1}}du\\
 & =\frac{2\sigma^{2}}{(2\sigma^{2})^{1/q}\Gamma(1/q)}\cdot(2\sigma^{2})^{1/q-1}\cdot\int_{0}^{\infty}\exp(-u)u^{1/q-1}du\\
 & =(2\sigma^{2})^{1-1/q}(2\sigma^{2})^{1/q-1}\frac{1}{\Gamma(1/q)}\Gamma(1/q)\\
 & =1.
\end{align*}

\end_inset

Next, we show that this distribution recovers the normal gaussian distribution
 when 
\begin_inset Formula $q=2.$
\end_inset

 But first, we need a lemma.
 
\end_layout

\begin_layout Lemma
For all 
\begin_inset Formula $s\in\mathbb{C},$
\end_inset

 
\begin_inset Formula $\Gamma(s)\Gamma(1-s)=\frac{\pi}{\sin\pi s}.$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: gamma function reflection property"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Hence, we let 
\begin_inset Formula $s=\frac{1}{2}$
\end_inset

 in the previous lemma; and we will get 
\begin_inset Formula $\Gamma(1/2)^{2}=\pi,$
\end_inset

 which implies that 
\begin_inset Formula $\Gamma(1/2)=\sqrt{\pi}.$
\end_inset

 Therefore, we plugin and get 
\begin_inset Formula 
\[
f\left(x\vert\sigma^{2},\frac{1}{2}\right)=\frac{2}{2(2\sigma^{2})^{1/2}\sqrt{\pi}}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right)=\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
For the regression model, 
\begin_inset Formula $t=y(x,w)+\varepsilon$
\end_inset

 where 
\begin_inset Formula $\varepsilon$
\end_inset

 is a random noise variable drawn from the generalized univariate gaussian
 distribution, we note that 
\begin_inset Formula $t-y(x,w)=\varepsilon$
\end_inset

, which is generalized univariate gaussian.
 Hence, the likelihood function 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\P(t\vert x,w,\sigma^{2})=\prod_{i=1}^{n}\frac{q}{2(2\sigma^{2})^{1/q}\Gamma(1/q)}\exp\left(-\frac{\left|t_{i}-y(x_{i},w\right|^{q}}{2\sigma^{2}}\right).
\]

\end_inset

Then we take the logarithm to get the likelihood function: 
\begin_inset Formula 
\begin{align*}
\ell(t\vert x,w,\sigma^{2}) & =\log\P(t\vert x,w,\sigma^{2})=\sum_{i=1}^{n}\log\left(\frac{q}{2(2\sigma^{2})^{1/q}\Gamma(1/q)}\exp\left(-\frac{\left|t_{i}-y(x_{i},w)\right|^{q}}{2\sigma^{2}}\right)\right)\\
 & =\sum_{i=1}^{n}\frac{\left|t_{i}-y(x_{i},w)\right|^{q}}{2\sigma^{2}}-\log(2\sigma^{2})+\mathrm{const}.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.44 - Posterior of Gaussian with Gauss-Gamma is Gauss Gamma
\end_layout

\end_inset

Consider a univariate Gaussian distribution 
\begin_inset Formula $N(x|\mu,\lambda^{-1})$
\end_inset

 having conjugate Gaussian-gamma prior given by (2.154), and a data set 
\begin_inset Formula $x=\{x_{1},\cdots,x_{n}\}$
\end_inset

 of i.i.d observations.
 Show that the posterior distribution is also a Gaussian-gamma distribution
 of the same functional form as the prior, and write down expressions for
 the parameters of this posterior distribution
\end_layout

\end_inset


\end_layout

\begin_layout Standard
First, we note that by Eq.(2.152) in the book, 
\begin_inset Formula 
\begin{align*}
\P(X\vert\mu,\lambda) & =\prod_{i=1}^{n}\left(\frac{\lambda}{2\pi}\right)^{1/2}\exp\left(-\frac{\lambda}{2}(x_{n}-\mu)^{2}\right)\propto\left[\lambda^{1/2}\exp\left(-\frac{\lambda\mu^{2}}{2}\right)\right]^{n}\exp\left(\lambda\mu\sum_{i=1}^{n}x_{i}-\frac{\lambda}{2}\sum_{i=1}^{n}x_{i}^{2}\right).
\end{align*}

\end_inset

And by assumption, 
\begin_inset Formula 
\[
\P(\mu,\lambda)=\mathrm{N}(\mu\vert\mu_{0},(\beta\lambda)^{-1})\mathrm{Gamma}(\lambda\vert a,b)=(\beta\lambda)^{1/2}\exp\left(-\frac{\beta\lambda}{2}(\mu^{2}-\mu_{0})^{2}\right)\lambda^{a-1}\exp(-b\lambda).
\]

\end_inset

Then it follows that 
\begin_inset Formula 
\begin{align*}
\P(\mu,\lambda\vert X) & \propto\P(X\vert\mu,\lambda)\P(\mu,\lambda)\\
 & \propto\left[\lambda^{\frac{1}{2}}\exp\left(-\frac{\lambda\mu^{2}}{2}\right)\right]^{n}\exp\left(\lambda\mu\sum_{i=1}^{n}x_{i}-\frac{\lambda}{2}\sum_{i=1}^{n}x_{i}^{2}\right)(\beta\lambda)^{\frac{1}{2}}\exp\left(-\frac{\beta\lambda}{2}(\mu^{2}-\mu_{0})^{2}\right)\lambda^{a-1}\exp(-b\lambda)\\
 & =\lambda^{\frac{n}{2}}(\beta\lambda)^{\frac{1}{2}}\lambda^{a-1}\exp\bigg(-\frac{n\lambda\mu^{2}}{2}+\lambda\mu\sum_{i=1}^{n}x_{i}-\frac{\lambda}{2}\sum_{i=1}^{n}x_{i}^{2}-\frac{\beta\lambda}{2}(\mu^{2}-2\mu\mu_{0}+\mu_{0})-b\lambda\bigg)\\
 & =\beta^{\frac{1}{2}}\lambda^{\frac{1}{2}+\frac{n}{2}+a-1}\exp\bigg(\mu^{2}\left(-\frac{n\lambda}{2}-\frac{\beta\lambda}{2}\right)+\mu\bigg(\beta\lambda\mu_{0}+\lambda\sum_{i=1}^{n}x_{i}\bigg)-b\lambda-\frac{\lambda}{2}\sum_{i=1}^{n}x_{i}^{2}-\frac{\beta\lambda}{2}\mu_{0}^{2}\bigg)\\
 & =\beta^{\frac{1}{2}}\lambda^{\frac{1}{2}+\frac{n}{2}+a-1}\exp\bigg(-\frac{\lambda(n+\beta)}{2}\mu^{2}+\mu\lambda\bigg(\beta\mu_{0}+\sum_{i=1}^{n}x_{i}\bigg)-\lambda\bigg(b+\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}+\frac{\beta}{2}\mu_{0}^{2}\bigg)\bigg)\\
 & =\beta^{\frac{1}{2}}\lambda^{\frac{1}{2}+\frac{n}{2}+a-1}\exp\bigg(-\frac{\lambda(n+\beta)}{2}\bigg(\mu^{2}-\frac{\beta\mu_{0}+\sum_{i=1}^{n}x_{i}}{n+\beta}\bigg)^{2}-\lambda\bigg(b+\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}+\frac{\beta}{2}\mu_{0}^{2}-\frac{(\beta\mu_{0}+\sum_{i=1}^{n}x_{i})^{2}}{2(n+\beta)}\bigg)\bigg)\\
 & =(\lambda(n+\beta))^{\frac{1}{2}}\exp\bigg(-\frac{\lambda(n+\beta)}{2}\bigg(\mu^{2}-\frac{\beta\mu_{0}+\sum_{i=1}^{n}x_{i}}{n+\beta}\bigg)^{2}\bigg)\\
 & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \times\lambda^{a+\frac{n}{2}-1}\exp\bigg(-\bigg(b+\frac{1}{2}\sum_{i=1}^{n}x_{n}^{2}+\frac{\beta}{2}\mu_{0}^{2}-\frac{(\beta\mu_{0}+\sum_{i=1}^{n}x_{i})^{2}}{2(n+\beta)}\bigg)\lambda\bigg).
\end{align*}

\end_inset

By comparing coefficients, we get that 
\begin_inset Formula 
\[
(\lambda(n+\beta))^{\frac{1}{2}}\exp\bigg(-\frac{\lambda(n+\beta)}{2}\bigg(\mu^{2}-\frac{\beta\mu_{0}+\sum_{i=1}^{n}x_{i}}{n+\beta}\bigg)^{2}\bigg)\sim\mathrm{N}(\mu\vert\mu_{n},\lambda_{n}^{-1}),
\]

\end_inset

where 
\begin_inset Formula 
\[
\mu_{n}=\frac{\beta\mu_{0}+\sum_{i=1}^{n}x_{i}}{n+\beta},\ \ \lambda_{n}=\frac{1}{\lambda(n+\beta)}.
\]

\end_inset

And that 
\begin_inset Formula 
\[
\lambda^{a+\frac{n}{2}-1}\exp\bigg(-\bigg(b+\frac{1}{2}\sum_{i=1}^{n}x_{n}^{2}+\frac{\beta}{2}\mu_{0}^{2}-\frac{(\beta\mu_{0}+\sum_{i=1}^{n}x_{i})^{2}}{2(n+\beta)}\bigg)\lambda\bigg)\sim\mathrm{Gamma}(\lambda\vert a_{n},b_{n}),
\]

\end_inset

where 
\begin_inset Formula 
\[
a_{n}=a+\frac{n}{2},\ \ b_{b}=b+\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}+\frac{\beta}{2}\mu_{0}^{2}-\frac{(\beta\mu_{0}+\sum_{i=1}^{n}x_{n})^{2}}{2(n+\beta)}.
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.45 - Wishart distribution is conjugate prior of Gaussian precision
\end_layout

\end_inset

 Verify that the Wishart distribution defined by (2.155) is indeed a conjugate
 prior for the precision matrix of multivariate Gaussian.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that by Bayes update formula, 
\begin_inset Formula 
\begin{align*}
f(\Lambda\vert X_{1},\ldots,X_{n}) & \propto f(X_{1},\ldots,X_{n}\vert\Lambda)f(\Lambda)=\prod_{i=1}^{n}f_{\mathrm{N}(\mu_{0},\Lambda^{-1})}(x_{i})f_{\mathrm{Wishart}(W,\nu)}(\Lambda)\\
 & =\bigg(\prod_{i=1}^{n}\frac{\left|\det\Lambda\right|{}^{1/2}}{(2\pi)^{d/2}}\bigg)\exp\bigg(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{T}\Lambda(x_{i}-\mu)\bigg)\cdot B\left|\det\Lambda\right|^{(v-d-1)/2}\exp\left(-\frac{1}{2}\mathrm{tr}(W^{-1}\Lambda)\right)\\
 & =\bigg(\prod_{i=1}^{n}\frac{\left|\det\Lambda\right|{}^{1/2}}{(2\pi)^{d/2}}\bigg)\exp\bigg(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{T}\Lambda(x_{i}-\mu)\bigg)\cdot B\left|\det\Lambda\right|^{(v-d-1)/2}\exp\left(-\frac{1}{2}\mathrm{tr}(W^{-1}\Lambda)\right)\\
 & \propto\left|\det\Lambda\right|^{n/2+(v-d-1)/2}\exp\bigg(-\frac{1}{2}\sum_{i=1}^{n}\mathrm{tr}((x_{i}-\mu)^{T}\Lambda(x_{i}-\mu)-\frac{1}{2}\mathrm{tr}(\Lambda W^{-1})\bigg)\\
 & =\left|\det\Lambda\right|^{(n+v-d-1)/2}\exp\bigg(-\frac{1}{2}\mathrm{tr}\bigg(\Lambda\bigg(\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{T}+W\bigg)\bigg)\bigg)\\
 & \propto f_{\mathrm{Wishart}(\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{T}+W,n+v)}(\Lambda).
\end{align*}

\end_inset

Therefore, we have shown that Wishart distribution is a conjugate prior
 for the precision matrix of multivariate Gaussian.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.46 - Verify that evaluating the integral in (2.158) leads to the
 result (2.159).
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that we have 
\begin_inset Formula 
\begin{align*}
f_{\mathrm{St}(x\vert\mu,\lambda,v)}(x) & =\int_{0}^{\infty}\frac{b^{a}e^{-b\lambda}\lambda^{a-1}}{\Gamma(a)}\biggp{\frac{\lambda}{2\pi}}^{1/2}\exp\biggp{-\frac{\lambda}{2}(x-\mu)^{2}}d\lambda\\
 & =\frac{b^{a}}{\Gamma(a)}\biggp{\frac{1}{2\pi}}^{1/2}\int_{0}^{\infty}\lambda^{a-1/2}\exp\biggp{-\lambda\biggp{b+\frac{1}{2}(x-\mu)^{2}}}d\lambda.\tag{1}
\end{align*}

\end_inset

Now, define the homeomorphism 
\begin_inset Formula $T:[0,\infty)\rightarrow[0,\infty)$
\end_inset

 by 
\begin_inset Formula $y\mapsto(b+\frac{1}{2}(x-\mu)^{2})^{-1}y$
\end_inset

 and apply 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: transformation thm"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we see that 
\begin_inset Formula 
\begin{align*}
\mathrm{Eq}.(1) & =\frac{b^{a}}{\Gamma(a)}\biggp{\frac{1}{2\pi}}^{1/2}\int_{0}^{\infty}y^{a-1/2}\left(b+\frac{1}{2}(x-\mu)\right)^{-a+1/2}\exp(-y)\left(b+\frac{1}{2}(x-\mu)^{2}\right)^{-1}dy\\
 & =\frac{b^{a}}{\Gamma(a)}\biggp{\frac{1}{2\pi}}^{1/2}\left(b+\frac{1}{2}(x-\mu)\right)^{-a-1/2}\int_{0}^{\infty}y^{a-1/2}\exp(-y)dy.\\
 & =\frac{\Gamma(a+\frac{1}{2})}{\Gamma(a)}b^{a}\left(\frac{1}{2\pi}\right)^{1/2}\biggp{b+\frac{1}{2}(x-\mu)}^{-a-1/2}\tag{2 }
\end{align*}

\end_inset

Now we substitute in 
\begin_inset Formula $a=b=v/2,$
\end_inset

 we get 
\begin_inset Formula 
\begin{align*}
\mathrm{Eq.}(2) & =\frac{\Gamma(a+\frac{1}{2})}{\Gamma(a)}\biggp{\frac{v}{2}}^{v/2}\biggp{\frac{1}{2\pi}}^{1/2}\biggp{\frac{v}{2}+\frac{1}{2}(x-\mu)}^{-v/2-1/2}\\
 & =\frac{\Gamma(a+\frac{1}{2})}{\Gamma(a)}\biggp{\frac{v}{2}}^{v/2}\biggp{\frac{1}{2\pi}}^{1/2}\biggp{\frac{v}{2}}^{-v/2-1/2}\biggp{1+\frac{1}{v}(x-\mu)}^{-v/2-1/2}\\
 & =\frac{\Gamma(a+\frac{1}{2})}{\Gamma(a)}\biggp{\frac{1}{\pi v}}^{1/2}\biggp{1+\frac{1}{v}(x-\mu)}^{-(v+1)/2},
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.47 - Student 
\begin_inset Formula $t$
\end_inset

-distribution converges to Gaussian
\end_layout

\end_inset

 Show that in the limit 
\begin_inset Formula $v\rightarrow\infty$
\end_inset

, the 
\begin_inset Formula $t$
\end_inset

-distribution (2.159) becomes a Gaussian.
 Hint: ignore the normalization coefficient, and simply look at the dependence
 on 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before, we go into the solution, we need an asymptotic approximation of
 ratio of Gamma function.
 We provide this approximation and give a proof as follows.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\end_layout

\begin_layout Lemma
For any 
\begin_inset Formula $\alpha\in\mathbb{R},$
\end_inset

 we have 
\begin_inset CommandInset label
LatexCommand label
name "lem: gamma stirling approximation"

\end_inset


\begin_inset Formula 
\[
\lim_{x\rightarrow\infty}\frac{\Gamma(x+\alpha)}{\Gamma(x)x^{\alpha}}=1.
\]

\end_inset


\end_layout

\begin_layout Proof
First, we note that by OEIS A046968 and A046969 we have that for 
\begin_inset Formula $z\in\mathbb{R}$
\end_inset


\begin_inset Formula 
\begin{align*}
\log\Gamma(z) & =z\log z-z+\frac{1}{2}(\log2\pi+\log\frac{1}{z})+\frac{1}{12z}-\frac{1}{360z^{3}}+\frac{1}{1260z^{5}}-\cdots\\
 & =z(\log z-1)+\frac{1}{2}(\log2\pi+\log\frac{1}{z})+\frac{1}{12z}+O\bigg(\frac{1}{z^{3}}\bigg).\tag{1 }
\end{align*}

\end_inset

Hence, it suffices to prove that 
\begin_inset Formula $\log\frac{\Gamma(x+\alpha)}{\Gamma(x)x^{\alpha}}\rightarrow0$
\end_inset

 as 
\begin_inset Formula $x\rightarrow\infty.$
\end_inset

 Note that 
\begin_inset Formula 
\begin{align*}
\log\frac{\Gamma(x+\alpha)}{\Gamma(x)x^{\alpha}} & =\log\Gamma(x+\alpha)-\log\Gamma(x)-\alpha\log x\\
 & =(x+\alpha)(\log(x+\alpha)-1)+\frac{1}{2}\bigg(\log2\pi+\log\frac{1}{x+\alpha}\bigg)+\frac{1}{12(x+\alpha)}+O\bigg(\frac{1}{(x+\alpha)^{3}}\bigg)\\
 & \ \ -x\log x-x-\frac{1}{2}\bigg(\log2\pi+\log\frac{1}{\alpha}\bigg)-\frac{1}{12x}-O\bigg(\frac{1}{x^{3}}\bigg)-\alpha\log x\\
 & =x\log\bigg(1+\frac{\alpha}{x}\bigg)-\frac{1}{2}\log\bigg(1+\frac{\alpha}{x}\bigg)+\alpha\log\bigg(1+\frac{\alpha}{x}\bigg)+\frac{1}{12}\bigg(\frac{1}{x+\alpha}-\frac{1}{x}\bigg)+O\bigg(\frac{1}{x^{3}}\bigg).\tag{2}
\end{align*}

\end_inset

Note that 
\begin_inset Formula $\log(1+\frac{\alpha}{x})=\frac{\alpha}{x}+O(\frac{1}{x^{2}})=\frac{\alpha}{x}-\frac{\alpha^{2}}{2x^{2}}+O(\frac{1}{x^{3}})$
\end_inset

 via Taylor expansion.
 So substituting back we have 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\mathrm{Eq}.(2) & =x\bigg(\frac{\alpha}{x}-\frac{\alpha^{2}}{2x^{2}}+O\bigg(\frac{1}{x^{3}}\bigg)\bigg)-\frac{1}{2}\biggp{\frac{\alpha}{x}+O\biggp{\frac{1}{x^{2}}}}+O\biggp{\frac{1}{x^{2}}}-\alpha\biggp{\frac{\alpha}{x}+O\biggp{\frac{1}{x^{2}}}}-\alpha+O\biggp{\frac{1}{x^{3}}}\\
 & =-\frac{\alpha^{2}}{2x}+\frac{\alpha^{2}}{x}-\frac{\alpha}{2x}+O\biggp{\frac{1}{x^{3}}}+O\biggp{\frac{1}{x^{2}}}\\
 & =\frac{\alpha(\alpha-1)}{2x}+O\biggp{\frac{1}{x^{2}}},
\end{align*}

\end_inset

where the last equality follows from the assumption 
\begin_inset Formula $x\geq1$
\end_inset

 and the fact that 
\begin_inset Formula $O(f(x))+O(g(x))=O(\max(f(x)+g(x))$
\end_inset

.
 Now we take the limit of 
\begin_inset Formula $x\rightarrow\infty,$
\end_inset

 we get that 
\begin_inset Formula 
\[
\lim_{x\rightarrow\infty}\log\frac{\Gamma(x+\alpha)}{\Gamma(x)x^{\alpha}}=\lim_{x\rightarrow\infty}\left[\frac{\alpha(\alpha-1)}{2x}+O\biggp{\frac{1}{x^{2}}}\right]=0\implies\lim_{x\rightarrow\infty}\frac{\Gamma(x+\alpha)}{\Gamma(x)x^{\alpha}}=1.
\]

\end_inset


\end_layout

\begin_layout Standard
Now we observe that 
\begin_inset Formula 
\begin{align*}
f_{\mathrm{St}(x\vert\mu,\lambda,v)}(x) & =\frac{\Gamma(\frac{v}{2}+\frac{1}{2})}{\Gamma(\frac{v}{2})}\biggp{\frac{\lambda}{\pi v}}^{1/2}\left[1+\frac{\lambda(x-\mu)^{2}}{v}\right]^{-v/2-1/2}\\
 & =\frac{\Gamma(\frac{v}{2}+\frac{1}{2})}{\Gamma(\frac{v}{2})(\frac{v}{2})^{1/2}}\underbrace{\bigg(\frac{v}{2}\bigg)^{1/2}\biggp{\frac{\lambda}{\pi v}}^{1/2}\exp\left[-\frac{v+1}{2}\log\biggp{1+\frac{\lambda(x-\mu)^{2}}{v}}\right]}_{:=\mathcal{H}_{1}(x)}.
\end{align*}

\end_inset

Now note that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gamma stirling approximation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\frac{\Gamma(\frac{v}{2}+\frac{1}{2})}{\Gamma(\frac{v}{2})(\frac{v}{2})^{1/2}}\rightarrow1$
\end_inset

 as 
\begin_inset Formula $v\rightarrow\infty.$
\end_inset

 And by Taylor expansion of logarithm, 
\begin_inset Formula 
\begin{align*}
\mathcal{H}_{1}(x) & =\frac{1}{(2\pi\lambda^{-1})^{1/2}}\exp\left[-\frac{v+1}{2}\biggp{\frac{\lambda(x-\mu)^{2}}{v}+O\biggp{\frac{1}{v^{2}}}}\right]\\
 & =\frac{1}{(2\pi\lambda^{-1})^{1/2}}\exp\left[-\frac{v+1}{v}\frac{(x-\mu)^{2}}{2\lambda^{-1}}+O\biggp{\frac{1}{v}}\right]\\
 & \xrightarrow{v\rightarrow\infty}\frac{1}{(2\pi\lambda^{-1})^{1/2}}\exp\left(\frac{(x-\mu)^{2}}{2\lambda^{-1}}\right).
\end{align*}

\end_inset

Hence, combined together we have that 
\begin_inset Formula $f_{\mathrm{St}(x\vert\mu,\lambda,v)}(x)\xrightarrow{v\rightarrow\infty}f_{\mathrm{N}(\mu,\lambda^{-1})}(x).$
\end_inset

 
\end_layout

\begin_layout Remark
The argument we used in solution is almost perfectly rigorous except a subtle
 point that we need to show that convergence in density functions implies
 convergence in distribution.
 This is actually a quite famous result, called Scheffe's lemma.
 We state without a proof the theorem statement below.
 A proof can be found in the reference provided.
 
\end_layout

\begin_layout Lemma
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
after "Lemma 8.2.1"
key "resnickProbabilityPath2014"
literal "false"

\end_inset


\end_layout

\end_inset

Let 
\begin_inset Formula $\{X_{n}\}_{n\in\mathbb{N}}$
\end_inset

 be absolutely continuous random variables with densities 
\begin_inset Formula $f_{X_{n}}$
\end_inset

, such that 
\begin_inset Formula $f_{X_{n}}(x)\rightarrow f(x)$
\end_inset

 almost everywhere, where 
\begin_inset Formula $f$
\end_inset

 is the density of the absolutely continuous random variable 
\begin_inset Formula $X$
\end_inset

.
 Then 
\begin_inset Formula $X_{n}$
\end_inset

 converges to 
\begin_inset Formula $X$
\end_inset

 in total variation, and therefore, also in distribution.
 
\begin_inset CommandInset label
LatexCommand label
name "lem: scheffe'e lemma"

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.48 - Multivariate 
\begin_inset Formula $t$
\end_inset

-distribution: derivation 
\end_layout

\end_inset

 By following analogous steps to those used to derive the univariate Student’s
 
\begin_inset Formula $t$
\end_inset

-distribution (2.159), verify the result (2.162) for the multivariate form
 of the Student’s 
\begin_inset Formula $t$
\end_inset

-distribution, by marginalizing over the variable 
\begin_inset Formula $\eta$
\end_inset

 in (2.161).
 Using the definition (2.161), show by exchanging integration variables that
 the multivariate 
\begin_inset Formula $t$
\end_inset

-distribution is correctly normalized.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
WLOG, assume we are working in 
\begin_inset Formula $\mathbb{R}^{d}.$
\end_inset

 By definition, we have that 
\begin_inset Formula 
\begin{align*}
f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x) & =\int_{0}^{\infty}f_{\mathrm{MVN}(x\vert\mu,(\eta\Lambda)^{-1})}f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}d\eta\\
 & =\int_{0}^{\infty}\frac{\det((\eta I)\Lambda)^{1/2}}{(2\pi)^{d/2}}\exp\biggp{-\frac{1}{2}(x-\mu)^{T}(\eta\Lambda)(x-\mu)}\frac{1}{\Gamma(v/2)}\eta^{v/2-1}\biggp{\frac{v}{2}}^{v/2}\exp(-\eta v/2)d\eta\\
 & =\frac{1}{\Gamma(v/2)}\frac{1}{(2\pi)^{d/2}}\biggp{\frac{v}{2}}^{v/2}\int\eta^{d/2+v/2-1}\exp\biggp{-\eta\left(\frac{(x-\mu)^{T}\Lambda(x-\mu)}{2}+\frac{v}{2}\right)}d\eta\\
 & =\frac{\left|\det\Lambda\right|^{1/2}}{\Gamma(v/2)}\frac{1}{(2\pi)^{d/2}}\biggp{\frac{v}{2}}^{v/2}\biggp{\frac{(x-\mu)^{T}\Lambda(x-\mu)}{2}+\frac{v}{2}}^{-d/2-v/2}\int\lambda^{d/2+v/2-1}\exp(-\lambda)d\lambda\\
 & =\frac{\Gamma(v/2+d/2)}{\Gamma(v/2)}\frac{\left|\det\Lambda\right|^{1/2}}{(2\pi)^{d/2}}\biggp{\frac{v}{2}}^{v/2}\biggp{\frac{v}{2}}^{-d/2-v/2}\biggp{\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)+1}^{-d/2-v/2}\\
 & =\frac{\Gamma(v/2+d/2)}{\Gamma(v/2)}\frac{\left|\det\Lambda\right|^{1/2}}{(\pi v)^{d/2}}\biggp{\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)+1}^{-d/2-v/2}.
\end{align*}

\end_inset

To see that it normalizes to 
\begin_inset Formula $1$
\end_inset

, we note that 
\begin_inset Formula 
\begin{align*}
\int f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x) & =\int\int f_{\mathrm{MVN}(x\vert\mu,(\eta\Lambda)^{-1})}f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}d\eta dx\\
 & =\int\biggp{\int f_{\mathrm{MVN}(x\vert\mu,(\eta\Lambda)^{-1})}dx}f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}d\eta\tag{by Fubini's theorem}\\
 & =\int f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}d\eta=1.
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.49 - Multivariate 
\begin_inset Formula $t$
\end_inset

 distribution: mean, variance, mode
\end_layout

\end_inset

 By using the definition (2.161) of the multivariate Student’s 
\begin_inset Formula $t$
\end_inset

-distribution as a convolution of a Gaussian with a gamma distribution,
 verify the properties (2.164), (2.165), and (2.166) for the multivariate 
\begin_inset Formula $t$
\end_inset

-distribution defined by (2.162).
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 In the discussion below, we let 
\begin_inset Formula $X$
\end_inset

 be a random variable with multivariate 
\begin_inset Formula $t$
\end_inset

 distribution.
 Then for expectation, we note 
\begin_inset Formula 
\begin{align*}
\E[X] & =\int_{\mathbb{R}^{d}}\frac{\Gamma(\frac{d}{2}+\frac{v}{2})}{\Gamma(\frac{v}{2})}\frac{(\det\Lambda)^{1/2}}{(\pi v)^{d/2}}\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2}xdx\\
 & =\int_{\mathbb{R}^{d}}\biggp{1+\frac{z^{T}\Lambda z}{v}}^{-d/2-v/2}(z+\mu)dz\tag{let \ensuremath{x=z+\mu}}\\
 & =\frac{\Gamma(\frac{d}{2}+\frac{v}{2})}{\Gamma(\frac{v}{2})}\frac{(\det\Lambda)^{1/2}}{(\pi v)^{d/2}}\int_{\mathbb{R}^{d}}\biggp{1+\frac{z^{T}\Lambda z}{v}}^{-d/2-v/2}zdz+\int_{\mathbb{R}^{d}}f_{\mathrm{St}(z\vert0,\Lambda,v)}(z)\mu dz.
\end{align*}

\end_inset

Note that using similar argument as in the proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: odd function in multivariate dimension"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have that 
\begin_inset Formula $\int_{\mathbb{R}^{d}}(1+\frac{z^{T}\Lambda z}{v})^{-d/2-v/2}zdz=0$
\end_inset

.
 Also, we have 
\begin_inset Formula $\int_{\mathbb{R}^{d}}f_{\mathrm{St}(z\vert0,\Lambda,v)}(z)\mu dz=\mu\int_{\mathbb{R}^{d}}f_{\mathrm{St}(z\vert0,\Lambda,v)}(z)dz=1.$
\end_inset

 Therefore, it follows that 
\begin_inset Formula $\E[X]=\mu.$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To find the covariance, we note that 
\begin_inset Formula 
\begin{align*}
\mathrm{Cov}[X] & =\int f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x)(x-\mu)(x-\mu)^{T}dx\\
 & =\int\int f_{\mathrm{MVN}(x\vert\mu,(\eta\Lambda)^{-1})}(x)f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}(\eta)d\eta(x-\mu)(x-\mu)^{T}dx\\
 & =\int\biggp{\int f_{\mathrm{MVN}(x\vert\mu,(\eta\Lambda)^{-1})}(x)(x-\mu)(x-\mu)^{T}dx}f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}(\eta)d\eta\tag{by Fubini's theorem}\\
 & =\Lambda^{-1}\int\eta^{-1}f_{\mathrm{Gamma}(\eta\vert v/2,v/2)}(\eta)d\eta.\tag{1}
\end{align*}

\end_inset

To evaluate the Eq.(1), we need an additional lemma regarding moments of
 Gamma distribution.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $X$
\end_inset

 be an 
\begin_inset Formula $\mathbb{R}$
\end_inset

-valued random variable such that 
\begin_inset Formula $X\sim\mathrm{Gamma}(a,b).$
\end_inset

 Then 
\begin_inset Formula $\E[X^{-1}]=\frac{b}{a-1}.$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem: E[1/X] for gamma"

\end_inset


\end_layout

\begin_layout Proof
Note that 
\begin_inset Formula 
\begin{align*}
\E[X^{-1}] & =\int_{0}^{\infty}\frac{1}{x}\frac{1}{\Gamma(a)}x^{a-1}b^{a}\exp(-bx)dx\\
 & =\frac{b\Gamma(a-1)}{\Gamma(a)}\int_{0}^{\infty}\frac{1}{\Gamma(a-1)}x^{(a-1)-1}b^{a-1}\exp(-bx)dx\\
 & =\frac{b}{a-1}\int_{0}^{\infty}f_{\mathrm{Gamma}(x\vert a-1,b)}(x)dx\\
 & =\frac{b}{a-1}.
\end{align*}

\end_inset

as desired.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Now we come back to the solution.
 Note that by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: E[1/X] for gamma"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have that Eq.(1) = 
\begin_inset Formula $\frac{b}{a-1}$
\end_inset

 and as a result, 
\begin_inset Formula 
\[
\mathrm{Cov}[X]=\Lambda^{-1}\frac{v/2}{v/2-1}=\frac{v}{v-2}\Lambda^{-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
Now to find node, it suffices for us to find the critical points of the
 density function.
 Note that 
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial x}f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x) & =\frac{\partial}{\partial x}\bigg[\frac{\Gamma(\frac{d}{2}+\frac{v}{2})}{\Gamma(\frac{v}{2})}\frac{(\det\Lambda)^{1/2}}{(\pi v)^{d/2}}\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2}\bigg]\\
 & \propto\frac{\partial}{\partial x}\bigg[\bigg(1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)\bigg)^{-d/2-v/2}\bigg]\\
 & =-\biggp{\frac{d}{2}+\frac{v}{2}}\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2-1}\frac{\partial}{\partial x}[(x-\mu)^{T}\Lambda(x-\mu)]\\
 & =-2\biggp{\frac{d}{2}+\frac{v}{2}}\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2-1}\Lambda(x-\mu).
\end{align*}

\end_inset

Setting it to zero:
\begin_inset Formula 
\begin{align*}
\nabla f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x)=0 & \implies\Lambda(x-\mu)=0\tag{since \ensuremath{-2(\frac{d}{2}+\frac{v}{2})(1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)^{-d/2-v/2-1})\neq0}}\\
 & \implies x=\mu\tag{since \ensuremath{\Lambda\ }is p.s.d thus invertible}.
\end{align*}

\end_inset

Hence, we have that 
\begin_inset Formula $\mathrm{mode}[X]=\mu.$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.50 - Multivariate 
\begin_inset Formula $t$
\end_inset

 distribution converges to multivariate gaussian as 
\begin_inset Formula $v\rightarrow\infty$
\end_inset


\end_layout

\end_inset

 Show that in the limit 
\begin_inset Formula $v\rightarrow\infty,$
\end_inset

 the multivariate Student 
\begin_inset Formula $t$
\end_inset

 distribution reduces to a Gaussian with mean 
\begin_inset Formula $\mu$
\end_inset

 and precision 
\begin_inset Formula $\Lambda$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.
 
\series default
Recall that the multivariate 
\begin_inset Formula $t$
\end_inset

 distribution has density of the form 
\begin_inset Formula 
\[
f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x)=\underbrace{\frac{\Gamma(\frac{d}{2}+\frac{v}{2})}{\Gamma(\frac{v}{2})}\frac{\left|\det\Lambda\right|{}^{1/2}}{(\pi v)^{d/2}}}_{:=\mathcal{H}_{1}(x)}\underbrace{\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2}}_{:=\mathcal{H}_{2}(x)}.
\]

\end_inset

We massage term by term: 
\begin_inset Formula 
\begin{align*}
\mathcal{H}_{1}(v) & =\frac{\Gamma(\frac{d}{2}+\frac{v}{2})}{\Gamma(\frac{v}{2})(\frac{v}{2})^{d/2}}v^{d/2}2^{-d/2}\frac{\left|\det\Lambda\right|^{1/2}}{\pi^{d/2}}v^{-d/2}\xrightarrow{v\rightarrow\infty}\frac{\left|\det\Lambda\right|^{1/2}}{(2\pi)^{d/2}}\tag{by \ref{lem: gamma stirling approximation}}
\end{align*}

\end_inset

Also, we have 
\begin_inset Formula 
\begin{align*}
\mathcal{H}_{2}(v) & =\exp\bigg[\log\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}^{-d/2-v/2}\bigg]\\
 & =\exp\left[-\frac{d+v}{2}\log\biggp{1+\frac{1}{v}(x-\mu)^{T}\Lambda(x-\mu)}\right]\\
 & =\exp\bigg[-\frac{d+v}{2}\biggp{\frac{\Delta^{2}}{v}+O(v^{-2})}\bigg]\\
 & =\exp\bigg[-\frac{\Delta^{2}}{2}-\underbrace{\frac{d\Delta^{2}}{2}\frac{1}{v}}_{=O(v^{-1})}-\frac{d+v}{2}O(v^{-2})\bigg]\\
 & =\exp\bigg[-\frac{\Delta^{2}}{2}+O(v^{-1})\bigg]\xrightarrow{v\rightarrow\infty}\exp\left[-\frac{1}{2}(x-\mu)^{T}\Lambda(x-\mu)\right].
\end{align*}

\end_inset

Combined, we get that 
\begin_inset Formula 
\[
f_{\mathrm{St}(x\vert\mu,\Lambda,v)}(x)\xrightarrow{v\rightarrow\infty}\frac{\det\left|\Lambda\right|^{1/2}}{(2\pi)^{d/2}}\exp\left[-\frac{1}{2}(x-\mu)^{T}\Lambda(x-\mu)\right].
\]

\end_inset

Then the result follow by 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: scheffe'e lemma"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.51 - Verify Eq.
 (2.177), (2.178), (2.283)
\end_layout

\end_inset

 The various trigonometric identities used in the discussion of periodic
 variables in this chapter can be proven easily from the relation 
\begin_inset Formula 
\[
\exp(iA)=\cos A+i\sin A,
\]

\end_inset

in which 
\begin_inset Formula $i$
\end_inset

 is the square root of minus one.
 By considering the identity 
\begin_inset Formula 
\[
\exp(iA)\exp(-iA)=1,
\]

\end_inset

prove the result (2.177).
 Similarly, using the identity 
\begin_inset Formula 
\[
\cos(A-B)=\mathrm{Re}\{\exp(i(A-B))\},
\]

\end_inset

where 
\begin_inset Formula $\mathrm{Re}\{\cdot\}$
\end_inset

 denotes the real part, prove (2.178).
 Finally, by using 
\begin_inset Formula $\sin(A-B)=\mathrm{Im}\{\exp(i(A-B))\}$
\end_inset

 denotes the imaginary part, prove the result (2.183).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
  First, to verify (2.177) we note that 
\begin_inset Formula 
\[
\cos^{2}A+\sin^{2}A=(\cos A+i\sin A)(\cos A-i\sin A)=\exp(iA)\exp(-iA)=1.
\]

\end_inset

Then, to verify (2.178), we have that 
\begin_inset Formula 
\begin{align*}
\cos(A-B) & =\mathrm{Re}\left\{ \exp(i(A-B))\right\} =\mathrm{Re}\left\{ \exp(iA)\exp(-iB)\right\} \\
 & =\mathrm{Re}\left\{ (\cos A+i\sin A)(\cos B-i\sin B)\right\} \\
 & =\cos A\cos B+\sin A\sin B.
\end{align*}

\end_inset

Finally, to verify (2.183), we have that 
\begin_inset Formula 
\begin{align*}
\sin(A-B) & =\mathrm{Im}\left\{ \exp(i(A-B))\right\} =\mathrm{Im}\{\exp(iA)\exp(-iB)\}\\
 & =\mathrm{Im}\left\{ (\cos A+i\sin A)(\cos B-i\sin B)\right\} \\
 & =\sin A\cos B-\cos A\sin B
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.52 - von Mise distribution converge to Gaussian asymptotically
\end_layout

\end_inset

 For large 
\begin_inset Formula $m$
\end_inset

, the von Mises distribution (2.179) becomes sharply peaked around the mode
 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 By defining 
\begin_inset Formula $\xi=m^{1/2}(\theta-\theta_{0})$
\end_inset

 and making the Taylor expansion of the cosine function given by 
\begin_inset Formula 
\[
\cos\alpha=1-\frac{\alpha^{2}}{2}+O(\alpha^{4})
\]

\end_inset

show that as 
\begin_inset Formula $m\rightarrow\infty,$
\end_inset

 the von Mises distribution tends to a Gaussian.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 We follow the hint to use the substitution 
\begin_inset Formula $\xi=m^{1/2}(\theta-\theta_{0}),$
\end_inset

 with which we can write the density of von Mises distribution as 
\begin_inset Formula 
\begin{align*}
f(\theta\vert\theta_{0},m) & =f(\xi)\propto\exp(m\cos(\xi m^{1/2}))\\
 & =\exp\biggp{m\biggp{1-\frac{1}{2}m^{-1}\xi^{2}+O(m^{-2})}}\\
 & \propto\exp\biggp{-\frac{\xi^{2}}{2}}=\exp\biggp{-\frac{m(\theta-\theta_{0})^{2}}{2}},
\end{align*}

\end_inset

and the result follows from 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem: gaussian completion of squares"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.53 - MLE of von Mises distribution
\end_layout

\end_inset

 Using the trigonometric identity (2.183), show that solution of (2.182) for
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is given by (2.184).
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Solution.

\series default
 Using the formula provided in Eq.(2.183), we can rewrite Eq.(2.182) as 
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{n}\sin(\theta_{i}-\theta_{0}) & =\sum_{i=1}^{n}(\cos\theta_{0}\sin\theta_{i}-\cos\theta_{i}\sin\theta_{0})\\
 & =\cos\theta_{0}\biggp{\sum_{i=1}^{n}\sin\theta_{i}}-\sin\theta_{0}\biggp{\sum_{i=1}^{n}\cos\theta_{i}}\\
 & =0.
\end{align*}

\end_inset

Rearranging the terms we get 
\begin_inset Formula 
\begin{align*}
\cos\theta_{0}\biggp{\sum_{i=1}^{n}\sin\theta_{i}}=\sin\theta_{0}\biggp{\sum_{i=1}^{n}\cos\theta_{i}} & \implies\tan\theta_{0}=\frac{\sin\theta_{0}}{\cos\theta_{0}}=\frac{\sum_{i=1}^{n}\sin\theta_{i}}{\sum_{i=1}^{n}\cos\theta_{i}}\\
 & \implies\theta_{0}=\tan^{-1}\left\{ \frac{\sum_{i=1}^{n}\sin\theta_{i}}{\sum_{i=1}^{n}\cos\theta_{i}}\right\} 
\end{align*}

\end_inset

as desired.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Custom Color Box 1
status open

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Problem 2.54 - Mode of von Mises distribution
\end_layout

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_body
\end_document
